<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;zhang-xiaoxue.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Gemini&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;}}</script>
<meta name="description" content="Variational InferenceThe solution is obtained by exploring all possible input functions to find the one that maximizes, or minimizes, the functional.   restriction : factorization assumption  log marg">
<meta property="og:type" content="article">
<meta property="og:title" content="Variational Inference">
<meta property="og:url" content="https://zhang-xiaoxue.github.io/2020/07/30/Model%20Predictive%20Control/Variational%20Inference/index.html">
<meta property="og:site_name" content="Xiaoxue Zhang - NUS">
<meta property="og:description" content="Variational InferenceThe solution is obtained by exploring all possible input functions to find the one that maximizes, or minimizes, the functional.   restriction : factorization assumption  log marg">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/v2-ef06fbce0bb4818363ed2010fc357fdf_r.jpg">
<meta property="article:published_time" content="2020-07-30T04:00:00.000Z">
<meta property="article:modified_time" content="2021-05-13T07:32:02.216Z">
<meta property="article:author" content="Xiaoxue Zhang">
<meta property="article:tag" content="Reinforcement Learning, Optimization and Control, Intellegent Systems">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/v2-ef06fbce0bb4818363ed2010fc357fdf_r.jpg">


<link rel="canonical" href="https://zhang-xiaoxue.github.io/2020/07/30/Model%20Predictive%20Control/Variational%20Inference/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;zhang-xiaoxue.github.io&#x2F;2020&#x2F;07&#x2F;30&#x2F;Model%20Predictive%20Control&#x2F;Variational%20Inference&#x2F;&quot;,&quot;path&quot;:&quot;2020&#x2F;07&#x2F;30&#x2F;Model Predictive Control&#x2F;Variational Inference&#x2F;&quot;,&quot;title&quot;:&quot;Variational Inference&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Variational Inference | Xiaoxue Zhang - NUS</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Xiaoxue Zhang - NUS</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">NUS Ph.D.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Variational-Inference"><span class="nav-number">1.</span> <span class="nav-text">Variational Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Variational-Mixture-of-Gaussian"><span class="nav-number">1.1.</span> <span class="nav-text">Variational Mixture of Gaussian</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Variational-distrbution"><span class="nav-number">1.1.1.</span> <span class="nav-text">Variational distrbution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Variational-lower-bound"><span class="nav-number">1.1.2.</span> <span class="nav-text">Variational lower bound</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Predictive-density"><span class="nav-number">1.1.3.</span> <span class="nav-text">Predictive density</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Determining-the-number-of-components"><span class="nav-number">1.1.4.</span> <span class="nav-text">Determining the number of components.</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Simplified-description"><span class="nav-number">2.</span> <span class="nav-text">Simplified description</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Description"><span class="nav-number">2.1.</span> <span class="nav-text">Description</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xiaoxue Zhang"
      src="/images/one-piece-1-1.jpg">
  <p class="site-author-name" itemprop="name">Xiaoxue Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Zhang-Xiaoxue" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Zhang-Xiaoxue" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xiaoxuezhang@u.nus.edu" title="E-Mail → mailto:xiaoxuezhang@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/lisnol" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;lisnol" rel="noopener" target="_blank">知乎</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2020/07/30/Model%20Predictive%20Control/Variational%20Inference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/one-piece-1-1.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Variational Inference
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-07-30 12:00:00" itemprop="dateCreated datePublished" datetime="2020-07-30T12:00:00+08:00">2020-07-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-05-13 15:32:02" itemprop="dateModified" datetime="2021-05-13T15:32:02+08:00">2021-05-13</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Model-Predictive-Control/" itemprop="url" rel="index"><span itemprop="name">Model Predictive Control</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Variational-Inference"><a href="#Variational-Inference" class="headerlink" title="Variational Inference"></a>Variational Inference</h1><p>The solution is obtained by exploring all possible input functions to find the one that maximizes, or minimizes, the functional. </p>
<ul>
<li><p>restriction : factorization assumption</p>
</li>
<li><p>log marginal probability<br>$$<br>\ln p(\mathbf X) = \mathcal L(q)+ KL(q||p)<br>$$</p>
<ul>
<li>This differs from our discussion of EM only in that the parameter vector $\theta$ no longer appears, because the parameters are now stochastic variables and are absorbed into $\mathbf Z$.</li>
<li>maximize the lower bound $\mathcal L$ = minimize the KL divergence.</li>
</ul>
</li>
</ul>
<p>In particular, there is no ‘over-fitting’ associated with highly flexible distributions. Using more flexible approximations simply allows us to approach the true posterior distribution more closely.</p>
<ul>
<li>One way to restrict the family of approximating distributions is to use a parametric distribution $q(Z|ω)$ governed by a set of parameters $ω$. The lower bound $\mathcal L(q)$ then becomes a function of $ω$, and we can exploit standard nonlinear optimization techniques to determine the optimal values for the parameters. </li>
</ul>
<p>Method:</p>
<ol>
<li><p>Factorized distribution approximate to true posterior distribution  –&gt;&gt; Mean field theory<br>$$<br>KL(p||q) \quad \iff \quad KL(q||p)<br>$$</p>
<ol>
<li><p>if we consider approximating a multimodal distribution by a unimodal one,</p>
<p>Both types of multimodality were encountered in Gaussian mixtures, where they manifested themselves as<br>multiple maxima in the likelihood function, and a variational treatment based on the minimization of $KL(q||p)$ will tend to find one of these modes.</p>
</li>
<li><p>if we were to minimize $KL(p||q)$, the resulting approximations would average across all of the modes and, in the context of the mixture model, would lead to poor predictive distributions (because the average of two good parameter values is typically itself not a good parameter value). It is possible to make use of $KL(p||q)$ to define a useful inference procedure, but this requires a rather different approach about expectation propagation.</p>
</li>
</ol>
<p>The two forms of KL divergence are members of the <em>alpha family</em> of divergence.</p>
</li>
</ol>
<h2 id="Variational-Mixture-of-Gaussian"><a href="#Variational-Mixture-of-Gaussian" class="headerlink" title="Variational Mixture of Gaussian"></a>Variational Mixture of Gaussian</h2><ul>
<li><p>Likelihood function $p(\mathbf Z| \mathbf \pi) = \prod\limits_{n=1}^N \prod\limits_{k=1}^K \pi_k^{z_{nk}}$</p>
</li>
<li><p>Conditional distribution of observed data $p(\mathbf X|\mathbf Z,\mu, \Lambda) = \prod\limits_{n=1}^N \prod\limits_{k=1}^K \mathcal N(\mathbf x_n|\mu_k,\Lambda_k^{-1})^{z_{nk}}$</p>
</li>
<li><p>Prior distribution: $p(\pi) = \mathrm{Dir}(\pi|\alpha_0)$,  $p(\mu,\Lambda) = p(\mu|\Lambda)p(\Lambda)=\prod\limits_{k=1}^N \mathcal N(\mu_k | m_0, (\beta_0\Lambda_k)^{-1}) \mathcal W(\Lambda_k|\mathbf W_0, \nu_0)$</p>
<p>Variables such as $\mathbb z_n$ that appear inside the plate are regarded as latent variables because the number of such variables grows with the size of the data set. By contrast, variables such as μ that are outside the plate are fixed in number independently of the size of the data set, and so are regarded as parameters</p>
</li>
</ul>
<h3 id="Variational-distrbution"><a href="#Variational-distrbution" class="headerlink" title="Variational distrbution"></a>Variational distrbution</h3><p>Joint distribution<br>$$<br>p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})=p(\mathbf{X} | \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) p(\mathbf{Z} | \boldsymbol{\pi}) p(\boldsymbol{\pi}) p(\boldsymbol{\mu} | \boldsymbol{\Lambda}) p(\boldsymbol{\Lambda})<br>$$<br>Variational distribution (Assumption)<br>$$<br>q(\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) = q(\mathbf{Z})q(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})<br>$$<br>==Variational EM==</p>
<p>This effect can be understood qualitatively in terms of the automatic trade-off in a Bayesian model between fitting the data and the complexity of the model, in which the complexity penalty arises from components whose parameters are pushed away from their prior values. </p>
<p>Thus the optimization of the variational posterior distribution involves cycling between two stages analogous to the E and M steps of the maximum likelihood EM algorithm. </p>
<ul>
<li><p>Variational E-step</p>
<p>use the current distributions over the model parameters to evaluate the moments in (10.64), (10.65), and (10.66)<br>and hence evaluate $\mathbb E[z_{nk}] = r_{nk}$.</p>
</li>
<li><p>Variational M-step</p>
<p>keep these responsibilities fixed and use them to re-compute the variational distribution over the parameters using (10.57) and (10.59). In each case, we see that the variational posterior distribution has the same functional form as the corresponding factor in the joint distribution (10.41). This is a general result and is a consequence of the choice of conjugate distributions.</p>
</li>
</ul>
<p>==variational EM v.s. EM for maximum likelihood.==</p>
<ul>
<li>In fact if we consider the limit $N\rightarrow \infty$ then the Bayesian treatment converges to the maximum likelihood EM algorithm. </li>
<li>For anything other than very small data sets, the dominant computational cost of the variational algorithm for Gaussian mixturesa rises from the evaluation of the responsibilities, together with the evaluation and inversion of the weighted data covariance matrices. These computations mirror precisely those that arise in the maximum likelihood EM algorithm, and so there is little computational overhead in using this Bayesian approach as compared to the traditional maximum likelihood one. </li>
<li>The singularities that arise in maximum likelihood when a Gaussian component ‘collapses’ onto a specific data point are absent in the Bayesian treatment. Indeed, these singularities are removed if we simply introduce a prior and then use a MAP estimate instead of maximum likelihood. </li>
<li>Furthermore, there is no over-fitting if we choose a large number $K$ of components in the mixture. </li>
<li>Finally, the variational treatment opens up the possibility of determining the optimal number of components in the mixture without resorting to techniques such as cross validation.</li>
</ul>
<h3 id="Variational-lower-bound"><a href="#Variational-lower-bound" class="headerlink" title="Variational lower bound"></a>Variational lower bound</h3><p>Function:</p>
<ul>
<li>to monitor the bound during the re-estimation in order to test for convergence</li>
<li>provide a valuable check on both the mathematical expressions for the solutions and their software implementation, because at each step of the iterative re-estimation procedure the value of this bound should not decrease.</li>
<li>provide a deeper test of the correctness of both the mathematical derivation of the update equations and of their software implementation by using finite differences to check that each update does indeed give a (constrained) maximum of the bound</li>
</ul>
<p>lower bound provides an alternative approach for deriving the variational re-estimation equations in variational EM. To do<br>this we use the fact that, since the model has conjugate priors, the functional form of the factors in the variational posterior distribution is known, namely discrete for $\mathbf Z$, Dirichlet for $π$, and Gaussian-Wishart for $(μ_k,Λ_k)$. By taking general parametric forms for these distributions we can derive the form of the lower bound as a function of the parameters of the distributions. Maximizing the bound with respect to these  parameters then gives the required re-estimation equations.</p>
<h3 id="Predictive-density"><a href="#Predictive-density" class="headerlink" title="Predictive density"></a>Predictive density</h3><p>predictive density for a new value $\hat x$ of the observed variable.  corresponding to latent variable $\hat{\mathbf z}$.<br>$$<br>p(\widehat{\mathbf{x}} | \mathbf{X})=\sum_{\widehat{\mathbf{z}}} \iiint p(\widehat{\mathbf{x}} | \widehat{\mathbf{z}}, \mu, \Lambda) p(\widehat{\mathbf{z}} | \boldsymbol{\pi}) p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda} | \mathbf{X}) \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu} \mathrm{d} \boldsymbol{\Lambda} \<br>= \sum_{k=1}^K  \iiint \pi_k \mathcal N(\hat{\mathbf x} |\mu_k,\Lambda_k) q(\pi) q(\mu_k, \Lambda_k) d\pi d\mu_k d \Lambda_k<br>$$<br>remaining integrations can now be evaluated analytically giving a mixture of Student’s t-distributions:<br>$$<br>\begin{aligned}<br>p(\widehat{\mathbf{x}} | \mathbf{X})&amp;= \frac{1}{\widehat{\alpha}} \sum_{k=1}^{K} \alpha_{k} \operatorname{St}\left(\widehat{\mathbf{x}} | \mathbf{m}<em>{k}, \mathbf{L}</em>{k}, \nu_{k}+1-D\right) \<br>\qquad \mathbf{L}<em>{k}&amp;=\frac{\left(\nu</em>{k}+1-D\right) \beta_{k}}{\left(1+\beta_{k}\right)} \mathbf{W}_{k}<br>\end{aligned}<br>$$<br>where $\mathbf m_k$ is the mean of $k$th component.</p>
<h3 id="Determining-the-number-of-components"><a href="#Determining-the-number-of-components" class="headerlink" title="Determining the number of components."></a>Determining the number of components.</h3><p>variational lower bound can be used to determine a posterior distribution over the number $K$ of components in the mixture model.</p>
<p>If we have a mixture model comprising $K$ components, then each parameter setting will be a member of a family of $K!$ equivalent settings.</p>
<ul>
<li>in EM, this is irrelevant because the parameter optimization algorithm will, depending on the initialization<br>of the parameters, find one specific solution, and the other equivalent solutions play no role.</li>
<li>in Variational EM, we marginalize over all possible parameter values.</li>
<li>if the true posterior distribution is multimodal, variational inference based on the minimization of $KL(q||p)$ will tend to approximate the distribution in the neighborhood of one of the modes and ignore the others.</li>
<li>If we want to compare different value of $K$, we need to consider the multimodality.</li>
</ul>
<p>Solution: </p>
<ol>
<li>to add a term $\ln K!$ onto the lower bound when used for model comparison and averaging.</li>
<li>treat the mixing coefficients $π$ as parameters and make point estimates of their values by maximizing the lower bound with respect to $π$ instead of maintaining a probability distribution over them as in the fully Bayesian approach.<ul>
<li>This leads to the re-estimation equation and <u>this maximization is interleaved with the variational updates for the $q$ distribution over the remaining parameters</u>. </li>
<li>Components that provide insufficient contribution to explaining the data will have their mixing coefficients driven to zero during the optimization, and so they are effectively removed from the model through automatic relevance determination. This allows us to make a single training run in which we start with a relatively large initial value of K, and allow surplus components to be pruned out of the model. </li>
</ul>
</li>
</ol>
<p>==Comparison:==</p>
<ul>
<li>maximum likelihood would lead to values of the likelihood function that increase monotonically with K (assuming the singular solutions have been avoided, and discounting the effects of local maxima) and so cannot be used to determine an appropriate model complexity. </li>
<li>By contrast, Bayesian inference automatically makes the trade-off between model complexity and fitting the data.</li>
</ul>
<blockquote>
<p>maximum likelihood would lead to values of the likelihood function that increase monotonically with K (assuming the singular solutions have been avoided, and discounting the effects of local maxima) and so cannot be used to determine an appropriate model complexity. </p>
<p>By contrast, Bayesian inference automatically makes the trade-off between model complexity and fitting the data.</p>
</blockquote>
<h1 id="Simplified-description"><a href="#Simplified-description" class="headerlink" title="Simplified description"></a>Simplified description</h1><blockquote>
<p> Reference:  <a href="https://link.zhihu.com/?target=https://www.cnblogs.com/yifdu25/p/8181185.html">变分推断（Variational Inference）</a></p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38740118">https://zhuanlan.zhihu.com/p/38740118</a> </p>
</blockquote>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>based on input data to compute a distribution $P(x)$.</p>
<p>Idea: Use Gaussian distribution can function as <u>approximate distribution</u> to simulate the <u>real distribution</u> </p>
<p>&lt;用已知的分布模拟未知的分布&gt;</p>
<p>Parameters:</p>
<ul>
<li>input: $x$</li>
<li>hidden variable: $z$<ul>
<li>is parameter in real distribution. For GMM, $z$ is the mean and covariance of all gaussian distribution.</li>
</ul>
</li>
<li>Posterior distribution $P(z|x)$:<ul>
<li>using the data to guess the model. &lt;用输入去推断隐含变量&gt;</li>
</ul>
</li>
<li>likelihood distribution $P(x|z)$:<ul>
<li>distribution based on the model parameter.</li>
</ul>
</li>
<li>distribution of input $P(x)$</li>
<li>Prior distribution $q(z,v)$:<ul>
<li>the distribution to simulate the real distribution. $z$ is hidden variables, $v$ is the parameter of distribution about $z$, i.e., hyperparameter.</li>
<li>normally, the distribution about $z$ is conjugate distribution.  </li>
</ul>
</li>
</ul>
<p>Task:</p>
<p>adjust $v$ to make the approximate distribution close to the posterior distribution $p(z|x)$</p>
<img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/v2-ef06fbce0bb4818363ed2010fc357fdf_r.jpg" style="zoom: 60%">

<p>Hope the posterior distribution and the prior distribution closed to 0. But the posterior distribution is unknown. Therefore, introduce the KL divergence and ELBO.<br>$$<br>\log p(x) = KL(q(z;v) || p(z|x)) + ELBO(v)<br>$$<br>because $\log p(x)$ is constant, the minimum KL divergence is the maximum ELBO.</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Xiaoxue Zhang
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://zhang-xiaoxue.github.io/2020/07/30/Model%20Predictive%20Control/Variational%20Inference/" title="Variational Inference">https://zhang-xiaoxue.github.io/2020/07/30/Model Predictive Control/Variational Inference/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/06/30/Chance%20Constraints/Chance%20Constrained%20Problem/" rel="prev" title="Chance Constrained Problem">
                  <i class="fa fa-chevron-left"></i> Chance Constrained Problem
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/07/30/Model%20Predictive%20Control/Probabilistic%20Trajectory%20Prediction%20with%20Gaussian%20Mixture%20Models/" rel="next" title="Probabilistic Trajectory Prediction with Gaussian Mixture Models">
                  Probabilistic Trajectory Prediction with Gaussian Mixture Models <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoxue Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="199" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;single_dollars&quot;:true,&quot;cjk_width&quot;:0.9,&quot;normal_width&quot;:0.6,&quot;append_css&quot;:true,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
