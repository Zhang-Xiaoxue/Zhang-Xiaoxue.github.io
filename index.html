<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/Academy-favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/Academy-favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/Academy-favicon.png">
  <link rel="mask-icon" href="/images/Academy-favicon" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;zhang-xiaoxue.github.io&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;right&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;Searching...&quot;,&quot;empty&quot;:&quot;We didn&#39;t find any results for the search: ${query}&quot;,&quot;hits_time&quot;:&quot;${hits} results found in ${time} ms&quot;,&quot;hits&quot;:&quot;${hits} results found&quot;}}</script>
<meta property="og:type" content="website">
<meta property="og:title" content="Xiaoxue Zhang - NUS">
<meta property="og:url" content="https://zhang-xiaoxue.github.io/index.html">
<meta property="og:site_name" content="Xiaoxue Zhang - NUS">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Xiaoxue Zhang">
<meta property="article:tag" content="Reinforcement Learning, Optimization and Control, Intellegent Systems">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zhang-xiaoxue.github.io/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:true,&quot;isPost&quot;:false,&quot;lang&quot;:&quot;en&quot;,&quot;comments&quot;:&quot;&quot;,&quot;permalink&quot;:&quot;&quot;,&quot;path&quot;:&quot;index.html&quot;,&quot;title&quot;:&quot;&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Xiaoxue Zhang - NUS</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Xiaoxue Zhang - NUS</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">NUS Ph.D.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xiaoxue Zhang"
      src="/images/photo_blue.jpg">
  <p class="site-author-name" itemprop="name">Xiaoxue Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Zhang-Xiaoxue" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Zhang-Xiaoxue" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xiaoxuezhang@u.nus.edu" title="E-Mail → mailto:xiaoxuezhang@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.linkedin.com/in/xiaoxue-zhang-5233b611a/" title="https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;xiaoxue-zhang-5233b611a&#x2F;" rel="noopener" target="_blank">Linkedin</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/lisnol" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;lisnol" rel="noopener" target="_blank">知乎</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Zhang-Xiaoxue" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/0_Supplementary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/0_Supplementary/" class="post-title-link" itemprop="url">3. Convex Analysis</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 14:59:49" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="3-Convex-Analysis"><a href="#3-Convex-Analysis" class="headerlink" title="3. Convex Analysis"></a>3. Convex Analysis</h1><h2 id="3-1-Prove-convex-set"><a href="#3-1-Prove-convex-set" class="headerlink" title="3.1 Prove convex set:"></a><strong>3.1 Prove convex set:</strong></h2><ol>
<li>$x,y\in C$, then prove $\lambda x + (1-\lambda y) \in C$</li>
<li>intersection of convex sets are convex</li>
<li>$f$ is convex  $\iff$  epigraph set is convex $f(x)\le \alpha$</li>
</ol>
<h2 id="3-2-Prove-convex-function"><a href="#3-2-Prove-convex-function" class="headerlink" title="3.2 Prove convex function:"></a><strong>3.2 Prove convex function:</strong></h2><ol>
<li>Prove $f(\lambda x + (1-\lambda)y) \le \lambda f(x) + (1-\lambda) y$</li>
<li>(sum of) linear combination of convex function is convex</li>
<li>$h$ convex, $g$ non-decreasing convex, $g\circ h$ is convex</li>
<li>epigraph set is convex $f(x)\le \alpha$ $\iff$ $f$ is convex </li>
<li>convex, then $x=\sum_{k=1}^k \lambda_jx^{j}$, where $\sum_{j=1}^k \lambda_j =1$, we have $f(x)\le \sum_{k=1}^k \lambda_j f(x^j)$ <strong>Jensen’s inequality:</strong> </li>
<li>convex $\iff$   $f(y) \le f(x)+\nabla f(x)^T (y-x)$ <strong>Tangent plane characterization:</strong> </li>
<li>convex $\iff$  $\langle \nabla f(\mathbb y)-\nabla f(\mathbb x), \mathbb y - \mathbb x \rangle \ge 0, \quad \forall \mathbb x, \mathbb y \in S$   monotone gradient condition</li>
<li>convex $\iff$  Hessian is positive semidefinite</li>
</ol>
<p>For constrained convex programming $\min{f(x)|x\in C}$, $x^*$ is global minimizer $\iff$ $\langle \triangledown f(x^*), x-x^* \rangle &gt; 0, \forall x \in C$.</p>
<h2 id="3-3-Projection-Pi-C-z-mathrm-argmin-frac-1-2-x-z-2-vert-x-in-C"><a href="#3-3-Projection-Pi-C-z-mathrm-argmin-frac-1-2-x-z-2-vert-x-in-C" class="headerlink" title="3.3 Projection: $\Pi_C(z) = \mathrm{argmin} { \frac{1}{2} |x-z|^2 \vert x\in C }$"></a><strong>3.3 Projection:</strong> $\Pi_C(z) = \mathrm{argmin} { \frac{1}{2} |x-z|^2 \vert x\in C }$</h2><ul>
<li>$x^* = \Pi_C(z)$  $\iff$  $\langle z-x^*, x-x^* \rangle \leq 0, \forall x\in C$</li>
<li>$| \Pi_C(z) - \Pi_C(w) | \leq |z-w |$. </li>
</ul>
<blockquote>
<ul>
<li>C is linear subspace, $z-x^*\perp C$, then $z = \Pi_C(z) + (z-\Pi_C(z)), \quad \text{and } \quad \langle z-\Pi_C(z), \Pi_C(z)\rangle = 0$</li>
<li>C is closed convex cone, $\langle z-\Pi_C(z), \Pi_C(z)\rangle = 0$</li>
<li>If $\theta (x) = \frac{1}{2} | x-\Pi_C(x)|^2$, then $\theta(\cdot)$ is a continuously differentiable convex function and $\nabla \theta(x) = x-\Pi_C(x)$</li>
</ul>
</blockquote>
<h2 id="3-4-Cones"><a href="#3-4-Cones" class="headerlink" title="3.4 Cones:"></a><strong>3.4 Cones:</strong></h2><ul>
<li>Dual cone: $S^* = { y\in S | \langle x,y \rangle \ge 0, \forall x\in S }$. Self-dual cone $C^*=C$, including $\mathbb S_+^n$, $\mathbb R^n$,</li>
<li>Polar cone: $S^\circ = -S^*$</li>
<li>$(C^*)^*=C$.</li>
<li>Spectral cone $\mathcal K^*$, Nuclear-norm cone $\mathcal C$, $\mathrm {COP}_n$, $\mathrm {CPP}_n$, double nonnegative cone $\mathbb D_n$. [Chapter 3, P79]</li>
<li>$z-\Pi_C(z) = \Pi_{C^\circ}(z)$, if $C$ is closed convex cone.</li>
<li><strong>Normal cone</strong>:<ul>
<li> $N_C(\bar x) = {z\in \mathcal E | \langle z, x-\bar x \rangle \leq 0, \forall x\in C}$ where $\bar x \in C$.</li>
<li>$u\in N_C(y) \iff y=\Pi_C(y+u)$</li>
<li>$N_{C_1} (\bar x) + N_{C_2} (\bar x) \subset N_{C_1\cap C_2} (\bar x)$ where $\bar x \in C_1 \cap C_2$.</li>
<li>$C=\mathcal S^n_+$. $X=U_1D_1U_1^T, D_1 = \mathrm{diag}(d_1, \cdots, d_r)$ with descending order. $N_C(X)={-U_2\Delta U_2^T}$. [P81]</li>
</ul>
</li>
</ul>
<h2 id="3-5-Subgradient"><a href="#3-5-Subgradient" class="headerlink" title="3.5 Subgradient:"></a><strong>3.5 Subgradient:</strong></h2><ul>
<li><p>$\langle v-u, y-x \rangle \ge 0, \quad \forall u\in \part f(x), v\in \part f(y)$   Monotone mapping</p>
</li>
<li><p>$\part \delta_C(x) = \begin{cases}<br>\empty \quad &amp; \text{if }x\notin C \<br>\mathcal N_C(x)= \text{normal cone of $C$ at $x$} \quad &amp; \text{if }x\in C<br>\end{cases}$  where $C$ is convex subset of $\mathbb R^n$</p>
</li>
<li><p>$\part f(0) = { y\in \mathbb R^n | |y|_\infty \le 1 }$, if $f(x) = | x |_1$   [P85]</p>
</li>
</ul>
<h2 id="3-6-Frenchel-Conjugate"><a href="#3-6-Frenchel-Conjugate" class="headerlink" title="3.6 Frenchel Conjugate:"></a>3.6 Frenchel Conjugate:</h2><p>$$<br>f^*(y) = \sup { \langle y,x \rangle - f(x) | x\in \mathcal E }, y\in \mathcal \ E<br>$$</p>
<ul>
<li><p>$f^*$ is always closed and convex, even if $f$ is non-convex or not closed.</p>
</li>
<li><p>Equivalent condition</p>
<ol>
<li>$f(x) + f^*(x^*) = \langle x, x^* \rangle$</li>
<li>$x^* \in \part f(x)$</li>
<li>$x \in \part f^*(x^*)$</li>
<li>$\langle x, x^* \rangle - f(x) =\max_{z\in \mathcal E} {\langle z, x^* \rangle - f(z)}$</li>
<li>$\langle x, x^* \rangle - f^*(x^*) =\max_{z^<em>\in \mathcal E} {\langle x, z^</em> \rangle - f^*(z^*)}$</li>
<li>$(f^*)^* = f$</li>
</ol>
</li>
</ul>
<h2 id="3-7-Moreau-Yosida-regularization-and-proximal-mapping"><a href="#3-7-Moreau-Yosida-regularization-and-proximal-mapping" class="headerlink" title="3.7 Moreau-Yosida regularization and proximal mapping"></a>3.7 Moreau-Yosida regularization and proximal mapping</h2><p>$$<br>\begin{array}{ll}{\text { MY regularization of } f \text { at } x:} &amp; {M_{f}(x)=\min <em>{y \in \mathcal{E}}\left{\phi(y ; x):=f(y)+\frac{1}{2}|y-x|^{2}\right}} \ {\text { Proximal mapping } f \text { at } x:} &amp; {P</em>{f}(x)=\operatorname{argmin}_{y \in \mathcal{E}}{\phi(y ; x):=f(y)+\frac{1}{2}|y-x|^{2}}}\end{array}<br>$$</p>
<ul>
<li> $x=P_f(x)+P_{f^*}(x), \forall x\in \mathcal E$</li>
<li>$f(x)=\lambda |x|_1$, $f^*(z)=\delta_C(z)$ where $C=\part f(0)={z\in \mathbb R^n | |z|_\infty \le \lambda }$   [P91]</li>
<li> If  $f=\delta_C$, $P_{\delta_{C}}(x)=\operatorname{argmin}<em>{y \in \mathcal{E}}\left{\delta</em>{C}(y)+\frac{1}{2}|y-x|\right}=\operatorname{argmin}<em>{y \in C} \frac{1}{2}|y-x|^{2}=\Pi</em>{C}(x)$.  [P92]</li>
<li>if $C=\mathbb S_+^n$, Then $\Pi_C(x)=Q\mathrm{diag}d_+ Q^T \quad \text{using spectral decomposition } x=Q \mathrm{diag}dQ^T$ </li>
</ul>
<h1 id="4-Gradient-Methods"><a href="#4-Gradient-Methods" class="headerlink" title="4. Gradient Methods"></a>4. Gradient Methods</h1><p>Descent direction: direction $\mathbf d$ such that $\langle \nabla f(x^*), d\rangle &lt;0 $.</p>
<p>Steepest decent direction: direction $\mathbf d$ such that $\mathbf d^* = - \nabla f(x^*)$.</p>
<ol>
<li><p><strong>Steepest descent method with exact line search:</strong> + Convergence rate</p>
</li>
<li><p><strong>Accelerated Proximal Gradient method:</strong> + Error, complexity </p>
<ol>
<li>Sparse regression [P112]</li>
<li><u>Projection on to convex cone</u> $\mathrm{DNN}<em>n^* = \mathbb S</em>+^n + \mathcal N^n$  [P112]</li>
</ol>
</li>
<li><p>Gradient Projection Method:</p>
<p>$x^{k+1} = P_Q(x^k - \alpha_k \nabla f(x^k))$, where $x(\alpha) = x-\alpha \nabla f(x)$, $x_Q(\alpha) = P_!(x(\alpha))$</p>
</li>
<li><p>Stochastic Gradient Descent Method</p>
</li>
</ol>
<h1 id="5-Basic-NLP-KKT"><a href="#5-Basic-NLP-KKT" class="headerlink" title="5. Basic NLP (KKT)"></a>5. Basic NLP (KKT)</h1><p>(LICQ): Linear Independency Constraint Qualification : regular point : $\nabla h_i$ and $\nabla g_i$ are linear independent.</p>
<p><strong>KKT necessary condition:</strong> </p>
<ul>
<li><p>1st order:</p>
<p> $\begin{array}{l}{[1] \quad \nabla f\left(\mathrm{x}^{<em>}\right)+\sum_{i=1}^{m} \lambda_{i}^{</em>} \nabla g_{i}\left(\mathrm{x}^{<em>}\right)+\sum_{j=1}^{p} \mu_{j}^{</em>} \nabla h_{j}\left(\mathrm{x}^{<em>}\right)=0} \ {[2] \quad g_{i}\left(\mathrm{x}^{</em>}\right)=0, \quad \forall i=1,2, \cdots, m} \ {[3] \quad \mu_{j}^{<em>} \geq 0, h_{j}\left(\mathrm{x}^{</em>}\right) \leq 0, \quad \mu_{j}^{<em>} h_{j}\left(\mathrm{x}^{</em>}\right)=0, \quad \forall j=1,2, \cdots, p}\end{array}$</p>
</li>
<li><p>2nd order: $y^T H_L(x^*) y \ge 0$ where $y\in T(x^*)$ tangent plane</p>
<ul>
<li>Easier check: $Z(x^*)H_L(x^*)Z(x^*)$ is positive semidefinite</li>
</ul>
</li>
</ul>
<p><strong>Steps:</strong></p>
<ol>
<li>Check regularity condition</li>
<li>verify KKT first order necessary condition.</li>
<li>verify KKT second order necessary condition<ol>
<li>Hessian matrix</li>
<li>basis of null space of $\mathcal D(\mathbb x^*)^T$</li>
<li>definiteness check</li>
</ol>
</li>
</ol>
<p><strong>Projection onto simplex:</strong> [P139-140]</p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8iaab2yg8j31ae0kqwjo.jpg" style="zoom: 45%">

<p><em>relax multipliers</em></p>
<p><strong>KKT sufficient conditions</strong>: 1st + 2nd</p>
<p><strong>steps:</strong></p>
<ol>
<li>verify it’s KKT point. </li>
<li>verify it satisfy the second order sufficient condition<ol>
<li>Hessian matrix</li>
<li>$Z(\mathbf x^*)$</li>
<li>Determine definiteness</li>
</ol>
</li>
</ol>
<p><strong>Important points:</strong></p>
<ol>
<li><p><u><strong>KKT point is an optimal solution under convexity</strong></u>, <strong>but a global minimizer of a convex program may not be a KKT point.</strong></p>
</li>
<li><p><strong><u>With regularity condition</u></strong>, a global minimizer is a KKT point. For convex programming problem with at least one inequality constraints, the Slater’s condition ensures that a global minimizer is a KKT point.</p>
<p><strong><u>slater’s condition</u>:</strong> there exists $\hat{\mathbb x} ∈ \mathbb R^n$ such that $g_i(\hat{\mathbb x}) = 0, ∀ i = 1,\cdots, m$ and $h_j(\hat{\mathbb x}) &lt; 0, ∀ j = 1, . . . , p$.</p>
<pre><code> Slater&#39;s condition states that the feasible region must have an [interior point](https://en.wikipedia.org/wiki/Interior_(topology)).
</code></pre>
</li>
<li><p>Linear equality constrained problem: </p>
<p>a point $x^∗ ∈ S$ is a KKT point $\iff$  $x^∗$ is a global minimizer of $f$ on $S$.</p>
<p>No regularity or slater condition is needed.</p>
</li>
<li><p>LInear + convex quadratic programming.</p>
</li>
</ol>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8iaoz35m8j316u0qg7b3.jpg" style="zoom:40%">

<h1 id="6-Lagrangian-duality"><a href="#6-Lagrangian-duality" class="headerlink" title="6. Lagrangian duality"></a>6. Lagrangian duality</h1><p><u>optimal value of dual problem is smaller than optimal value of primal problem.</u>  Duality gap;</p>
<p><strong>Saddle point optimality $\longleftrightarrow$ KKT</strong></p>
<ol>
<li>For a fixed $\mathbb x^∗$, $(λ^∗, μ^∗)$ maximizes $L(\mathbb x^∗, λ, μ)$ over all $(λ, μ)$ with $μ ≥ 0$.</li>
<li>For a fixed $(λ^∗, μ^∗)$ , $\mathbb{x}^*$ minimizes $L(\mathbb x, λ^∗, μ^∗)$ over all $x ∈ X$.</li>
<li>saddle point $(\mathbb x^*, \lambda^*,\mu^*)$ $\Rightarrow$ KKT condition. but inverse is not true in general.</li>
<li>KKT point of a convex program is a saddle point.</li>
</ol>
<h1 id="7-Nonlinear-conic-programming"><a href="#7-Nonlinear-conic-programming" class="headerlink" title="7. Nonlinear conic programming"></a>7. Nonlinear conic programming</h1><p>​                        <img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ibxikl2ij30gk05674m.jpg" style="zoom:40%">         <img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8iby7vro4j30ju030q35.jpg" style="zoom: 40%"></p>
<h1 id="Sparse-regression-problem"><a href="#Sparse-regression-problem" class="headerlink" title="Sparse regression problem:"></a>Sparse regression problem:</h1><ul>
<li><p>objective function is convex</p>
</li>
<li><p>APG: [P112]</p>
<img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/20191031234729.png" style="zoom:35%"></li>
<li><p>KKT: [P153-154]</p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ialuccwmj31b0070gmo.jpg" style="zoom: 32%">

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8iamsirn8j30u00wajzd.jpg" style="zoom:50%"></li>
</ul>
<ol start="2">
<li><p>Chapter 7. conic programming</p>
</li>
<li><p>Chapter 8. sPADMM</p>
<p>P214</p>
</li>
<li></li>
</ol>
<img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/20191031183333.png" style="zoom: 60%">



<h2 id="Indicator-function"><a href="#Indicator-function" class="headerlink" title="Indicator function:"></a>Indicator function:</h2><ul>
<li>Indicator function: $\delta_C(x) = \begin{cases} 0, &amp; x\in C \ \infty, &amp; x\notin C \end{cases}$</li>
</ul>
<p>$$<br>\delta_{S_+^n}^* = \delta_{-S_+^n}<br>$$</p>
<p>$$<br>\mathrm{Prof}_f(x) = (I+\lambda \part f)^{-1}<br>$$</p>
<p>$P_f(x) = x-P_{f^*}(x)$</p>
<p>Proximal $P_f$ and Projection $\Pi_C(f(x))$ : When $f=\delta$ indicator function, the two operator are equivalent.</p>
<p> $f(x) = \lambda |x|_1 $,  then $f^*(z) = \delta_C(z)$ where $C=\part f(0) = {z\in\mathbb R^n | |z|_\infty \le \lambda }$, the proximal function $P_f(x) = \mathrm {sgn} (x) \circ \max{|x|-\lambda }$  [P91-92]</p>
<p>$f=\delta_C$, $C$ is closed and convex, the proximal and projection operator are equivalent, because $P_{\delta_{C}}(x)=\operatorname{argmin}<em>{y \in \mathcal{E}}\left{\delta</em>{C}(y)+\frac{1}{2}|y-x|\right}=\operatorname{argmin}<em>{y \in C} \frac{1}{2}|y-x|^{2}=\Pi</em>{C}(x)$. When $C=\mathbb S_+^n$, then $\Pi_C(x)=Q \mathrm {diag}(d_+) Q^T$ using spectral decomposition $x=Q\mathrm {diag}(d) Q^T$</p>
<p>$f(x)=|x|_*$, then the $P_f(x) = V\Sigma^+ U^T$.</p>
<p>$x^* = P_f(x^*)$ will converge to the optimum of $f$, i.e., $x^*$. </p>
<p>Dual norm: [P88]</p>
<ul>
<li>$f(x)=\lambda |x|<em>1$, then $f^*(x) = \delta</em>{B_\lambda}$, where $B_\lambda = {x\in \mathbb R^n | |x|_\infty\leq \lambda}$. </li>
<li>$f(x)=\lambda |x|<em>\infty$, then $f^*(x) = \delta</em>{B_\lambda}$, where $B_\lambda = {x\in \mathbb R^n | |x|_1\leq \lambda}$. </li>
<li>$f(x)=\lambda |x|<em>m$, then $f^*(x) = \delta</em>{B_\lambda}$, where $B_\lambda = {x\in \mathbb R^n | |x|_n\leq \lambda}$, $\frac{1}{m}+\frac{1}{n}=1$, called dual norm. </li>
<li>$\ell_2$ norm is self-dual norm. </li>
</ul>
<p>$y\in N_C(x)$, then $y\in \Pi_C(x+y)$</p>
<p>$x=P_f(x)+P_{f^*}(x), \forall x\in \mathcal E$</p>
<p>$G = \Pi_{\mathbb S_+^*}(G) - \Pi_{\mathbb S_+^n}(-G)$</p>
<p>$\min{ \frac{1}{2}|G+Z|^2 | Z\in \mathbb S_+^n} = \frac{1}{2} | G+\Pi_{\mathbb S^n_+}(G)|^2 = \frac{1}{2} |\Pi_{\mathbb S_+^n}(G) |^2$</p>
<ol>
<li>$\mathbb x \in S \iff \mathbb x = \mathbb x^* + \mathbb z$ for some $\mathbb z \in \operatorname{Null}(A)$.  【space of x】</li>
<li>$\mathbb y^T \mathbb z = 0$, $\forall \mathbb z \in \operatorname{Null}(\mathbf A) \iff \exist \lambda^* \in \mathbb R^m$ s.t. $\mathbb y+ \mathbf A^T \lambda^*=0$.</li>
</ol>
<p>The linear map $\mathcal A(X) = \begin{bmatrix} \langle A_1, X \rangle \ \vdots \  \langle A_m, X \rangle  \end{bmatrix}$. Let $\mathcal A^* $ be adjoint of $\mathcal A$ as $\mathcal A^* y = \sum\limits_{k=1}^m y_kA_k, y\in \mathbb R^m$.<br>$$<br>\langle \mathcal A^* y, X \rangle = \langle y, \mathcal A X \rangle<br>$$<br>if $\mathcal A(X) = \operatorname{diag} (X)$, then $A_k = E_{kk}$, and $\mathcal A^* y =  \operatorname{diag} (y)$.</p>
<h1 id="Dual-problems"><a href="#Dual-problems" class="headerlink" title="Dual problems:"></a>Dual problems:</h1><ul>
<li><p><strong>Linear programs + dual problem:</strong></p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8iauh86t5j313m0ak0uu.jpg" style="zoom: 40%">

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8iavh73xej313j0u0q8k.jpg" style="zoom:40%"></li>
<li><p><strong>Quadratic program + dual problem</strong></p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ibfziambj312j0u0tgd.jpg" style="zoom:40%"></li>
<li><p><strong>Square function + Linear inequality constraints  [P177]</strong></p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ibpeyfj2j31800u0gv5.jpg" style="zoom:40%"></li>
<li><p><strong>NLP + dual problem [P178,180]</strong></p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ic4fs783j31f20esmzw.jpg" style="zoom:39%">

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ic3knxdvj31e00ay0va.jpg" style="zoom: 40%"></li>
<li><p><strong>SDP + Dual Problem [P178, P180]</strong></p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8icadeavgj31f60b8wfs.jpg" style="zoom:38.5%">

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8iciivhccj31dc0u044k.jpg" style="zoom:40%"></li>
<li><p><strong>SDPLS + dual problem [P179, ]</strong></p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8icbl7sasj31es0b8q5k.jpg" style="zoom: 38%">

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8ice24l9uj31fc0cm0v3.jpg" style="zoom:37.5%">

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8icf6y0ktj30wv0u0ahy.jpg" style="zoom:57%">

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8icg6z4mnj31eo0aa763.jpg" style="zoom:37%"></li>
<li><p><strong>Quadratic Semidefinite problem + Dual problem</strong></p>
<img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/20191031184341.png" style="zoom: 60%"></li>
<li><p><strong>Convex quadratic composite optimization problem + dual problem:</strong></p>
<img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/20191031184559.png" style="zoom:60%"></li>
<li><p>d</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/2_Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/2_Introduction/" class="post-title-link" itemprop="url">2. Introduction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:00:12" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>in general, Hessian $H_f(\mathbb x)$ is not symmetric. But, if $f$ has continuous second order derivatives, then the Hessian matrix $H_f( \mathbb x)$ is symmetric.</p>
<p>Positive semidefinite: if $\mathbb x^T \mathbf A \mathbb x \ge 0, \forall \mathbb x \in \mathbb R^n$.</p>
<p>positive definite: if $\mathbb x^T \mathbf A \mathbb x \ge 0, \forall \mathbb x  \neq 0$.</p>
<p>if $\mathbf A$ is a real symmetric $n\times n$ matrix.</p>
<ul>
<li>if $\mathbf A$ is positive semidefinite $\iff$ every eigenvalue of $A$ is nonnegative $\lambda &gt; 0$.</li>
<li>$\lambda_{\min} (\mathbf A) | \mathbb x |^2 \leq \langle \mathbb x, \mathbf A \mathbb x\rangle \leq \lambda_{\max} (\mathbf A) |\mathbb x |^2, \  \forall \mathbb x \in \mathbb R^{n\times n}$</li>
</ul>
<p>Inner product of matrix:</p>
<ul>
<li>$A,B \in \mathbb R^{m\times n}$, have $\langle A,B \rangle = \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{n} A_{ij} B_{ij} = \operatorname{Tr}(A^TB)$</li>
</ul>
<p>Frobenius norm: </p>
<ul>
<li>$|A|<em>F = \sqrt{ \sum\limits</em>{i=1}^{m} \sum\limits_{j=1}^{n} |A_{ij}|^2} = \sqrt{\operatorname{Tr}(A^TA)}$</li>
</ul>
<p><strong>Sherman-Morrison-Woodbury (SMW) formula</strong> </p>
<p>Suppose $U,V \in \mathbb R^{n\times p}$ and $| UV^T | &lt;1$, Then<br>$$<br>(I+UV^T)^{-1} = I-UG^{-1} V^T \quad \in \mathbb R^{n\times n}<br>$$<br>where $G=I_p + V^TU \in \mathbb R^{p\times p}$</p>
<blockquote>
<p>When $n$  is large, it’s hard to compute the inverse of $(I+UV^T)$. But if using the SMW formula, it’s easy to compute by transforming it as $I-UG^{-1} V^T$, because the inverse of $G$ is easy to compute ($p$ is small).</p>
</blockquote>
<p>If $(I+UV^T)$ is nonsingular, than the SMW formula $(I+UV^T)^{-1} = I-UG^{-1} V^T$holds <u>without</u> requiring $| UV^T | &lt;1$</p>
<p>Bounded: $|\mathbb x | \leq M, \forall \mathbb x \in S$. If $C_1, C_2$ are bounded, then $C_1 \cup C_2$ is bounded. </p>
<p>Closed: $\lim\limits_{n\rightarrow \infin} \mathbb x_n \in S$. If $C_1, C_2$ are closed, then $C_1\cap C_2$ is closed.</p>
<p>Prove method: if function $g$ is continuous, then set $S={\mathbb x\in \mathbb R^n | g(\mathbb x) \le 0}$ is closed. Or $S={\mathbb x\in \mathbb R^n | g(\mathbb x) \ge 0}$ is closed. Or $S={\mathbb x\in \mathbb R^n | g(\mathbb x) = 0}$ is closed.</p>
<blockquote>
<p>Example:</p>
<p>Prove $C = \left{ \begin{pmatrix} x \ x^2 \end{pmatrix} \in \mathbb R^2 \right}$ is closed.</p>
<p>Solution:<br>$$<br>C = \left{ \begin{pmatrix} x \ x^2 \end{pmatrix} \in \mathbb R^2 \right} = \left{ \begin{pmatrix} x_1 \ x_2 \end{pmatrix} \in \mathbb R^2 \quad \vert \quad g(\mathbb x) := x_2-x_1^2 = 0 \right}.<br>$$<br>since $g$ is continuous, so $C$ is closed.</p>
</blockquote>
<p>Compactness: Closed + Bounded</p>
<p>Coercive function:<br>$$<br>\lim\limits_{|\mathbb x| \rightarrow \infty} f(\mathbb x) = +\infty<br>$$<br>Which means that the function value $f(\mathbb x)$ will increase without bound as $\mathbb x$ moves away from the origin in all possible directions.</p>
<p><strong>==Existence of optimal solution:==</strong></p>
<ol>
<li><p>a continuous function $g:\mathbb R^n \rightarrow \mathbb R$ on a nonempty closed and bounded set $S \subset \mathbb R^n$ has a global maximum and a global minimum point in $S$.</p>
</li>
<li><p>$f:\mathbb R^n \rightarrow \mathbb R$ is continuous function. If $f$ is coercive, then $f$ has at least one global minimizer.  (when x is not bounded)</p>
</li>
</ol>
<p>A stationary point $\mathbb x^*$ with $H_f(\mathbb x^*)$ positive semi-definite, the status of $\mathbb x^*$ can only be determined by analyzing $f(\mathbb x^*)$ is a neighborhood of $\mathbb x^*$.  Can obtain a global minimizer by comparing the objective value at the stationary points.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/4_Gradient%20Method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/4_Gradient%20Method/" class="post-title-link" itemprop="url">4. Gradient Method</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:01:06" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="4-1-Unconstrained-Optimization"><a href="#4-1-Unconstrained-Optimization" class="headerlink" title="4.1 Unconstrained Optimization"></a>4.1 Unconstrained Optimization</h1><p>Descent direction: direction $\mathbf d$ such that $\langle \nabla f(x^*), d\rangle &lt;0 $.</p>
<p>Steepest decent direction: direction $\mathbf d$ such that $\mathbf d^* = - \nabla f(x^*)$.</p>
<p><strong>Steepest descent method with exact line search:</strong></p>
<p>[step 0]</p>
<p>[step k]</p>
<p>steepest descent method follows a zig-zag path towards $\mathbf x^*$ with the right angle at each turn. $\longrightarrow$ slow</p>
<p>Theorem:</p>
<p>Steepest descent method with exact line search moves in perpendicular steps. More precisely, if $\mathbf x^{(k)}$ is a steepest descent sequence for a function $f(\mathbf x)$, then for each $k$, the vector joining $\mathbf x^{(k)}$ to $\mathbf x^{(k+1)}$ (i.e. $\mathbf x^{(k+1)} - \mathbf x^{(k)}$) is orthogonal (perpendicular) to the vector joining $\mathbf x^{(k+1)}$ to $\mathbf x^{(k+2)}$ (i.e. $\mathbf x^{(k+2)}-\mathbf x^{(k+1)}$).</p>
<p>Property:</p>
<ul>
<li>monotonic decreasing property</li>
<li>convergence<ul>
<li>limit of  any convergent subsequence of ${\mathbf x^{(k)}}$ is a critical point of $f(\mathbf x)$.</li>
</ul>
</li>
</ul>
<h2 id="4-1-1-convergence-rate-for-unconstrained-quadratic-minimization"><a href="#4-1-1-convergence-rate-for-unconstrained-quadratic-minimization" class="headerlink" title="4.1.1 convergence rate for unconstrained quadratic minimization"></a>4.1.1 convergence rate for unconstrained quadratic minimization</h2><p>$$<br>\min_{\mathbf x\in \mathbb R^n} q(\mathbf x)=\frac{1}{2}\mathbf x^T \mathbf Q \mathbf x<br>$$</p>
<p>Proposition:</p>
<p>$\mathbf Q$ is symmetric and positive definite, use steepest descent method with exact line search,</p>
<ol>
<li><p>Let $\mathbf d^k = \nabla q(\mathbf x^k)=\mathbf Q \mathbf x^k$.<br>$$<br>\frac{q\left(\mathrm{x}^{k+1}\right)}{q\left(\mathrm{x}^{k}\right)}= 1-\frac{\left\langle\mathrm{d}^{k}, \mathrm{d}^{k}\right\rangle^{2}}{\left\langle\mathrm{d}^{k}, \mathrm{Qd}^{k}\right\rangle\left\langle\mathrm{d}^{k}, \mathrm{Q}^{-1} \mathrm{d}^{k}\right\rangle}<br>$$</p>
</li>
<li><p>$$<br>\frac{q\left(\mathrm{x}^{(k+1)}\right)}{q\left(\mathrm{x}^{(k)}\right)} \leq\left[\frac{\kappa(\mathrm{Q})-1}{\kappa(\mathrm{Q})+1}\right]^{2}=: \rho(\mathrm{Q})<br>$$</p>
<p>where $\kappa(\mathrm{Q})=\lambda_n / \lambda_1$, $\lambda_n, \lambda_1$ are the largest and smallest eigenvalues of $\mathbf Q$.  </p>
</li>
</ol>
<p>Remark.</p>
<ul>
<li><p>the convergence rate $\rho(\mathbf Q)$ depends on $\kappa(\mathbf Q)$. When $\kappa(\mathbf Q)$ is large, the convergence rate<br>$$<br>\rho(\mathbf Q) \approx 1-\frac{4}{\kappa(\mathbf Q)}<br>$$</p>
</li>
<li><p>In $\mathbb R^2$, the contours of $q(\mathbf x)$ are ellipses. And $\kappa(\mathbf Q)$ is the ratio between the length of the principal axes of the ellipses. Thus the larger the value of $\kappa(\mathbf Q)$, the more elongated the ellipses are.</p>
</li>
<li><p>The number of iterations needed to reduce the relative error $q(\mathbf x^k)/q(\mathbf x^0)$ to smaller than $\epsilon$ is given by<br>$$<br>k=\left[ \frac{\log \epsilon}{\log \rho(\mathbf Q)} \right] + 1<br>$$<br>where $[a]$ denotes the largest integer less than or equal to $a$.</p>
</li>
</ul>
<h2 id="4-1-2-Convergence-rate-for-strongly-convex-function"><a href="#4-1-2-Convergence-rate-for-strongly-convex-function" class="headerlink" title="4.1.2 Convergence  rate for strongly convex function"></a>4.1.2 Convergence  rate for strongly convex function</h2><p>$f$: strongly convex with parameter $m$ and has $M$-Lipschitz continuous gradient. Its Hessian:<br>$$<br>mI \preceq H_f (\mathbf x) \preceq MI, \forall \mathbf x\in S<br>$$<br>Lemma:</p>
<p>$\mathbf x^*$ is minimizer of $\min{f(\mathbf x)| \mathbf x \in S}$. Then<br>$$<br>f(\mathrm{x})-\frac{1}{2 m}|\nabla f(\mathrm{x})|^{2} \leq f\left(\mathrm{x}^{*}\right) \leq f(\mathrm{x})-\frac{1}{2 M}|\nabla f(\mathrm{x})|^{2} \quad \forall \mathrm{x} \in S<br>$$<br>Theorem:</p>
<p>Let $\mathbf x^*$ be unique minimizer. $E_k = f(\mathbf x^k) - f(\mathbf x^*)$. Then<br>$$<br>\begin{aligned} E_{k+1} &amp; \leq E_{k}-\frac{1}{2 M}\left|\nabla f\left(\mathrm{x}^{k}\right)\right|^{2} \quad \text { (descent inequality) } \ E_{k+1} &amp; \leq E_{k}\left(1-\frac{m}{M}\right) \end{aligned}<br>$$<br>Remark:</p>
<p>based on this theorem, we have<br>$$<br>{\qquad E\left(\mathrm{x}^{k+1}\right) / E\left(\mathrm{x}^{1}\right) \leq(1-m / M)^{k} \leq \varepsilon}<br>$$<br>implies that we need the number of iterations $k$ to satisfy<br>$$<br>{\left.\qquad k \geq \frac{\log \varepsilon^{-1}}{\log \rho^{-1}} \approx \frac{m}{M} \log \varepsilon^{-1} \quad \text { (if } m / M \ll 1\right)}<br>$$<br>where $\rho = 1-m/M$.</p>
<h1 id="4-2Line-search-strategies"><a href="#4-2Line-search-strategies" class="headerlink" title="4.2Line search strategies"></a>4.2Line search strategies</h1><ol>
<li>minimization rule = exact line search</li>
<li>Armijo rule</li>
<li>nonmonotone line search</li>
<li>monotone line search , BB stepsize</li>
</ol>
<h1 id="4-3-Accelerated-proximal-gradient"><a href="#4-3-Accelerated-proximal-gradient" class="headerlink" title="4.3 Accelerated proximal gradient"></a>4.3 Accelerated proximal gradient</h1><p>smooth convex function $f$ with $L$-Lipstchitz continuous gradient<br>$$<br>\min { F(x):= f(x)+g(x) | x\in \mathbb R^n }<br>$$<br>$g$ is proper closed convex function (maybe non-differentiable).</p>
<p>Problem:<br>$$<br>\hat x = \mathrm{argmin} {g(x)+q(x;\bar x) | x\in \mathbb R^n }<br>$$<br>where $q(x;\bar x) =  f(\bar x) + \langle \nabla f(\bar x), x-\bar x\rangle + \frac{1}{2} \langle x-\bar x, H(x-\bar x) \rangle $.</p>
<p>APG method</p>
<p>[step 1]</p>
<p>[step 2]</p>
<p>Lemma:</p>
<p>Assume $f(\hat x) \le q(\hat x; \bar x)$. The descent property:<br>$$<br>F(x)+\frac{1}{2}|x-\bar{x}|<em>{H}^{2} \geq F(\hat{x})+\frac{1}{2}|x-\hat{x}|</em>{H}^{2} \quad \forall x \in \mathbb{R}^{n}<br>$$<br>(the assumption $\Longrightarrow$ 1-Lipschitz continuous )</p>
<p><strong>Theorem</strong></p>
<p>Error</p>
<p>complexity</p>
<p>Two for PG and APG</p>
<blockquote>
<p>example:</p>
<ol>
<li><p>Sparse regression problem $\min{\frac{1}{2} |Ax-b|^2 + \rho|x|_1 | x\in \mathbb R^n }$. Let $f(x) = \frac{1}{2} |Ax-b|^2$ and $g(x) = \rho|x|<em>1$. $\nabla f(x) = A^T(Ax-b)$ is Lipschitz continuous with modulus $L=\lambda</em>{\max}(AA^T)$. Let $H=LI_n$, the APG method subproblem is given by<br>$$<br>x^{k+1}=\operatorname{argmin}\left{g(x)+\left\langle\nabla f\left(\bar{x}^{k}\right), x-\bar{x}^{k}\right\rangle+\frac{L}{2}\left|x-\bar{x}^{k}\right|^{2} | x \in \mathbb{R}^{n}\right}<br>$$</p>
</li>
<li><p>given $G\in \mathbb S^n$. Projection onto the closed convex cone $\mathrm{DNN}<em>n^* = \mathbb S</em>+^n + \mathcal N^n$:<br>$$<br>\begin{array}{l}{\min \left{\frac{1}{2}|S+Z-G|^{2} | S \in \mathbb{S}<em>{+}^{n}, Z \in \mathcal{N}^n \right} }\ {=\min \left{\frac{1}{2}\left|\Pi</em>{\mathcal{N}^{n}}(S-G)\right|^{2} | S \in \mathbb{S}<em>{+}^{n}\right}} \ {=\min \left{f(S)+\delta</em>{\mathrm{S}<em>{+}^{n}}(S) | S \in \mathbb{S}^{n}\right}}\end{array}<br>$$<br>where $f(S)=\frac{1}{2} | \Pi</em>{\mathcal N^n} (S-G) |^2$. The optimality condition for a minimizer $\bar S$ of the last minimization problem is given as follows:<br>$$<br>\begin{array}{l}{0 \in \partial\left(f+\delta_{\mathrm{S}<em>{+}^{n}}\right)(\bar{S})=\partial f(\bar{S})+\partial \delta</em>{\mathrm{S}<em>{+}}^{\mathrm{n}}(\bar{S})} \ {\Leftrightarrow-\nabla f(\bar{S}) \in \partial \delta</em>{\mathrm{S}<em>{+}}^{n}(\bar{S})} \ {\Leftrightarrow-\nabla f(\bar{S}) \in\left(I+\partial \delta</em>{\mathrm{S}<em>{+}}\right)(\bar{S})} \ {\Leftrightarrow \bar{S}=\left(I+\partial \delta</em>{\mathrm{S}<em>{+}}\right)^{-1}(\bar{S}-\nabla f(\bar{S}))=\Pi</em>{\mathrm{S}<em>{+}^{n}}(\bar{S}-\nabla f(\bar{S}))} \<br>\end{array}<br>$$<br>we have $\nabla f(S) = \Pi</em>{mathcal N^n}(S-G)$, and<br>$$<br>{\begin{aligned}|\nabla f(S)-\nabla f(S)| &amp;=\left|\Pi_{N^{n}}(S-G)-\Pi_{\mathcal{N}^{n}}\left(S^{\prime}-G\right)\right| \ &amp; \leq\left|(S-G)-\left(S^{\prime}-G\right)\right|=\left|S-S^{\prime}\right| \end{aligned}}<br>$$<br>Thus $∇f(·)$ is Lipschitz continuous with modulus $L = 1$. Pick $H = LI_n$, the APG subproblem is given by<br>$$<br>\begin{aligned} S^{k+1} &amp;=\operatorname{argmin}\left{\delta_{\mathrm{S}<em>{+}^{n}}(S)+\left\langle\nabla f\left(\bar{S}^{k}\right), S-\bar{S}^{k}\right\rangle+\frac{1}{2}\left|S-\bar{S}^{k}\right|^{2} | S \in \mathbb{S}^{n}\right} \ &amp;=\operatorname{argmin}\left{\frac{1}{2}\left|S-\bar{S}^{k}+\nabla f\left(\bar{S}^{k}\right)\right|^{2} | S \in \mathbb{S}</em>{+}^{n}\right} \ &amp;=\prod_{\mathrm{S}_{+}^{n}}\left(\bar{S}^{k}-\nabla f\left(\bar{S}^{k}\right)\right) \end{aligned}<br>$$<br>Observe that the subproblem is like carrying out a fixed-point iteration on the optimality condition.</p>
</li>
</ol>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/5_Basic%20Nonlinear%20Programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/5_Basic%20Nonlinear%20Programming/" class="post-title-link" itemprop="url">5. Basic Nonlinear Programming</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:01:34" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Karush-Kuhn-Tucker (KKT) necessary conditions for local minimizers that generalize the Lagrange Multiplier Method. The KKT necessary conditions are useful in locating possible candidates for a global minimizer.</p>
<p>Constraint qualification</p>
<ul>
<li><p>Active constraints</p>
</li>
<li><p><strong>Regular point :</strong> </p>
<p>$x^*$ be feasible point of NLP, Let $J(x^*) = \left{ j\in {1, \cdots, p} : h_j(x^*)=0 \right}$ be index set of active constraints at $x^*$. The set of gradient vectors<br>$$<br>{ \nabla g_i(x^*) : i=1, 2, \cdots, m } \cup {\nabla h_j(x^*) : j\in J(x^*) }<br>$$<br>is linearly independent. $x^*$ is regular point.</p>
</li>
</ul>
<p>Linearly Independence Constraint Qualification (LICQ)</p>
<ul>
<li>check whether linear independence, check whether the matrix of all gradient vector of active constraints is full rank.</li>
</ul>
<h1 id="5-1-KKT-necessary-conditions"><a href="#5-1-KKT-necessary-conditions" class="headerlink" title="5.1 KKT necessary conditions"></a>5.1 KKT necessary conditions</h1><p>We state the <u>Karush-Kuhn-Tucker necessary conditions</u> for a local minimizer $x^∗$ at which the <u>regularity condition</u> holds</p>
<h2 id="5-1-1-KKT-first-order-necessary-conditions"><a href="#5-1-1-KKT-first-order-necessary-conditions" class="headerlink" title="5.1.1 KKT first order necessary conditions:"></a>5.1.1 KKT first order necessary conditions:</h2><p>$$<br>\begin{aligned} \nabla f\left(\mathrm{x}^{<em>}\right)+\sum_{i=1}^{m} \lambda_{i}^{</em>} \nabla g_{i}\left(\mathrm{x}^{<em>}\right)+\sum_{j=1}^{p} \mu_{j}^{</em>} \nabla h_{j}\left(\mathrm{x}^{<em>}\right)  = 0 \ \mu_{j}^{</em>} \geq 0,  &amp;\ \forall j=1,2, \cdots, p \ \mu_{j}^{<em>}=0,  &amp; \ \forall j \notin J\left(\mathrm{x}^{</em>}\right) \end{aligned}<br>$$</p>
<blockquote>
<ol>
<li><p>For NLP with no equality constraints, active constraint is only $h_1(x)\leq 0$. So, $\nabla f(x^*) + \mu_1^* \nabla h_1(x^*) = 0$,</p>
<p>the direction of steep descent $-\nabla f(x^*)$ must be in same direction as $\nabla h_1(x^*)$ onward normal to boundary.</p>
</li>
<li><p>For NLP with no equality constraints, only two active constraints, $\nabla f(x^*) + \mu_1^* \nabla h_1(x^*) + \mu_2^* \nabla h_2(x^*) = 0$.</p>
<p>the direction of steep descent $-\nabla f(x^*)$ must lie in the cone spanned by the outward normals $\nabla h_1(x^*), \nabla h_2(x^*)$ to the boundaries $h_1(x)=0$ and $h_2(x)=0$.</p>
</li>
<li><p>For NLP with no equality constraints, $x^*$ is interior of feasible region, $\nabla f(x^*)=0$, as unconstrained minimization problem.</p>
</li>
</ol>
</blockquote>
<h2 id="5-1-2-KKT-second-order-necessary-condition"><a href="#5-1-2-KKT-second-order-necessary-condition" class="headerlink" title="5.1.2 KKT second order necessary condition:"></a>5.1.2 KKT second order necessary condition:</h2><p>$$<br>{\qquad H_{L}\left(\mathrm{x}^{<em>}\right) :=H_{f}\left(\mathrm{x}^{</em>}\right)+\sum_{i=1}^{m} \lambda_{i}^{<em>} H_{g_{i}}\left(\mathrm{x}^{</em>}\right)+\sum_{j=1}^{p} \mu_{j}^{<em>} H_{h_{j}}\left(\mathrm{x}^{</em>}\right)}<br>$$</p>
<p>If $\mathbb x^* $ is a local minimizer, then<br>$$<br>{\mathrm{y}^{T} H_{L}\left(\mathrm{x}^{<em>}\right) \mathrm{y} \geq 0}<br>$$<br>for all $\mathbb y\in T(\mathbb x^</em>)$, where<br>$$<br>T(\mathbb x^*) = \left{ \mathbb y\in \mathbb{R}^n:<br>\begin{array}{}<br>\nabla g_{i}\left(\mathrm{x}^{<em>}\right)^{T} \mathbf{y} = 0, i=1,2, \cdots, m \<br>\nabla h_{j}\left(\mathrm{x}^{</em>}\right)^{T} \mathrm{y} =  0, j \in J\left(\mathrm{x}^{*}\right)<br>\end{array} \right}<br>$$</p>
<blockquote>
<p>in fact, $T(\mathbb x^*)$ is tangent plane</p>
<ol>
<li><p>set $T(x^*)$ consists of vectors in the tangent space to $S$ at $x^*$.</p>
<p>Normal space $N(x^*)$ is subspace spanned by $\nabla g_1(x^*), \cdots, \nabla g_m (x^*), \nabla h_j(x^*), \forall j \in J(x^*)$.</p>
<p>Tangent space $T(x^*) = { \mathbb y: \mathbb u^T \mathbb y = 0, \forall \mathbb u \in N(\mathbb x^*) }$.</p>
</li>
<li><p>For NLP with no equality constraints, if $x^*$ is in interior of feasible region, $T(x^*) = \mathbb R^*$. 2nd KKT becomes $H_L(x^*) = H_f(x^*)$ must be positive semidefinite.</p>
</li>
</ol>
</blockquote>
<p>​    easier check for 2nd KKT.</p>
<p>​    matrix $\mathcal D(\mathbb x^*) = \left( \nabla g_1(\mathbb x^*), \cdots, \nabla g_m(\mathbb x^*), [\nabla h_j(\mathbb x^*) : j \in J(\mathbb x^*)] \right)$<br>$$<br>\mathbb y^T H_L(\mathbb x^*) \mathbb y \ge 0, \forall \mathbb y \in T(\mathbb x^*) \iff Z(\mathbb x^*)^T H_L(\mathbb x^*) Z(\mathbb x^*) \text{ is positive semidefinite}<br>$$<br>​    where $Z(\mathbb x^*)$ is a matrix whose columns forms a basis of the null space of $\mathcal D (\mathbb x^*)$. $T(x) = Z(x) u$</p>
<h2 id="5-1-2-Steps"><a href="#5-1-2-Steps" class="headerlink" title="5.1.2 Steps:"></a>5.1.2 Steps:</h2><ol>
<li>Check regularity condition</li>
<li>verify KKT first order necessary condition.</li>
<li>verify KKT second order necessary condition<ol>
<li>Hessian matrix</li>
<li>basis of null space of $\mathcal D(\mathbb x^*)^T$</li>
<li>definiteness check</li>
</ol>
</li>
</ol>
<p>Since the regularity condition for the KKT Theorem is not satisfied, the global minimizer $x^∗$ may not be found among the KKT points.</p>
<p><u><em><strong>KKT necessary conditions can be used to find global minimizers.</strong></em></u></p>
<p>==Corollary 2.3==</p>
<p>Suppose the following two conditions hold:</p>
<ol>
<li>a global minimizer $x^∗$ is known to exist for an NLP.</li>
<li>$x^∗$ is a regular point.</li>
</ol>
<p>Then $x^∗$ is a KKT point, i.e. there exists $λ^∗ ∈ \mathbb R^m$ and $μ^∗ ∈ \mathbb R^p$ such that the following conditions hold.<br>$$<br>\begin{array}{l}{[1] \quad \nabla f\left(\mathrm{x}^{<em>}\right)+\sum_{i=1}^{m} \lambda_{i}^{</em>} \nabla g_{i}\left(\mathrm{x}^{<em>}\right)+\sum_{j=1}^{p} \mu_{j}^{</em>} \nabla h_{j}\left(\mathrm{x}^{<em>}\right)=0} \ {[2] \quad g_{i}\left(\mathrm{x}^{</em>}\right)=0, \quad \forall i=1,2, \cdots, m} \ {[3] \quad \mu_{j}^{<em>} \geq 0, h_{j}\left(\mathrm{x}^{</em>}\right) \leq 0, \quad \mu_{j}^{<em>} h_{j}\left(\mathrm{x}^{</em>}\right)=0, \quad \forall j=1,2, \cdots, p}\end{array}<br>$$<br><u>One approach to use the necessary conditions for solving an NLP is to consider separately all the possible combinations of constraints being active or inactive.</u></p>
<h1 id="5-2-Interpretation-of-Lagrange-Multipliers"><a href="#5-2-Interpretation-of-Lagrange-Multipliers" class="headerlink" title="5.2 Interpretation of Lagrange Multipliers"></a>5.2 Interpretation of Lagrange Multipliers</h1><p>Relax constraints –&gt; relax objective function with the rate of multipliers</p>
<h1 id="5-3-KKT-sufficient-conditions"><a href="#5-3-KKT-sufficient-conditions" class="headerlink" title="5.3 KKT sufficient conditions"></a>5.3 KKT sufficient conditions</h1><p><strong>the sufficient conditions for a strict local minimizer (or maximizer) do not require regularity conditions.</strong></p>
<p>Let $f, g_i, h_j : \mathbb R^n → \mathbb R, i = 1, 2, · · · ,m$ and $j = 1, 2, · · · , p$, be functions with continuous second partial derivatives. Let $S$ be the feasible set of (NLP). Suppose $\mathbb x^∗ ∈ S$ is a KKT point, i.e., there exist $λ^∗ ∈ \mathbb R^m$ and $μ^∗ ∈ \mathbb R^p$ such that<br>$$<br>\nabla f\left(\mathrm{x}^{<em>}\right)+\sum_{i=1}^{m} \lambda_{i}^{</em>} \nabla g_{i}\left(\mathrm{x}^{<em>}\right)+\sum_{j=1}^{p} \mu_{j}^{</em>} \nabla h_{j}\left(\mathrm{x}^{<em>}\right)  = 0 \ \mu_{j}^{</em>} \geq 0,  \ \forall j=1,2, \cdots, p \ \mu_{j}^{<em>}=0,  \ \forall j \notin J\left(\mathrm{x}^{</em>}\right)<br>$$<br>where $J(\mathbb x^∗)$ is the index set of active constraints at $\mathbb x^∗$. Let<br>$$<br>{\qquad H_{L}\left(\mathrm{x}^{<em>}\right) =H_{f}\left(\mathrm{x}^{</em>}\right)+\sum_{i=1}^{m} \lambda_{i}^{<em>} H_{g_{i}}\left(\mathrm{x}^{</em>}\right)+\sum_{j=1}^{p} \mu_{j}^{<em>} H_{h_{j}}\left(\mathrm{x}^{</em>}\right)}<br>$$<br>suppose<br>$$<br>{\mathrm{y}^{T} H_{L}\left(\mathrm{x}^{<em>}\right) \mathrm{y} &gt; 0}, \forall \mathbb y \in T(\mathbb x^</em>) - {0}<br>$$<br>where $T(\mathbb x^*)$ is tangent space. $\mathbb x^*$ is a strict local minimizer.</p>
<p><strong>steps:</strong></p>
<ol>
<li>verify it’s KKT point. </li>
<li>verify it satisfy the second order sufficient condition<ol>
<li>Hessian matrix</li>
<li>$Z(\mathbf x^*)$</li>
<li>Determine definiteness</li>
</ol>
</li>
</ol>
<h1 id="5-4-KKT-for-constrained-convex-programming-problems"><a href="#5-4-KKT-for-constrained-convex-programming-problems" class="headerlink" title="5.4 KKT for constrained convex programming problems"></a>5.4 KKT for constrained convex programming problems</h1><p>For a convex problem, a feasible point $x^∗$ is a KKT point implies it is a global minimizer.<br>$$<br>\begin{array}{ll}{\min } &amp; {f(\mathrm{x})} \ {\text { s.t. }} &amp; {g_{i}(\mathrm{x}) :=\mathbf{a}<em>{i}^{T} \mathrm{x}-b</em>{i}=0, \quad i=1, \ldots, m} \ {} &amp; {h_{j}(\mathrm{x}) \leq 0, \quad j=1, \ldots, p}\end{array}<br>$$<br>where $f, h_j : \mathbb{R}^n \rightarrow \mathbb{R}$ are convex function. </p>
<p><u><strong>KKT point is an optimal solution under convexity</strong></u></p>
<p><strong>but a global minimizer of a convex program may not be a KKT point.</strong></p>
<p><strong><u>With regularity condition</u></strong>, a global minimizer is a KKT point. For convex programming problem with at least one inequality constraints, the Slater’s condition ensures that a global minimizer is a KKT point.</p>
<p><strong><u>slater’s condition</u>:</strong> there exists $\hat{\mathbb x} ∈ \mathbb R^n$ such that $g_i(\hat{\mathbb x}) = 0, ∀ i = 1,\cdots, m$ and $h_j(\hat{\mathbb x}) &lt; 0, ∀ j = 1, . . . , p$.</p>
<pre><code> Slater&#39;s condition states that the feasible region must have an [interior point](https://en.wikipedia.org/wiki/Interior_(topology)).
</code></pre>
<h2 id="5-4-1-Linear-equality-constrained-convex-program"><a href="#5-4-1-Linear-equality-constrained-convex-program" class="headerlink" title="5.4.1 Linear equality constrained convex program"></a>5.4.1 Linear equality constrained convex program</h2><p>Theorem 5.5. (Linear equality constrained convex program)</p>
<p>Consider the following linear equality constrained convex NLP:<br>$$<br>\begin{array}{rl}<br>\text{(ECP)} &amp;<br>\begin{array}{l}<br>{\min f(\mathbb{x})} \<br>\text{s.t. } \mathbf A \mathbb x = \mathbb b<br>\end{array}<br>\end{array}<br>$$<br>where $\mathbf A$ is an $m × n$ matrix whose $i$-row is $\mathbb a_i^T$ and $f : \mathbb R^n → \mathbb R$ is a differentiable <del>convex</del> function. Suppose the feasible region $S$ is non-empty. Then a point $x^∗ ∈ S$ is a KKT point if and only if $x^∗$ is a global minimizer of $f$ on $S$.</p>
<blockquote>
<p>KKT point $\iff$ global minimizer</p>
</blockquote>
<p>==Note: no regularity or Slater’s condition is needed.==</p>
<p>Facts:</p>
<ol>
<li>$\mathbb x \in S \iff \mathbb x = \mathbb x^* + \mathbb z$ for some $\mathbb z \in \operatorname{Null}(A)$.  【space of x】</li>
<li>$\mathbb y^T \mathbb z = 0$, $\forall \mathbb z \in \operatorname{Null}(\mathbf A) \iff \exist \lambda^* \in \mathbb R^m$ s.t. $\mathbb y+ \mathbf A^T \lambda^*=0$.</li>
</ol>
<h2 id="5-4-2-Linear-amp-Convex-quadratic-programming-problems"><a href="#5-4-2-Linear-amp-Convex-quadratic-programming-problems" class="headerlink" title="5.4.2 Linear &amp; Convex quadratic programming problems"></a>5.4.2 Linear &amp; Convex quadratic programming problems</h2><ol>
<li><p>Linear programming<br>$$<br>\begin{array}{rl}{\min } &amp; {f(\mathrm{x}) :=\mathrm{c}^{T} \mathrm{x}} \ {(\mathrm{LP}) \quad \text { s.t. }} &amp; {\mathrm{b}-\mathrm{A} \mathrm{x}=0} \ {} &amp; {\mathrm{x} \geq 0}\end{array}<br>$$<br>where $\mathbf A$ is $m\times n$, $\mathbb b\in \mathbb R^m$, $\mathbb a_i^T$ is $i$th row of $\mathbf A$.</p>
<p>KKT conditions are:<br>$$<br>\begin{array}{l}{[1] \quad \mathrm{c}-\mathrm{A}^{T} \lambda-\mu=0} \ {[2] \quad \mathrm{b}-\mathrm{Ax}=0} \ {[3] \quad \mu \geq 0, \mathrm{x} \geq 0, \mu \circ \mathrm{x}=0}\end{array}<br>$$<br>where<br>$$<br>\mu \circ \mathrm{x} :=\left[\begin{array}{c}{\mu_{1} x_{1}} \ {\mu_{2} x_{2}} \ {\vdots} \ {\mu_{n} x_{n}}\end{array}\right]<br>$$</p>
</li>
<li><p>Convex quadratic programming<br>$$<br>\begin{array}{rl}{\min } &amp; {f(\mathbf{x}) :=\frac{1}{2} \mathbf{x}^{T} \mathbf{Q} \mathbf{x}+\mathbf{c}^{T} \mathbf{x}} \ {(\mathrm{QP})\quad  \text { s.t. }} &amp; {\mathbf{A} \mathbf{x}-\mathbf{b} \leq \mathbf{0}} \ {} &amp; {\mathbf{x} \geq \mathbf{0}, \mathbf{x} \in \mathbb{R}^{n}}\end{array}<br>$$<br>where $\mathbf Q$ is positive semidefinite, $\mathbf A$ is $m\times n$, $\mathbb b\in \mathbb R^m$, $\mathbb a_i^T$ is $i$th row of $\mathbf A$.</p>
<p>KKT conditions are:<br>$$<br>\begin{array}{l}{[1] \quad \mathrm{Qx+c}+\mathrm{A}^{T} \mu-\hat\mu=0} \<br>{[2] \quad \mu \geq 0,\ \mathrm{Ax} \leq \mathbb b,\ \mu \circ \mathrm{(Ax-b)}=0}\<br>{[2] \quad \hat\mu \geq 0,\ \mathrm{x} \geq 0,\  \hat\mu \circ \mathrm{x}=0}\<br>\end{array}<br>$$<br>Let $\mathbb z = \mathrm{b-Ax}$, rewriteen:<br>$$<br>\begin{array}{c}<br>{\mathrm{Qx+c}+\mathrm{A}^{T} \mu-\hat\mu=0} \<br>{\mathrm{Ax+z}=\mathbb b} \<br>{\mu \circ \mathrm{z}=0}\<br>{\hat\mu \circ \mathrm{x}=0}\<br>{\mu, \hat\mu, \mathbb x, \mathbb z \geq 0}.<br>\end{array}<br>$$<br>Example:</p>
<p><strong>Sparse Regression Problem</strong>: $\frac{1}{2} | Ax-b|^2 + \rho |x|_1$ where $\rho &gt; 0$.</p>
<p>Reformulated as:<br>$$<br>\begin{array}{ll}{\min } &amp; {f\left(x, u^{(1)}, u^{(2)}\right) :=\frac{1}{2}|A x-b|^{2}+\rho\left\langle e, u^{(1)}+u^{(2)}\right\rangle} \ {\text { s.t. }} &amp; { g\left(x, u^{(1)}, u^{(2)}\right) :=x-u^{(1)}+u^{(2)}=0} \ {} &amp; {h_{1}\left(x, u^{(1)}, u^{(2)}\right) :=-u^{(1)} \leq 0} \ {} &amp; {h_{2}\left(x, u^{(1)}, u^{(2)}\right) :=-u^{(2)} \leq 0}\end{array}<br>$$<br>KKT conditions:<br>$$<br>\begin{array}{c}<br>{\left(\begin{array}{c}{A^{T}(A x-b)} \ {\rho e} \ {\rho e}\end{array}\right)+\left(\begin{array}{c}{I} \ {-I} \ {I}\end{array}\right) \lambda+\left(\begin{array}{c}{0} \ {-I} \ {0}\end{array}\right) \mu^{(1)}+\left(\begin{array}{c}{0} \ {0} \ {-I}\end{array}\right) \mu^{(2)}=\left(\begin{array}{c}{0} \ {0} \ {0}\end{array}\right)} \<br>{\qquad \begin{array}{c}{x-u^{(1)}+u^{(2)}=0} \<br>{\mu^{(1)} \circ u^{(2)} = 0, \quad \mu^{(2)} \circ u^{(2)}=0} \<br>{u^{(1)} , u^{(2)}, \mu^{(1)}, \mu^{(2)} \geq 0}<br>\end{array}}\end{array}<br>$$<br>eliminated the system, we have:<br>$$<br>\begin{array}{c}<br>{u^{T}(A x-b)+\lambda=0} \<br>{-\rho e \leq \lambda \leq \rho e} \<br>{(\rho e-\lambda) \circ \max {x, 0}=0, \quad(\rho e+\lambda) \circ \max {-x, 0}=0} \<br>\iff \lambda_i \in \begin{cases} { \rho } &amp; \text{if } x_i&gt;0, \ [-\rho, \rho] \quad &amp; \text{if } x_i = 0 \ { -\rho } \quad &amp; \text{if } x_i &lt; 0 \end{cases} \quad i=1,\cdots, n.<br>\end{array}<br>$$</p>
</li>
</ol>
<h1 id="5-5-Proof-of-KKT-1st-necessary-conditions"><a href="#5-5-Proof-of-KKT-1st-necessary-conditions" class="headerlink" title="5.5 Proof of KKT 1st necessary conditions"></a>5.5 Proof of KKT 1st necessary conditions</h1><p>via the Penalty Approach</p>
<h1 id="5-6-Summary"><a href="#5-6-Summary" class="headerlink" title="5.6 Summary"></a>5.6 Summary</h1><img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/20190924224456.png" style="zoom:70%">








      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/6_Basic%20Lagrangian%20duality%20and%20Saddle%20point/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/6_Basic%20Lagrangian%20duality%20and%20Saddle%20point/" class="post-title-link" itemprop="url">6. Basic Lagrangian Duality and Saddle Point</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:02:07" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Basic Lagrangian duality and saddle point optimality conditions</p>
<p>Primal problem &lt;——-&gt; Lagrangian dual problem</p>
<h1 id="6-1-Lagrangian-dual-problem"><a href="#6-1-Lagrangian-dual-problem" class="headerlink" title="6.1 Lagrangian dual problem"></a>6.1 Lagrangian dual problem</h1><ul>
<li>Primal problem:<br>$$<br>\begin{array}{l}<br>{\text{(P)}} &amp;<br>\begin{array} {l}<br>{\text{min}} &amp; f(\mathbb x) \<br>\text{s.t.} &amp; g_i(\mathbb x) = 0 \<br>\quad &amp; h_j(\mathbb x) \leq 0\<br>\quad &amp; \mathbb x \in X<br>\end{array}<br>\end{array}<br>$$<br>where $x \subset \mathbb R^n$.</li>
</ul>
<h2 id="6-1-1-Lagrangian-dual-problem"><a href="#6-1-1-Lagrangian-dual-problem" class="headerlink" title="6.1.1 Lagrangian dual problem"></a>6.1.1 Lagrangian dual problem</h2><ul>
<li><p><u>Lagrangian dual function:</u><br>$$<br>\theta(\lambda, \mu) = \inf \left{ f(\mathbb x) + \sum\limits_{i=1}^m \lambda_i g_i(\mathbb x) + \sum\limits_{j=1}^p \mu_j h_j(\mathbb x) \ : \  \mathbb x \in X  \right}<br>$$<br>(unconstrained problem), $\mu&gt;0$.</p>
<p>Then, we will have<br>$$<br>\theta(\lambda, \mu) \leq f(\mathbb x^*) + \sum\limits_{j=1}^p \mu_j h_j(\mathbb x) \leq f(\mathbb x^*) \quad \theta(\lambda, \mu) \text{ is lower bound of } f(\mathbb x^*)<br>$$<br>Find the greatest lower bound.</p>
</li>
<li><p><u>Lagrangian Dual Problem:</u><br>$$<br>(\text{D}) \quad \max \  \theta(\lambda, \mu) = \inf \left{ f(\mathbb x) + \lambda^T g_i(\mathbb x) + \mu^T h_j(\mathbb x) \ : \  \mathbb x \in X  \right} \<br>\text{s.t. }\quad  \lambda \in \mathbb R^m,\quad  \mu\in R_+^p.<br>$$<br><strong>$\theta(\lambda, \mu)$ is concave function.</strong></p>
</li>
</ul>
<p>Convex programming, for D or P, if one has optimal solution, the other will also have. </p>
<p>Nonconvex programming, if one has optimal solution, the other may not have.</p>
<p>If convexity + suitable constraints, D and  P may have equal optimal solution. </p>
<h2 id="6-1-2-Weak-Duality"><a href="#6-1-2-Weak-Duality" class="headerlink" title="6.1.2 Weak Duality"></a>6.1.2 Weak Duality</h2><p>Optimal primal (minimization) objective value ≥ Optimal dual (maximization) objective value<br>$$<br>\min { f(\mathbb x) : \mathbb x \in S} \geq \max { \theta(\lambda. \mu):\lambda \in \mathbb R^m, \mu \geq 0 }<br>$$<br>If $f(\mathbb x^*) = \theta (\lambda^*, \mu ^*)$, then $x^*$ and $(\lambda^*, \mu^*)$ are optimal solution. </p>
<p><strong>Duality Gap</strong>: $\min { f(\mathbb x) : \mathbb x \in S} - \max { \theta(\lambda. \mu):\lambda \in \mathbb R^m, \mu \geq 0 }$</p>
<h2 id="6-1-3-Strong-Duality"><a href="#6-1-3-Strong-Duality" class="headerlink" title="6.1.3 Strong Duality"></a>6.1.3 Strong Duality</h2><p>$X$ : convex set. $f, h$ : convex functions. $g$ : affine functions,<br>$$<br>\inf \  { f(x): g(x)=0, h(x)\leq 0, x\in X } = \sup \ { \theta(\lambda, \mu):\mu \leq 0, \lambda \in \mathbb R^m }<br>$$</p>
<h1 id="6-2-Saddle-Point-Optimality-amp-KKT"><a href="#6-2-Saddle-Point-Optimality-amp-KKT" class="headerlink" title="6.2 Saddle Point Optimality &amp; KKT"></a>6.2 Saddle Point Optimality &amp; KKT</h1><p>$(\mathbb x^*, \lambda^*, \mu^*)$ is <strong>saddle point</strong> of Lagrangian function $L(\mathbb x, \lambda, \mu) = f(\mathbb x) +\lambda ^T g(\mathbb x) + \mu^T h(\mathbb x)$, if $\mathbb x^* \in X, \mu \geq 0$ and<br>$$<br>L(\mathbb x^*, \lambda, \mu) \leq L(\mathbb x^*, \lambda^*, \mu^*) \leq L(\mathbb x, \lambda^*, \mu ^*)<br>$$<br>Remark</p>
<ol>
<li>For a fixed $\mathbb x^∗$, $(λ^∗, μ^∗)$ maximizes $L(\mathbb x^∗, λ, μ)$ over all $(λ, μ)$ with $μ ≥ 0$.</li>
<li>For a fixed $(λ^∗, μ^∗)$ , $\mathbb{x}^*$ minimizes $L(\mathbb x, λ^∗, μ^∗)$ over all $x ∈ X$.</li>
</ol>
<p>$\mathbb x^*$ is optimal solution of (P) and $(\lambda^*, \mu^*)$ is optimal solution of (D). $(\mathbb x^*, \lambda^*,\mu^*)$ satisfies KKT conditions.</p>
<p><em><em>i.e., saddle point $(\mathbb x^</em>, \lambda^</em>,\mu^*)$ $\Rightarrow$ KKT condition**. but inverse is not true in general.</p>
<p><strong>Convex Programming (CP)</strong>:</p>
<p>KKT point of CP $\Rightarrow$ a saddle point.  </p>
<p>$(\mathbb x^*, \lambda^*,\mu^*)$ is KKT point for CP, $x^*$ and $(\lambda^*, \mu^*)$ are optimal solution of P and D.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/8_ADMM_DMPC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/8_ADMM_DMPC/" class="post-title-link" itemprop="url">9. ADMM-DMPC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:03:31" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>$$<br>\begin{array}{ll}<br>\min\limits_{u\in \mathbb R^{n_u}, v\in \mathbb R^{n_v}} &amp; f(u)+g(v) \<br>\text{s.t. } &amp; Au+bv=b<br>\end{array}<br>$$</p>
<p>where $f$ and $g$ are convex. </p>
<p>Consensus ADMM is suitable to solve the distributed model fitting problem. </p>
<p>Consensus method assign a separate copy of the unknowns, $u_i$, to each processor, and then apply ADMM to solve<br>$$<br>\begin{array}{ll}<br>\min\limits_{u_i\in \mathbb R^{d}, v\in \mathbb R^{d}} &amp; \sum\limits_{i=1}^N f_i(u_i)+g(v) \<br>\text{s.t. } &amp; u_i = v<br>\end{array}<br>$$<br>This is identical to the previous problem. Let $u=(u_1; \cdots; u_N) \in \mathbb R^{dN}$, $A=I_{dN}\in \mathbb R^{dN\times dN}$, and $B=-(I_d;\cdots; I_d)\in \mathbb R^{dN \times d}$.</p>
<h1 id="DMPC-ADMM"><a href="#DMPC-ADMM" class="headerlink" title="DMPC_ADMM"></a>DMPC_ADMM</h1><h2 id="Problem-formulation"><a href="#Problem-formulation" class="headerlink" title="Problem formulation"></a>Problem formulation</h2><p>$N$ independent agents, $\mathcal G=(\mathcal V,\mathcal E)$, vertex set $\mathcal V={1,\cdots,N}$, edge set $\mathcal E\subseteq \mathcal V\times \mathcal V$. neighbors $\mathcal N_i = {j:(i,j)\in \mathcal E}$.</p>
<p><strong>Local host system dynamics:</strong><br>$$<br>x_i(t+1) = A_ix_i(t) + B_iu_i(t), \quad i=1,\cdots,N<br>$$</p>
<p><strong>Global system dynamics:</strong><br>$$<br>x(t+1) = Ax(t) + Bu(t),<br>$$<br>where $x(t) = [x_1(t)^T, \cdots, x_N(t)^T]$ and $u(t) = [u_1(t)^T, \cdots, u_N(t)^T]^T$. </p>
<p><strong>The objective</strong> is to minimize the prediction horizon cost function<br>$$<br>J=\sum_{t=0}^{T-1} \sum_{i=1}^N \ell_i ( x_{\mathcal N_i}(t), u_{\mathcal N_i}(t) )<br>$$<br>where $\ell_i$ are convex, closed and proper stage cost funciton. $x_{\mathcal N_i}$ and $u_{\mathcal N_i}$ are concatenations of the state and input vectors of agent $i$ and its neighbors. </p>
<p><strong>local constraints</strong> are $x_{\mathcal N_i} (t) \in \mathcal X_i$, $u_{\mathcal N_i}(t)\in \mathcal U_i$, where the $\mathcal X_i, \mathcal U_i$ are convex subsets that couple the states and inputs of neighboring agents.</p>
<p><strong>global constraints</strong> are $\mathcal X \subseteq \mathbb R^{\sum_i n_i}, \ \mathcal U\subseteq\mathbb R^{\sum_i m_i}$.</p>
<p><strong>Problem:</strong><br>$$<br>\begin{array}{ll}<br>\min &amp; \sum\limits_{t=0}^{T-1} \sum\limits_{i=1}^N \ell_i ( x_{\mathcal N_i}(t), u_{\mathcal N_i}(t) ) + \sum\limits_{i=1}^N \ell_{i,f} (x_{\mathcal N_i}(T), u_{\mathcal N_i}(T)) \<br>\text{s.t.} &amp; x_i(t+1) = A_ix_i(t) + B_iu_i(t) \<br>&amp; x_{\mathcal N_i}(t) \in \mathcal X_i, \quad u_{\mathcal N_i}(t) \in \mathcal U_i \<br>&amp; i = 1,\cdots,N, \quad t=0,1,\cdots, T-1, \<br>&amp; x_{\mathcal N_i}(T) \in \mathcal X_{i,f}, \quad i = 1, \cdots, N<br>\end{array}<br>$$</p>
<blockquote>
<p>For centralized problem, there are various methods for choosing the terminal cost functions and constraint set s to the feasibility and stability of the closed-loop system.</p>
<p>For distributed problem, these quantities should be choosen to reflect the information architecture, i.e., the terminal cost and constraint sets for agent $i$ should only depend on $x_{\mathcal N_i}$.</p>
</blockquote>
<h2 id="Dealing-with-constraints"><a href="#Dealing-with-constraints" class="headerlink" title="Dealing with constraints"></a>Dealing with constraints</h2><p>All of the coupling constraints can be reduced to so-called consistency constraints by introducing the local copies of variables at each subsystem and requiring the local copies of the coupling vairables to be the same across coupled subsystem. </p>
<p>let $\mathbf x_i$ be a local variable vector for agent $i$ that includes a copy of the state and input vectors over the finite horizon. For example, if agent 1 is connected to agent 2, the $\mathbf x_1$ includesa local copy of $x_1, u_1, x_2, u_2$ over the finite horizon. Likewise,  for agent 2. the $\mathbf x_2$ containts $x_1, u_1, x_2, u_2$.</p>
<p>Let $\mathbf x = [\mathbf x_1^T, \cdots, \mathbf x_N^T]$ be a vector that includes all copies of all variables in the problem.</p>
<p>The problem has the form:<br>$$<br>\begin{array}{ll}<br>\min\limits_{\mathbf x_i \in \mathbf X_i} &amp; \sum\limits_{i=1}^N f_i(\mathbf x_i) \<br>\text{s.t.} &amp; \mathbf x_i = \mathbf E_i \mathbf x_i, \quad i=1,\cdots,N.<br>\end{array}<br>$$</p>
<p>$\mathbf E_i$ is a matrix that picks out components of $\mathbf x$ that match components of the local variable $\mathbf x_i$. The constraint set $\mathbf X_i$ is convex and includes all constraints for agent $i$ and all of its neighbors. </p>
<p>Deduction of this simplification:</p>
<p>$\mathbf x_i = [x_{\mathcal N_i}, u_{\mathcal N_i}] = [x_1,u_1, x_3,u_3, \cdots, x_j,u_j], \quad \forall j \in \mathcal N_i$<br>$$<br>f_i(\mathbf x_i) = \sum_{t=0}^{T-1} \ell_i(x_{\mathcal N_i}(t), u_{\mathcal N_i}(t)) + \ell_{i,f}(x_{\mathcal N_i}(T), u_{\mathcal N_i}(T))<br>$$</p>
<p>The problem now has a separable equality constraints, which enforce consistency of local variable copies.</p>
<h1 id="Parallel-Multi-block-ADMM"><a href="#Parallel-Multi-block-ADMM" class="headerlink" title="Parallel Multi-block ADMM"></a>Parallel Multi-block ADMM</h1><p>$$<br>\begin{array}{ll}<br>\min &amp; f_1(x_1) + \cdots + f_N(x_N) \<br>\text{s.t.} &amp; A_1 x_1 + \cdots + A_Nx_N = c \<br>&amp; x_1\in \mathcal X_1, \cdots, x_N \in \mathcal X_N<br>\end{array}<br>$$</p>
<p>where $x_i\in \mathbb R^{n_i}$, $A_i\in \mathbb R^{m\times n_i}$, $c\in \mathbb R^m$, $f_i:\mathbb R^{n_i} \rightarrow (-\infty, +\infty]$. The constraint $x_i\in \mathcal X_i$ can be incorporated in objective function $f_i$ via indicator function $\delta_{\mathcal X_i}(x_i)$. </p>
<h2 id="Method-1-Dual-decomposition-dual-ascent-method"><a href="#Method-1-Dual-decomposition-dual-ascent-method" class="headerlink" title="Method 1: Dual decomposition + dual ascent method"></a>Method 1: Dual decomposition + dual ascent method</h2><p>$$<br>\begin{array}{l}<br>{\mathcal{L}\left(\mathrm{x}<em>{1}, \ldots, \mathrm{x}</em>{N}, \lambda\right) =\sum_{i=1}^{N} f_{i}\left(\mathrm{x}<em>{i}\right)-\lambda^{\top}\left(\sum</em>{i=1}^{N} A_{i} \mathrm{x}<em>{i}-c\right)} \<br>{\begin{cases}<br>(x_1^{k+1}, x_2^{k+1}, \cdots, x_N^{k+1}) &amp;= \operatorname{argmin}</em>{x_i} \mathcal L(x_1, \cdots, x_N, \lambda^k)\<br>\lambda^{k+1} &amp;=\lambda^{k}-\alpha_{k}\left(\sum_{i=1}^{N} A_{i} \mathrm{x}_{i}^{k+1}-c\right)<br>\end{cases}}\<br>\end{array}<br>$$</p>
<p>where  $\lambda \in \mathbb{R}^{m}$ is the Lagrangianl multiplier or the dual variable. Since all the $x_i$ are separable in the Lagrangian<br>function, the $x$-update step reduces to solving $N$ individual $x_i$-subproblems.</p>
<p>Convergence rate: $O(\frac{1}{\sqrt{k}})$.</p>
<h2 id="Method-2-ADMM"><a href="#Method-2-ADMM" class="headerlink" title="Method 2: ADMM"></a>Method 2: ADMM</h2><p>Augmented Lagrangian function<br>$$<br>\mathcal{L}<em>{\rho}\left(\mathrm{x}</em>{1}, \ldots, \mathrm{x}<em>{N}, \lambda\right)=\sum</em>{i=1}^{N} f_{i}\left(\mathrm{x}<em>{i}\right)-\lambda^{\top}\left(\sum</em>{i=1}^{N} A_{i} \mathrm{x}<em>{i}-c\right)+\frac{\rho}{2}\left|\sum</em>{i=1}^{N} A_{i} \mathrm{x}<em>{i}-c\right|</em>{2}^{2}<br>$$<br>introduce a quadratic penalty of the constraints. </p>
<p>To solve multi-block ADMM, one can first convert the multi-block problem into 2-block problem via variable splitting:<br>$$<br>\begin{aligned}<br>\min <em>{\left{\mathbf{x}</em>{i}\right},\left{\mathbf{z}<em>{i}\right}} &amp; \sum</em>{i=1}^{N} f_{i}\left(\mathbf{x}<em>{i}\right)+I</em>{\mathcal{Z}}\left(\mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right) \ \text { s.t. } &amp; A_{i} \mathbf{x}<em>{i}-\mathbf{z}</em>{i}=\frac{c}{N}, \forall i=1,2, \ldots, N<br>\end{aligned}<br>$$<br>Here, the convex set $\mathcal{Z}=\left{\left(\mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right): \sum_{i=1}^{N} \mathbf{z}_{i}=0\right}$. </p>
<p>Two block: $x:= (x_1, \cdots, x_N)$ and $z:= (z_1, \cdots, z_N)$. –&gt;  ADMM</p>
<h3 id="Variable-Splitting-ADMM"><a href="#Variable-Splitting-ADMM" class="headerlink" title="Variable Splitting ADMM"></a>Variable Splitting ADMM</h3><p>$$<br>\mathcal{L}<em>{\rho}(\mathrm{x}, \mathrm{z}, \lambda)=\sum</em>{i=1}^{N} f_{i}\left(\mathrm{x}<em>{i}\right)+I</em>{\mathcal{Z}}(\mathrm{z})-\sum_{i=1}^{N} \lambda_{i}^{\top}\left(A_{i} \mathrm{x}<em>{i}-\mathrm{z}</em>{i}-\frac{c}{N}\right)+\frac{\rho}{2} \sum_{i=1}^{N}\left|A_{i} \mathrm{x}<em>{i}-\mathrm{z}</em>{i}-\frac{c}{N}\right|_{2}^{2}<br>$$</p>
<p>$$<br>\mathrm{z}^{k+1}=\underset{\left{\mathrm{z}: \sum_{i=1}^{N} \mathrm{z}<em>{i}=0\right}}{\arg \min } \sum</em>{i=1}^{N} \frac{\rho}{2}\left|A_{i} \mathrm{x}<em>{i}^k-\mathrm{z}</em>{i}-\frac{c}{N}-\frac{\lambda_{i}^{k}}{\rho}\right|_{2}^{2}<br>$$</p>
<p>$$<br>\begin{cases}<br>\mathrm{z_i}^{k+1}= \left(A_i\mathrm x_i^k -\frac{c}{N}-\frac{\lambda_{i}^{k}}{\rho} \right) -\frac{1}{N} \left{ \sum_{j=1}^N A_j\mathrm x_j^k -\frac{c}{N}-\frac{\lambda_{j}^{k}}{\rho} \right} \<br>\mathrm{x}<em>i^{k+1}=\underset{\left{\mathrm{x}<em>i\right}}{\arg \min } \sum</em>{i=1}^{N} f</em>{i}\left(\mathrm{x}<em>{i}\right)+I</em>{\mathcal{Z}}(\mathrm{z}) + \sum_{i=1}^{N} \frac{\rho}{2}\left|A_{i} \mathrm{x}<em>{i}-\mathrm{z}</em>{i}-\frac{c}{N}-\frac{\lambda_{i}^{k}}{\rho}\right|<em>{2}^{2}\<br>\lambda^{k+1}<em>i =\lambda_i^{k}-\rho\left(A</em>{i} \mathrm{x}</em>{i}^{k+1}-\mathrm{z}_{i}^{k+1}-\frac{c}{N}\right)<br>\end{cases}<br>$$</p>
<p>This method introduce splitting variables, which substantially increases the number of variables and constraints in the problem, especially when $N$ is large.</p>
<blockquote>
<p>first convert problem to a 2-block problem, then apply the classic ADMM</p>
</blockquote>
<h3 id="sGS-ADMM"><a href="#sGS-ADMM" class="headerlink" title="sGS-ADMM"></a>sGS-ADMM</h3><p>simply replace the two-block alternating minimization scheme by a sweep of Gauss-Seidel update, namely, update $x_i$ for $i = 1, 2, \cdots, N$ sequentially as follows:<br>$$<br>\begin{aligned} \mathrm{x}<em>{i}^{k+1} &amp;=\underset{\mathrm{x}</em>{i}}{\arg \min } \ \mathcal{L}<em>{\rho}\left(\mathrm{x}</em>{1}^{k+1}, \ldots, \mathrm{x}<em>{i-1}^{k+1}, \mathrm{x}</em>{i}, \mathrm{x}<em>{i+1}^{k}, \ldots, \mathrm{x}</em>{N}^{k}, \lambda^{k}\right) \ &amp;=\underset{\mathrm{x}<em>{i}}{\arg \min }\ f</em>{i}\left(\mathrm{x}<em>{i}\right)+\frac{\rho}{2}\left|\sum</em>{j&lt;i} A_{j} \mathrm{x}<em>{j}^{k+1}+A_{i} \mathrm{x}<em>{i}+\sum</em>{j&gt;i} A</em>{j} \mathrm{x}<em>{j}^{k}-c-\frac{\lambda^{k}}{\rho}\right|</em>{2}^{2} \end{aligned}<br>$$</p>
<p>$$<br>\begin{cases}<br>{\mathbf{x}<em>{i}^{k+1}=\min <em>{\mathbf{x}</em>{i}} f</em>{i}\left(\mathbf{x}<em>{i}\right)+\frac{\rho}{2}\left|\sum</em>{j&lt;i} A_{j} \mathbf{x}<em>{j}^{k+1}+A_{i} \mathbf{x}<em>{i}+\sum</em>{j&gt;i} A</em>{j} \mathbf{x}<em>{j}^{k}-c-\frac{\lambda^{k}}{\rho}\right|</em>{2}^{2}} \<br>{\lambda^{k+1}=\lambda^{k}-\rho\left(\sum_{i=1}^{N} A_{i} \mathbf{x}_{i}^{k+1}-c\right)}<br>\end{cases}<br>$$</p>
<p>The algorithm may not converge for $N&gt;3$. Although lack of convergence guarantee, some empirical studies show that Algorithm is still very effective at solving many practical problems.</p>
<p>A disadvantage of Gauss-Seidel ADMM is that the blocks are updated one after another, which is not amenable for parallelization.</p>
<h3 id="Jacobian-ADMM"><a href="#Jacobian-ADMM" class="headerlink" title="Jacobian ADMM"></a>Jacobian ADMM</h3><p>Jacobi-type scheme that updates all the N blocks in parallel<br>$$<br>\begin{aligned} \mathrm{x}<em>{i}^{k+1} &amp;=\underset{\mathrm{x}</em>{i}}{\arg \min } \ \mathcal{L}<em>{\rho}\left(\mathrm{x}</em>{1}^{k}, \ldots, \mathrm{x}<em>{i-1}^{k}, \mathrm{x}</em>{i}, \mathrm{x}<em>{i+1}^{k}, \ldots, \mathrm{x}</em>{N}^{k}, \lambda^{k}\right) \ &amp;=\underset{\mathrm{x}<em>{i}}{\arg \min }\ f</em>{i}\left(\mathrm{x}<em>{i}\right)+\frac{\rho}{2}\left|\sum</em>{j\neq i} A_{j} \mathrm{x}<em>{j}^{k+1}+A</em>{i} \mathrm{x}<em>{i}-c-\frac{\lambda^{k}}{\rho}\right|</em>{2}^{2} \end{aligned}<br>$$</p>
<p>$$<br>\begin{cases}<br>{\mathbf{x}<em>{i}^{k+1}=\underset{\mathrm{x}</em>{i}}{\arg \min }\ f_{i}\left(\mathrm{x}<em>{i}\right)+\frac{\rho}{2}\left|\sum</em>{j\neq i} A_{j} \mathrm{x}<em>{j}^{k+1}+A</em>{i} \mathrm{x}<em>{i}-c-\frac{\lambda^{k}}{\rho}\right|</em>{2}^{2} }\<br>{\lambda^{k+1}=\lambda^{k}-\rho\left(\sum_{i=1}^{N} A_{i} \mathbf{x}_{i}^{k+1}-c\right)}<br>\end{cases}<br>$$</p>
<p>this scheme is more likely to diverge than the Gauss-Seidel scheme for the same parameter $\rho$. In fact, it may diverge<br>even in the two-block case; see [16] for such an example. In order to guarantee its convergence, either additional assumptions or modifications to the algorithm must be made.</p>
<h3 id="Proximal-Jacobian-ADMM"><a href="#Proximal-Jacobian-ADMM" class="headerlink" title="Proximal Jacobian ADMM"></a>Proximal Jacobian ADMM</h3><p>adding proximal term  $\frac{1}{2}|\mathrm x_i - \mathrm x_i^k |<em>{P_i}^2$ for each $\mathrm x_i$-subproblem. Here $P_i \succ  0$ is some symmetric and positive semi-definite matrix and we let $|\mathrm x_i |</em>{P_i}^2 := \mathrm x_i^T P_i \mathrm x_i$. </p>
<p>adding damping parameter $\gamma &gt;0$ for the update of $\lambda$.<br>$$<br>\begin{cases}<br>{\mathbf{x}<em>{i}^{k+1}=\underset{\mathrm{x}</em>{i}}{\arg \min }\ f_{i}\left(\mathrm{x}<em>{i}\right)+\frac{\rho}{2}\left|\sum</em>{j\neq i} A_{j} \mathrm{x}<em>{j}^{k+1}+A</em>{i} \mathrm{x}<em>{i}-c-\frac{\lambda^{k}}{\rho}\right|</em>{2}^{2}+\frac{1}{2}| \mathrm{x}<em>i-\mathrm{x}<em>i^k|</em>{P_i}^2 }\<br>{\lambda^{k+1}=\lambda^{k}-\gamma\rho\left(\sum</em>{i=1}^{N} A_{i} \mathbf{x}<em>{i}^{k+1}-c\right)}<br>\end{cases}<br>$$<br>where<br>$$<br>\left{\begin{array}{l}{P</em>{i} \succ \rho\left(\frac{1}{\epsilon_{i}}-1\right) A_{i}^{\top} A_{i}, i=1,2, \ldots, N} \ {\sum_{i=1}^{N} \epsilon_{i}&lt;2-\gamma}\end{array}\right.<br>$$<br>Disadvantages:</p>
<p>global convergence as well as an $o(\frac{1}{k})$ convergence rate under conditions on $P_i$ and $\gamma$.</p>
<p>when the $\mathrm x_i$-subproblem is not strictly convex, adding the proximal term can make the subproblem strictly or strongly convex, making it more stable.</p>
<p>provide multiple choices for matrices $P_i$ with which the subproblems can be made easier to solve.</p>
<blockquote>
<p>$x_i$-subproblem contains a quadratic term $\frac{\rho}{2} \mathrm x_i \mathrm A_i^T \mathrm A_i \mathrm x_i$. When $\mathrm A_i^T\mathrm A_i$ is ill-conditioned or computationally expensive to invert, one can let $P_i = D_i - \rho \mathrm A_i^T \mathrm A_i$, which cancels the quadratic term $\frac{\rho}{2} \mathrm x_i \mathrm A_i^T \mathrm A_i \mathrm x_i$ and adds $\frac{1}{2} \mathrm x_i D_i \mathrm x_i$. The matrix $D_i$ can be chosen as some well-conditioned and simple matrix (e.g., a diagonal matrix), thereby leading to an easier subproblem.</p>
<ul>
<li><p>$P_i=\tau_iI (\tau_i&gt;0)$: Proximal method</p>
</li>
<li><p>$P_i=\tau_iI - \rho A_i^TA_i (\tau_i&gt;0)$: Prox-linear method. linearize the quadratic penalty term of lagrangian at the current $\mathrm x_i^k$ and adds a proximal term.<br>$$<br>\mathbf{x}<em>{i}^{k+1}=\underset{\mathbf{x}</em>{i}}{\arg \min } f_{i}\left(\mathbf{x}<em>{i}\right)+\left\langle\rho A</em>{i}^{\top}\left(A \mathbf{x}^{k}-c-\lambda^{k} / \rho\right), \mathbf{x}<em>{i}\right\rangle+\frac{\tau</em>{i}}{2}\left|\mathbf{x}<em>{i}-\mathbf{x}</em>{i}^{k}\right|^{2}<br>$$<br>use $\tau_i I$ to approximate the Hessian matrix $\rho A_i^T A_i$  of the quadratic penalty term.</p>
</li>
</ul>
</blockquote>
<h2 id="Adaptive-Parameter-Tuning"><a href="#Adaptive-Parameter-Tuning" class="headerlink" title="Adaptive Parameter Tuning"></a>Adaptive Parameter Tuning</h2><p>adaptively adjusting the matrices ${P_i}$</p>
<img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/20191203172838.png" style="zoom:40%">

<p>The above strategy starts with relatively small proximal terms and gradually increase them. After a finite<br>number of iterations, ${P_i}$ will remain constant for convergence. </p>
<p>Empirical evidence shows that the paramters ${P_i}$ typically adjust themselves only during the first few iterations and then remain constant afterwards. Alternatively, one may also decrease the parameters after every few iterations or after they have not been updated for a certain number of iterations. But the total times of decrease should be bounded to guarantee convergence. By using this adaptive strategy, the resulting paramters ${P_i}$ are usually much smaller than those required by the condition (2.8), thereby leading to substantially faster convergence in practice.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/3_Basic%20Convex%20Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/3_Basic%20Convex%20Analysis/" class="post-title-link" itemprop="url">3. Convex Analysis</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:00:44" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="3-1-Convex-set"><a href="#3-1-Convex-set" class="headerlink" title="3.1 Convex set"></a>3.1 Convex set</h1><p>To prove a given set C is convex</p>
<ol>
<li>For any two arbitrary points $\mathbb x, \mathbb y$ in $C$, and any $λ ∈ [0, 1]$.  Method is to argue that $λx + (1 − λ)y ∈ C$</li>
<li>If $C_1, C_2, \cdots, C_m$ are convex set, then $C= \cap_{i=1}^m C_i$ is also convex.</li>
</ol>
<blockquote>
<p>for any $m\times n$ matrix $\mathbf A$ and an m-column vector $\mathbb b$, the polyhedron or polyhedral set ${\mathbb x\in\mathbb R^n | \mathbf A \mathbb x \leq \mathbb b}$ is convex.</p>
</blockquote>
<ol start="3">
<li>Suppose $D\subset \mathbb R^n$ is convex, If $f:D\rightarrow \mathbb R$ is convex, then for any $\alpha \in \mathbb R$, the set $S_\alpha = { \mathbb x \in D | f(\mathbb x) \le \alpha}$ is convex. </li>
<li>Epigraph of $f$ : $E_f={[\mathbb x; \alpha] ; \mathbb x\in D, \alpha \in \mathbb R, f(\mathbb x) \leq \alpha }$ is convex set $\iff$ $f$ is convex function.</li>
</ol>
<h1 id="3-2-Convex-function"><a href="#3-2-Convex-function" class="headerlink" title="3.2 Convex function"></a>3.2 Convex function</h1><h2 id="3-2-1-Prove-convexity-of-a-function"><a href="#3-2-1-Prove-convexity-of-a-function" class="headerlink" title="3.2.1 Prove convexity of a function"></a>3.2.1 Prove convexity of a function</h2><ul>
<li><p>Definition: </p>
<ul>
<li><p>$f(\lambda \mathbb x + (1-\lambda)\mathbb y) \leq \lambda f(\mathbb x) + (1-\lambda) f(\mathbb y)$.</p>
</li>
<li><p>if $f_1, f_2 : D\rightarrow \mathbb R$ are convex functions on a convex set $D \subset \mathbb R^n$, then $f_1 + f_2$ is convex. $\alpha f_1$ is convex if $\alpha &gt;0$.</p>
</li>
<li><p>Let $f_1, f_2, \cdots, f_k: D\rightarrow \mathbb R$ be convex on a convex set $D \subset \mathbb R^n$, then if $\alpha_j \ge 0, \forall j$,  $f(\mathbb x)=\sum\limits_{j=1}^k \alpha_j f_j(\mathbb x)$ is convex, </p>
</li>
<li><p>$h: D\rightarrow \mathbb R$ is convex. $g:\mathcal X \rightarrow \mathbb R$ is nondecreasing convex function. The composite function $f=g \circ h$ is convex.</p>
<blockquote>
<p>Objective function $f(\mathbb x) = \frac{1}{2} | A\mathbb x-\mathbb b |^2 + \lambda |\mathbb x |_1$ for sparse regression problem is convex.</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Epigraph:</p>
<ul>
<li>Epigraph of $f$ : $E_f={[\mathbb x; \alpha] ; \mathbb x\in D, \alpha \in \mathbb R, f(\mathbb x) \leq \alpha }$ is convex set $\iff$ $f$ is convex function.</li>
</ul>
</li>
<li><p><strong>Jensen’s inequality:</strong> </p>
<ul>
<li>$f$ is convex function on convex set $S$, let $\mathbb x = \sum\limits_{j=1}^k \lambda_j \mathbb x^{(j)}$, where $\sum\limits_{j=1}^k \lambda_j =1, \lambda_j \ge 0$. Then<br>$$<br>f(\mathbb x) \leq \sum\limits_{j=1}^k \lambda_kf(\mathbb x^{(j)})<br>$$<br>When function is twice differentiable, some ways to check convexity.</li>
</ul>
</li>
<li><p><em>Tangent plane characterization:</em></p>
<ul>
<li><p>$f(\mathbb x)$ has continuous first partial derivatives on an open convex set $S$. Then the function $f$ is convex $\iff$<br>$$<br>\underbrace{f(\mathbb x) + \nabla f(\mathbb x)^T (\mathbb y - \mathbb x)}_{\text{tangent plane}} \leq f(\mathbb y)<br>$$<br>means that the tangent plane is below the surface for a convex function.</p>
</li>
<li><p>$f(\mathbb x)$ is differentiable on the open convex subset $S$. Then $f$ is convex on $S$ $\iff$<br>$$<br>\text{(monotone gradient condition)} \quad \langle \nabla f(\mathbb y)-\nabla f(\mathbb x), \mathbb y - \mathbb x \rangle \ge 0, \quad \forall \mathbb x, \mathbb y \in S<br>$$</p>
<blockquote>
<p>(方向基本相同，在0-90之间)</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Test for convexity of a differentiable function (second partial derivatives)</p>
<ul>
<li><p>$f(\mathbb x)$ has continuous second partial derivatives on an open convex set $D$ in $\mathbb R^n$. </p>
<ol>
<li><p>$f$ is convex on $D$ $\iff$ the Hessian matrix $H_f(\mathbb x)$ is positive semidefinite </p>
</li>
<li><p>If $H_f(\mathbb x)$ is positive definite, then $f$ is strictly convex.</p>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="3-2-2-Equivalent-statement-of-convex-function"><a href="#3-2-2-Equivalent-statement-of-convex-function" class="headerlink" title="3.2.2 Equivalent statement of convex function"></a>3.2.2 Equivalent statement of convex function</h2><p>(i) $f(\mathbb{y}) \ge f(\mathbb{x})+\triangledown f(\mathbb{x})^T(\mathbb{y}-\mathbb{x}) $ Tangent plane characterization</p>
<p>(ii) $\langle \triangledown f(\mathbf{y}) - \triangledown f(\mathbf{x}), \mathbf{y}-\mathbf{x} \rangle \ge 0, \forall  \mathbf{x}, \mathbf{y} \in S \quad \text{monotone gradient condition}$</p>
<p>(iv) $f (λx + (1 − λ)y) ≤ λf (x) + (1 − λ)f (y)$ convex</p>
<h2 id="3-2-3-Strongly-Convex-Function"><a href="#3-2-3-Strongly-Convex-Function" class="headerlink" title="3.2.3 Strongly Convex Function"></a>3.2.3 Strongly Convex Function</h2><p>Strong Convex Function:<br>$$<br>f(\lambda \mathbf{x}+(1-\lambda) \mathbf{y}) \leq \lambda f(\mathbf{x})+(1-\lambda) f(\mathbf{y})-\frac{c}{2} \lambda(1-\lambda)|\mathbf{x}-\mathbf{y}|^{2}<br>$$<br>Theorem 3.3:</p>
<p>Suppose $f : D → R$ is a differentiable function. Then following statements are equivalent:</p>
<p>(i) ${f(\mathbf{y}) \geq f(\mathbf{x})+\langle\nabla f(\mathbf{x}), \mathbf{y}-\mathbf{x}\rangle+\frac{c}{2}|\mathbf{y}-\mathbf{x}|^{2}, \quad \forall \mathbf{x}, \mathbf{y} \in D}$</p>
<p>(ii) ${g(\mathbf{x})=f(\mathbf{x})-\frac{c}{2}|\mathbf{x}|^{2}}$ is convex</p>
<p>(iii) ${\langle\nabla f(\mathbf{x})-\nabla f(\mathbf{y}), \mathbf{x}-\mathbf{y}\rangle \geq c|\mathbf{x}-\mathbf{y}|^{2}}$</p>
<p>(iv) $f$ is strongly convex.</p>
<blockquote>
<p>Proof:</p>
<p>( (i) $\iff$ (ii) )</p>
<p>“ $\Longleftarrow$ “ </p>
<p>If ${g(\mathbf{x})=f(\mathbf{x})-\frac{c}{2}|\mathbf{x}|^{2}}$ is convex, then we have<br>$$<br>g(\mathbb y) \geq g(\mathbb x) + \nabla g(\mathbb x)(\mathbb y - \mathbb x)\<br>\Rightarrow f(\mathbb y) -\frac{c}{2}|\mathbb{y}|^{2} \ge f(\mathbb{x})-\frac{c}{2}|\mathbb{x}|^{2} + (\nabla f(\mathbb x) - c \mathbb x)^T (\mathbb y - \mathbb x)\<br>\Rightarrow f(\mathbb y) \ge f(\mathbb x) + \nabla f(\mathbb x)^T(\mathbb y - \mathbb x) - \frac{c}{2}|\mathbb{x}|^{2} - c\mathbb x^T(\mathbb y - \mathbb x) + \frac{c}{2}|\mathbb{y}|^{2} \<br>\Rightarrow f(\mathbb y) \ge f(\mathbb x) + \nabla f(\mathbb x)^T(\mathbb y - \mathbb x) - \frac{c}{2} (|\mathbb{y}|^{2} - 2\mathbb x^T \mathbb y + |\mathbb{x}|^{2})\<br>\Rightarrow f(\mathbb y) \ge f(\mathbb x) + \langle\nabla f(\mathbf{x}), \mathbf{y}-\mathbf{x}\rangle+\frac{c}{2}|\mathbf{y}-\mathbf{x}|^{2}<br>$$</p>
<p>“ $\Longrightarrow$ “</p>
<p>can easily proved by using tangent plane characterization.</p>
<p>( (ii) $\iff$ (iii) )</p>
<p>prove by monotone gradient condition</p>
<p>( (ii) $\iff$ (iv) )</p>
<p>proved by definition of convexicity</p>
</blockquote>
<p>Let $\mathbb{x}^*$ be minimizer of $f$ over $D$<br>$$<br>\begin{array}{l}{\text { (a) } \frac{1}{2 c}|\nabla f(\mathrm{x})|^{2} \geq f(\mathrm{x})-f\left(\mathrm{x}^{*}\right)} \ {\text { (b) }|\nabla f(\mathrm{x})-\nabla f(\mathrm{y})| \geq c|\mathrm{x}-\mathrm{y}|} \ {\text { (c) } f(\mathrm{y}) \leq f(\mathrm{x})+\langle\nabla f(\mathrm{x}), \mathrm{y}-\mathrm{x}\rangle+\frac{1}{2 c}|\nabla f(\mathrm{y})-\nabla f(\mathrm{x})|^{2}} \ {\text { (d) }\langle\nabla f(\mathrm{x})-\nabla f(\mathrm{y}), \mathrm{x}-\mathrm{y}\rangle \leq \frac{1}{c}|\nabla f(\mathrm{x})-\nabla f(\mathrm{y})|^{2}}\end{array}<br>$$</p>
<blockquote>
<p>Proof.</p>
<p>(a)</p>
<p>We already have $f(\mathbf{y}) \geq f(\mathbf{x})+\langle\nabla f(\mathbf{x}), \mathbf{y}-\mathbf{x}\rangle+\frac{c}{2}|\mathbf{y}-\mathbf{x}|^{2}$.  For a fixed $\mathbb x$, we can obtain the minimum value of the RHS, which is $f(\mathbf{x})+\langle\nabla f(\mathbf{x}), \mathbf{y}-\mathbf{x}\rangle+\frac{c}{2}|\mathbf{y}-\mathbf{x}|^{2} \ge f(\mathbb x) - \frac{1}{2c} |\nabla f(\mathbb x)|^2$. Then, we have $f(\mathbf{y}) \ge f(\mathbb x) - \frac{1}{2c} |\nabla f(\mathbb x)|^2$. Let $\mathbb y = \mathbb x^*$, we can have (a). Q.E.D.</p>
<p>This means when the gradient value is smaller, the current function value is getting closer to the optimal value.</p>
<p>(b)</p>
<p>follows from the (iii) by Cauchy-Schwartz inequality</p>
<p>(c)</p>
<p>Define a function $\phi_\mathbb x (\mathbb z) = f(\mathbb z) - \langle \nabla f(\mathbb x), \mathbb z \rangle$. FInd the minimizer of this function as </p>
<p>$\nabla \phi_\mathbb x(\mathbb z) = \nabla f(\mathbb z) - \nabla f(\mathbb x) = 0 \Longrightarrow \mathbb z^* = x$. Then, we can use the (a), have $\frac{1}{2 c}|\nabla f(\mathrm{y}) - \nabla f(\mathrm{x})|^{2} \geq f(\mathrm{y}) - \langle\nabla f(\mathbb x), \mathbb x - \mathbb y \rangle - f\left(\mathrm{x}\right)$ .</p>
<p>(d)</p>
<p>interchanging $\mathbb x$ and $\mathbb y$ in (c), we get<br>$$<br>f(\mathrm{x}) \leq f(\mathrm{y})+\langle\nabla f(\mathrm{y}), \mathrm{x}-\mathrm{y}\rangle+\frac{1}{2 c}|\nabla f(\mathrm{x})-\nabla f(\mathrm{y})|^{2}<br>$$<br>Combining the two inequalities, we can get (d).  Q.E.D.</p>
</blockquote>
<h1 id="3-3-Unconstrained-Programming"><a href="#3-3-Unconstrained-Programming" class="headerlink" title="3.3 Unconstrained Programming"></a>3.3 Unconstrained Programming</h1><h2 id="3-3-1-Unconstrained-Convex-Programming"><a href="#3-3-1-Unconstrained-Convex-Programming" class="headerlink" title="3.3.1 Unconstrained Convex Programming"></a>3.3.1 Unconstrained Convex Programming</h2><p>For a convex programming, a local minimizer is also a global minimizer. </p>
<p>Suppose $\mathbb x^*$ is a stationary pointm i.e. $\nabla f(\mathbb x^*)=0$. For any $\mathbb y\in D$, we have $f(\mathbb y) \ge f(\mathbb x^*)+\nabla f(\mathbb x^*)^T(\mathbb y - \mathbb x^*) = f(\mathbb x^*)$. Hence, $\mathbb x^*$ us a global minimizer.</p>
<h2 id="3-3-2-Unconstrained-Convex-Quadratic-Programming"><a href="#3-3-2-Unconstrained-Convex-Quadratic-Programming" class="headerlink" title="3.3.2 Unconstrained Convex Quadratic Programming"></a>3.3.2 Unconstrained Convex Quadratic Programming</h2><p>Let $\mathbf Q$ be an $n\times n$ <u>symmetric</u> matrix and $\mathbf c \in \mathbb R^n$. The quadratic function<br>$$<br>q(\mathbb{x})=\frac{1}{2} \mathbb{x}^T \mathbf{Q} \mathbb{x} + \mathbb{c}^T \mathbb{x}<br>$$</p>
<p>is a convex function $\iff$ $\mathbf{Q}$ is positive semidefinite.</p>
<p><u>***global minimizer <em><em><em></u> $\mathbb{x}^</em>$ $\iff$  $\mathbf{Q}\mathbb{x}^</em>=-\mathbb{c}$.   if $\mathbf{Q}^{-1}$ exists, $\mathbb{x}^</em>=-\mathbf{Q}^{-1}\mathbb{c}$.</p>
<blockquote>
<p>==Exercise:== How to prove it? </p>
<p>Show that<br>$$<br>\inf_{\mathbb x\in \mathbb R^n} \left{ \frac{1}{2}\langle\mathbb x, \mathbf Q\mathbb x \rangle - \langle \mathbf c, \mathbb x \rangle \right} =<br>\begin{cases}<br>-\infty \quad &amp; \text{if } c\notin \operatorname{Range}(\mathbf Q) \<br>\frac{1}{2} \langle \mathbb w, \mathbf Q \mathbb w \rangle \quad &amp; \text{if } \mathbb c=\mathbf Q \mathbb w<br>\end{cases}<br>$$<br>The latter is achieved for any $\mathbb x$ s.t.<br>$$<br>\mathbf Q (\mathbb x - \mathbb w) = 0 \iff \mathbb x \in \mathbb w + \operatorname{Null}(\mathbf Q)<br>$$</p>
</blockquote>
<h1 id="3-4-Constrained-Convex-Programming"><a href="#3-4-Constrained-Convex-Programming" class="headerlink" title="3.4 Constrained Convex Programming"></a>3.4 Constrained Convex Programming</h1><h2 id="3-4-1-Minimizer"><a href="#3-4-1-Minimizer" class="headerlink" title="3.4.1 Minimizer"></a>3.4.1 Minimizer</h2><p><strong>Theorem 3.7.</strong> </p>
<p>Let $f : C → R$ be a convex and continuously differentiable function on the convex set $C ⊂ \mathcal{E}$. Consider the constrained minimization problem<br>$$<br>\min { f(x) \vert c\in C}<br>$$<br>Then, $x^<em>\in C$ is a global minimizer $\iff$<br>$$<br>\langle \triangledown f(x^</em>), x-x^* \rangle &gt; 0, \forall x \in C<br>$$<br>Using <strong>Normal cone</strong> to rephrase it, </p>
<p>​        $x^*$ is a global minimizer of $\min { f(x) \vert c\in C}$ $\iff$  $-\triangledown f(x^*) \in N_C (x^*)$</p>
<h2 id="3-4-2-Projection"><a href="#3-4-2-Projection" class="headerlink" title="3.4.2 Projection"></a>3.4.2 Projection</h2><p><strong>Theorem 3.8. (Projection Theorem)</strong></p>
<p> Let $C$ be a closed convex set in $\mathcal{E}$. </p>
<p>(a) For every $z ∈ \mathcal{E}$, there exists a unique minimizer (denoted as $Π_C(z)$ and called as the <strong>projection</strong> of $z$ onto $C$) of<br>$$<br>\min { \frac{1}{2} |x-z|^2 \vert x\in C }<br>$$<br>(b) $x^* := \Pi_C (z)$ is projection of $z$ onto $C$ iff<br>$$<br>\langle z-x^*, x-x^* \rangle \leq 0, \forall x\in C<br>$$<br>(c) <u>[firmly non-expensive property</u>] For any $z, w \in \mathcal{E}$<br>$$<br>| \Pi_C(z) - \Pi_C(w) |^2 \leq \langle z-w, \Pi_C(z) - \Pi_C(w) \rangle<br>$$<br>Hence $| \Pi_C(z) - \Pi_C(w) |^2 \leq |z-w |$, i.e. $\Pi_C(\cdot)$ is Lipschitz continuous with modulus 1.</p>
<blockquote>
<p>Proof. </p>
<p>(a).</p>
<p>Let $\bar x \in C$. set $S = {x\in C| f(x)\le f(\bar x) }$. Obviously, the set $S$ is closed and bounded. By using Weierstrass’s Theorem, there exists $x^* \in S$ such that $f(x^*) \le f(x), \quad \forall x\in S$. If $\bar x \in S$, then we can also have $f(x^*)\le f(\bar x)$. </p>
<p>Besides, we can also have<br>$$<br>f(x)&gt;f(\bar x)\ge f(x^*), \quad \forall x\in C\backslash S<br>$$<br>Thus we have shown that $f(x^*)\le f(x)$ for all $x\in S \cup (C\backslash S) = C$. </p>
<p>Since the function $f(x)= \frac{1}{2} |x-z|^2$ is strictly convex on $C$, the minimizer is unique. </p>
<p>(b).</p>
<p>Based on Theorem 3.7, we have that<br>$$<br>0\le \langle f(x^*), x-x^* \rangle = \langle x^*-z, x-x^* \rangle \quad \forall x\in C<br>$$<br>(c).</p>
<p>Based on (b), $w,z \in \mathcal E$, we hanve<br>$$<br>\langle z-\Pi_C(z), \Pi_C(w)-\Pi_C(z)\rangle \le 0 \<br>\langle w-\Pi_C(w), \Pi_C(z)-\Pi_C(w)\rangle \le 0 \<br>$$<br>superposition the two inequalities, we can have (c)</p>
</blockquote>
<p>Remark.</p>
<p>If $C$ is linear subspace of $\mathbb R^n$, then $z-x^*\perp C$. This means<br>$$<br>z = \Pi_C(z) + (z-\Pi_C(z)), \quad \text{and } \quad \langle z-\Pi_C(z), \Pi_C(z)\rangle = 0<br>$$<br>More generally, If $C$ is a closed convex cone, we have<br>$$<br>\langle z-\Pi_C(z), \Pi_C(z)\rangle = 0<br>$$<br><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8bm0hlukmj31ac0quadl.jpg" style="zoom: 40%"></p>
<blockquote>
<p>Example:</p>
<ol>
<li><p>$C = \mathbb R_+^n$. For any $z\in \mathbb R^n$, $\Pi_C(z)=\max {z,0}$.</p>
<p>Proof. </p>
<p>$\Pi_C(z)$ is minimizer of $\min { \frac{1}{2} |x-z|^2 | x\in \mathbb R_+^n }$. We have<br>$$<br>|x-z|^2=(x_1^2-2z_1x_1 + z_1^2) + \cdots + (x_n^2 -2z_nx_n + z_n^2)<br>$$<br>Each part should be minimized. So, we have $\Pi_C(z)=\max {z,0}$.</p>
</li>
<li><p>$C={ x\in \mathbb R^n | \sum_{i=1}^n x_i = 1, x \ge 0 }$ is a simplex. Suppose $z\in \mathbb R^n$ satisfies the condition that $z_1 \ge z_2 \ge \cdots \ge z_n$. Let $k$ be the maximum index in the set ${ 1\leq j \leq n | z_j + \frac{1}{j} (1-\sum_{i=1}^jz_i)&gt;0 }$. Set<br>$$<br>\lambda = z_k + \frac{1}{k}(1-\sum_{i=1}^kz_i)<br>$$<br>Then the projection onto the simplex $C$ is given by<br>$$<br>\Pi_C(z) = \max {z+\lambda e, 0} \quad (e \text{ is the vector of all ones})<br>$$<br>Proof. </p>
<p>In the later Chaper. Also in Assignment 1.</p>
</li>
<li><p>$C=\mathbb S_+^n$, the cone of $n\times n$ symmetric positive semidefinite matrices. For a given $A\in \mathbb S^n$, let the eigenvalue decomposition of $A$ be $A=U \operatorname{diag}(d) U^T$. Then<br>$$<br>\Pi_C(A) = U \operatorname{diag}(\max{d,0}) U^T<br>$$<br>Proof.</p>
<p>Note that $U$ is orthogonal. $|UY|_F = |YU|_F$ for any $Y\in \mathbb S^n$.<br>$$<br>\begin{aligned}<br>\min\left{ \frac{1}{2} |X-A|<em>F^2 \vert X\in \mathbb S</em>+^n \right} &amp;= \min\left{ \frac{1}{2} |U(U^TXU-\operatorname{diag}(d))U^T|<em>F^2 \vert X\in \mathbb S</em>+^n \right}\<br>&amp;= \min\left{ \frac{1}{2} |\bar X-\operatorname{diag}(d)|<em>F^2 \vert \bar X\in \mathbb S</em>+^n \right} (\text{ where} \bar X = U^TXU)\<br>&amp;= \min\left{ \frac{1}{2} |\operatorname{diag}(x)-\operatorname{diag}(d)|_F^2 \vert x\ge0 \right} (\text{ off-diagonal elements of optimal} \bar X \text{ should be 0, so } \bar X= \operatorname{diag}(x))\<br>&amp;= \min\left{ \frac{1}{2} |x-d|_F^2 \vert x\ge0 \right}\<br>\end{aligned}<br>$$<br>Then, we have $x^* = \max{d,0}$</p>
<p>Optimal $\bar X^* = \operatorname{diag}(x^*) = \operatorname{diag}(\max{d,0})$. Hence optimal $X^*=U\bar X^* U^T = U\operatorname{diag}(\max{d,0}) U^T$.</p>
</li>
</ol>
</blockquote>
<h2 id="3-4-3-Frechet-differentiable"><a href="#3-4-3-Frechet-differentiable" class="headerlink" title="3.4.3 Frechet differentiable"></a>3.4.3 Frechet differentiable</h2><p>Let $\mathcal E_1,\mathcal E_2$ be two finite-dimensional real Euclidean spaces. A function $f : \mathcal E_1 → \mathcal E_2$ is said to be <strong>Frechet differentiable</strong> at $x ∈ \mathcal E_1$ if there exists a linear map $f′(x): \mathcal E_1 →\mathcal E_2$ such that for any $h→0$,<br>$$<br>f(x + h) − f(x) − f′(x)[h] = o(|h|).<br>$$<br>This means<br>$$<br>\lim_{h\rightarrow 0 } \frac{f(x+h)-f(x)}{h} = f’(x)\<br>\iff \lim_{h\rightarrow 0 } \frac{f(x+h)-f(x) - f’(x)h}{h} = 0\<br>\iff f(x + h) − f(x) − f′(x)[h] = o(|h|)<br>$$<br><strong>Proposition</strong></p>
<p>Let $C$ be a nonempty closed convex set in $\mathcal E$. For any $x$, let $\theta (x) = \frac{1}{2} | x-\Pi_C(x)|^2$. The $\theta(\cdot)$ is a continuously differentiable convex function and<br>$$<br>\nabla \theta(x) = x-\Pi_C(x)<br>$$</p>
<h1 id="3-5-Convex-Separation"><a href="#3-5-Convex-Separation" class="headerlink" title="3.5 Convex Separation"></a>3.5 Convex Separation</h1><p><strong>Definition :</strong></p>
<ol>
<li><p><strong>affine hull:</strong> $\operatorname{aff}(S)$<br>$$<br>\operatorname{aff}(S) = \left{ \sum_{i=1}^k \alpha_ix_i \vert \sum_{i=1}^k \alpha_i=1, x_i\in S, \alpha_i\in \mathbb R, k&gt;0 \right}<br>$$</p>
</li>
<li><p><strong>convex hull</strong>: $\operatorname{conv} (S)$:<br>$$<br>\operatorname{conv}(S)=\left{\sum_{i=1}^{k} \alpha_{i} x_{i} | \sum_{i=1}^{k} \alpha_{i}=1, x_{i} \in S, \alpha_{i} \geq 0, k&gt;0\right}<br>$$</p>
</li>
<li><p><strong>relative interior point</strong>: $\operatorname{ri}(S)$:<br>$$<br>\operatorname{ri} S=\left{x \in \operatorname{aff} S | \exists \varepsilon&gt;0, B_{\varepsilon}(x) \cap \operatorname{aff} S \subset S\right}<br>$$</p>
</li>
</ol>
<blockquote>
<p>Example:</p>
<ul>
<li>The affine hull of a singleton (a set made of one single element) is the singleton itself. </li>
<li>The affine hull of a set of two different points is the line through them. </li>
<li>The affine hull of a set of three points not on one line is the plane going through them. </li>
<li>The affine hull of a set of four points not in a plane in $\mathbb R^3$ is the entire space $\mathbb R^3$. </li>
<li>$\operatorname{aff}(\mathbb R_+^n) = \mathbb R^n$. </li>
</ul>
</blockquote>
<p><strong>Proposition:</strong> </p>
<p>$C$ Is a subset of Euclidean space $\mathcal E$. If $C$ is convex, then<br>$$<br>\operatorname{aff}(\mathrm{ri} C)=\operatorname{aff} C=\operatorname{aff}(\mathrm{cl}(C))<br>$$<br>where $\operatorname{cl}(C)$ us the closure.</p>
<p><u><em><strong>Any point outside a closed convex set can be separated from a set with a hyperplane.</strong></em></u></p>
<p><strong>Proposition:</strong></p>
<p>Let $\Omega\in \mathbb R^n$ be a nonempty closed convex set and let $\bar x \notin \Omega$. Then exists a nonzero vector $v\in \Omega $  such that<br>$$<br>\sup { \langle v,x \rangle | x\in\Omega } &lt; \langle v, \bar x \rangle<br>$$</p>
<blockquote>
<p>In fact, $v=\bar x - \Pi_\Omega(\bar x)$. </p>
</blockquote>
<p><strong>Proposition:</strong> </p>
<p>Let $Ω_1$ and $Ω_2$ be nonempty, closed, convex subsets of $\mathbb R^n$ with $Ω_1 ∩ Ω_2 = ∅$. If $Ω_1$ or $Ω_2$ is bounded, then there is a nonzero element $v ∈ \mathbb R^n$ such that<br>$$<br>\sup { \langle v,x \rangle | x\in\Omega_1 } &lt; \inf { \langle v,y \rangle | y\in\Omega_2 }<br>$$<br><u><em><strong>Separation property in subspace of $\mathbb R^n$, instead of in $\mathbb R^n$.</strong></em></u></p>
<p>$L$ is subspace of $\mathbb R^n$ and let $\Omega \subset L$ be nonempty convex set with $\bar x\in L$ and $\bar x \notin \bar \Omega$ (the closure of $\Omega$). Then there exists a nonzero $v\in L$ such that<br>$$<br>\sup { \langle v,x \rangle | x\in \Omega} &lt; \langle v, \bar x \rangle<br>$$<br><strong>Definition (properly separated):</strong></p>
<p>We say that two nonempty convex sets $\Omega_1$ and $\Omega_2$ can be <strong>properly separated</strong> if there exists a nonzero vector $v \in \mathbb R^n$ such that<br>$$<br>\begin{array}{rcl}<br>\sup \left{ \langle v, x\rangle | x \in \Omega_{1} \right} &amp;\leq&amp;  \ \inf \left{ \langle v, y\rangle | y \in \Omega_{2}\right} \<br>\inf \left{ \langle v, x\rangle | x \in \Omega_{1}\right } &amp;  &lt; &amp; \sup \left{ \langle v, y\rangle | y \in \Omega_{2}\right}.<br>\end{array}<br>$$<br><strong>Proposition:</strong></p>
<p>$\Omega$ is a nonempty convex set. Then $0\notin \operatorname{ri}(\Omega)$ $\iff$ the set $\Omega$ and ${0}$ can be properly separated, i.e., there exists a nonzero $v\in \mathbb R^n$ such that<br>$$<br>\begin{array}{l}{\qquad \begin{array}{c}{\sup \left{\langle v, x\rangle | x \in \Omega \right} \leq 0 } \<br>{\inf \left{\langle v, x\rangle | x \in \Omega \right}&lt;0}\end{array}}\end{array}<br>$$<br>Theorem:</p>
<p>Let $\Omega_1$ and $\Omega_2$ are two nonempty convex subset of $\mathbb R^n$. Then $\Omega_1$ and $\Omega_2$ can be properly separated $\iff$<br>$$<br>\operatorname{ri}(\Omega_1) \cap \operatorname{ri}(\Omega_2) = \empty<br>$$</p>
<p><strong>Definition (Affine independence):</strong></p>
<p>Elements $v_0, \cdots, v_m \in \mathbb R^n, m\ge 1$ are affinely independent, if<br>$$<br>\sum_{i=1}^m \lambda_i v_i = 0,\quad \sum_{i=1}^m \lambda_i = 0 \Longrightarrow \lambda_i = 0, \forall i=0, \cdots,m<br>$$</p>
<blockquote>
<p>Like linear independence but without the restriction that the subset of lower dimension the points lies in contains the origin. So, three points are affinely independent if the smallest flat thing containing them is a plane. They are affinely dependent if they lie on a line (or are the same point)</p>
</blockquote>
<p><strong>Proposition</strong></p>
<ol>
<li>Set $\Omega$ is affine $\iff$ $\Omega$ contains all affine combinations of its elements.</li>
<li>$\Omega_1, \Omega_2$ are affine set, $\Longrightarrow$ $\Omega_1 \times \Omega_2$ is affine set.</li>
<li>Sum and scalar product of an affine set is still affine</li>
<li>Set $\Omega$ is linear subspace of $\mathbb R^n$  $\iff$  $\Omega$ is an affine set containing the origin.</li>
</ol>
<h1 id="3-6-Cones"><a href="#3-6-Cones" class="headerlink" title="3.6 Cones"></a>3.6 Cones</h1><p>Consider $p : \mathcal E → (−∞, ∞]$.<br> (a) $p$ is said to be a <strong>proper function</strong> if $p(x)$ is not identically equal to $∞$. </p>
<p>(b) $p$ is said to be <strong>closed</strong> if its epi-graph $\operatorname{epi}(p) := {(α, x) ∈ \mathbb R×\mathcal E | p(x) &lt; α}$ is a closed subset of $\mathbb R×\mathcal E$. </p>
<p>(c) The domain of $p$ is defined to be the set $\operatorname{dom}(p)={x∈\mathbb R^n |f(x)&lt;∞}$. </p>
<p><strong>[Linearity space] :</strong></p>
<p>$C$ is a closed convex cone. The linearity space of $C$ is the set<br>$$<br>\operatorname{lin}(C) = C \cap (-C)<br>$$<br>It’s the largest linear subspace of $\mathcal E$ contained in $C$.</p>
<p><strong>[Dual and polar cone] :</strong> </p>
<p>Dual cone : $S^* = { y\in S | \langle x,y \rangle \ge 0, \forall x\in S }$</p>
<p>Polar cone: $S^\circ = -S^*$</p>
<p>if $C^*=C$, the it’s self-dual.</p>
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8bxmx5c3yj30ty0myjuf.jpg" style="zoom: 40%">

<p><strong>Proposition:</strong></p>
<p>If $C$ be a cone in $\mathcal E$.</p>
<p>(a). $C^*$ is a closed convex cone.</p>
<p>(b). If $C$ is a nonempty closed convex cone, then $(C^*)^* = C$</p>
<blockquote>
<p>Example.</p>
<p>If $C = \mathbb R_+^n$. Then</p>
<ul>
<li>$\operatorname{aff} (C) = \mathbb R^n$</li>
<li>$\operatorname{lin} (C) = {0}$</li>
<li>$C^* = C$</li>
</ul>
</blockquote>
<h1 id="3-7-Normal-Cones"><a href="#3-7-Normal-Cones" class="headerlink" title="3.7 Normal Cones"></a>3.7 Normal Cones</h1><p>Normal cone of $C$ at $bar x \in C$ is<br>$$<br>N_C(\bar x) = {z\in \mathcal E | \langle z, x-\bar x \rangle \leq 0, \forall x\in C}<br>$$<br><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g8bzu9dz8xj31aq0dqq6s.jpg" style="zoom: 40%"></p>
<p>Proposition:</p>
<p>$C$ is convex set and $\bar x \in C$.</p>
<ul>
<li>$N_C(\bar x)$ is a closed and convex cone.</li>
<li>If $\bar x\in \operatorname{int}(C)$, then $N_C(\bar x) = {0}$</li>
<li>If $C$ is also a cone, then $N_C(\bar x) \subset C^\circ$.</li>
</ul>
<p>Proposition:</p>
<p>$C\subset \mathbb R^n$ is closed convex set. For any $u,y\in C$,<br>$$<br>u\in N_C(y) \iff y=\Pi_C(y+u)<br>$$<br>Proposition:</p>
<p>Let $C_1, C_2 \subset \mathbb R^n$ be convex sets such that $\bar x \in C_1 \cap C_2$.</p>
<ul>
<li><p>$N_{C_1} (\bar x) + N_{C_2} (\bar x) \subset N_{C_1\cap C_2} (\bar x)$</p>
</li>
<li><p>If the relative interior condition holds $\operatorname{ri}(C_1) \cap \operatorname{ri}(C_2) \neq \empty$, then we have<br>$$<br>N_{c_!\cap C_2} (\bar x) \subset N_{C_1}(\bar x) + N_{C_2}(\bar x)<br>$$</p>
</li>
</ul>
<h1 id="3-8-Subgradient"><a href="#3-8-Subgradient" class="headerlink" title="3.8 Subgradient"></a>3.8 Subgradient</h1><p><strong>Definition:</strong></p>
<p>a vector $v$ is a subgradient of $f$ at $x\in \operatorname{dom}(f)$ if<br>$$<br>f(z) \ge f(x)+\langle v,z-x \rangle, \quad \forall z\in \mathcal E<br>$$<br>denoted as $\part f(x)$. If $x\notin \operatorname{dom}(f)$, $\part f(x) = \empty$.</p>
<p><strong>Proposition</strong>:</p>
<p>$\part f$ is a monotone mapping $\longrightarrow$<br>$$<br>\langle v-u, y-x \rangle \ge 0, \quad \forall u\in \part f(x), v\in \part f(y)<br>$$</p>
<blockquote>
<p>example:</p>
<ul>
<li><p>if $f$ is differentiable at $x$, then $\part f(x) = {\nabla f(x)}$</p>
</li>
<li><p>$C$ is a convex subset of $\mathbb R^n$, then<br>$$<br>\part \delta_C(x) = \begin{cases}<br>\empty \quad &amp; \text{if }x\notin C \<br>\mathcal N_C(x)= \text{normal cone of $C$ at $x$} \quad &amp; \text{if }x\in C<br>\end{cases}<br>$$</p>
</li>
<li><p>Let $f(x) = | x |_1$ for $x \in \mathbb R^n$. Then, $\part f(0) = { y\in \mathbb R^n | |y|_\infty \le 1 }$</p>
<p>Proof.</p>
<p>“$\Longrightarrow$”:</p>
<p>Assume $|y|_\infty \le 1$. We need to prove<br>$$<br>f(z)\ge f(0) + \langle y, z-0 \rangle, \forall z\in \mathbb R^n \iff |z|_1 \ge \langle y,z \rangle, \forall z\in \mathbb R^n<br>$$</p>
<p>$$<br>\langle y,z \rangle = y_1z_1 + \cdots + y_nz_n \le |y_1z_1| + \cdots + |y_nz_n| \le |y_1||z_1| + \cdots + |y_n||z_n| \le |z_1|+\cdots+|z_n| = |z|_1.<br>$$</p>
<p>“$\Longleftarrow$”:</p>
<p>Assume $|z|_1 \ge \langle y,z \rangle, \forall z\in \mathbb R^n$. We need to prove $|y|_\infty \le 1$. Prove it by contradiction.</p>
<p>Assume $|y|_\infty &gt;1$. Let $z$ be a set, the $i$th element be $\max(y)$ where $i$ is the index of the max element. Then we have the contradiction.</p>
</li>
</ul>
</blockquote>
<p><strong>Definition:</strong> </p>
<p> Let $f:\mathcal E→(−∞,∞]$ be an extended real-valued function. Define epigraph and effective domain as<br>$$<br>\begin{aligned} \mathrm{epi} f &amp;={(x, \mu) \in \mathcal{E} \times \mathbb{R} | f(x) \leq \mu} \ \mathrm{dom} f &amp;={x \in \mathcal{E} | f(x)&lt;\infty} \end{aligned}<br>$$</p>
<ol>
<li>$f$ is said to be convex (closed) if $\mathrm{epi} f$ is convex (closed).</li>
<li>$f$ is said to be proper if $\mathrm{dom} f$ is nonempty.</li>
<li>$f$ is said to be <u>positively homogeneous</u> if $f(λx) = λf(x)$ for all $x ∈ \mathcal E$ and $λ &gt; 0$. </li>
<li>Any norm function on $\mathcal E$ is <u>positively homogenous</u>.</li>
</ol>
<p><strong>Definition: (Lipschitz)</strong></p>
<p><strong>Theorem (Rademarcher’s theorem)</strong></p>
<p>Let $\mathcal O$ be a open subset of $\mathcal E_1$ and $F:\mathcal O→\mathcal E_2$ is locally Lipschitz on $\mathcal O$, then $F$ is almost everywhere Fréchet differentiable on $\mathcal O$. </p>
<p><strong>Proposition:</strong></p>
<h1 id="3-9-Fenchel-conjugate"><a href="#3-9-Fenchel-conjugate" class="headerlink" title="3.9 Fenchel conjugate"></a>3.9 Fenchel conjugate</h1><p>Fenchel conjugate:<br>$$<br>f^*(y) = \sup { \langle y,x \rangle - f(x) | x\in \mathcal E }, y\in \mathcal \ E<br>$$<br><em><em>$f^</em>$ is always closed and convex, even if $f$ is non-convex or not closed.</em>* </p>
<p><strong>Proposition:</strong></p>
<p>Let $f$ be a closed proper convex function on $\mathcal E$. For any $x\in \mathcal E$, we have the following equivalent conditions for a verctor $x^* \in \mathcal E$:</p>
<ol>
<li>$f(x) + f^*(x^*) = \langle x, x^* \rangle$</li>
<li>$x^* \in \part f(x)$</li>
<li>$x \in \part f^*(x^*)$</li>
<li>$\langle x, x^* \rangle - f(x) =\max_{z\in \mathcal E} {\langle z, x^* \rangle - f(z)}$</li>
<li>$\langle x, x^* \rangle - f^*(x^*) =\max_{z^<em>\in \mathcal E} {\langle x, z^</em> \rangle - f^*(z^*)}$</li>
</ol>
<p><strong>Remark</strong>:</p>
<ul>
<li>$C$  convex set. $\delta_C^*(x) = \sup { \langle x,y \rangle - \delta_C(y) | y\in \mathcal E } = \sup { \langle x,y \rangle | y\in C }$.</li>
<li>if $f:\mathcal E \rightarrow (-\infty, \infty]$ is a proper closed convex function, then $(f^*)^* = f$.</li>
<li>if $f:\mathcal E \rightarrow (-\infty, \infty]$ is a proper closed convex function, positively homogeneous and $f(0)=0$, then $f^* = \delta_C$, where $C=\part f(0)$.</li>
</ul>
<p>Example.</p>
<blockquote>
<ol>
<li><p>$f(x) = |x|<em>#$ is any norm function defined on $\mathcal E$ and $| \cdot|</em><em>$ is the dual norm of $| \cdot |<em>#$, i.e., for any $x\in \mathcal S, |x|</em></em> = \sup_{y\in \mathcal E} {\langle x, y \rangle | |y|<em># \le 1 }$. Since $f$ is a postively homogeneous closed convex function, $f^*=\delta_C$ where<br>$$<br>C:= \part f(0) = B</em>* := {x\in \mathcal E | |x|_*\le 1 }.<br>$$</p>
</li>
<li><p>$f:\mathbb R^n \rightarrow \mathbb R$ is  defined by $f(x)=\max_{i=1,\cdots,n} x_i$. Then<br>$$<br>S:= \part f(0) = { x\in \mathbb R^n | \sum_{i=1}^n x_i = 1, x\ge 0 }.<br>$$<br>since $f$ is positively homogeneous and convex, $f^* =  \delta_S$. It is known that the projection of $x$ onto $S$ admits a fast algorithm with computational complexity $O(n)$.</p>
</li>
<li><p>$g(x) = \max_{i=1,\cdots, n}x_i$ and $f(x)=g(x)+\delta_{\mathbb R_+^n}(x)$. $f(\cdot)$ is a positively homogeneous convex function. Then<br>$$<br>\partial f(0)=\partial g(0)+\partial \delta_{\mathbb{R}<em>{+}^{n}}(0)= S-\mathbb{R}</em>{+}^{n}=\left{x \in \mathbb{R}^{n} | e^{T} x^{+} \leq 1\right}<br>$$<br>where $S:= \part f(0) = { x\in \mathbb R^n | \sum_{i=1}^n x_i = 1, x\ge 0 }$ and $x_{i}^{+}:=\max \left(x_{i}, 0\right)$. Let $x \in \mathbb{R}^{n}$ be given. Define $\ I_{+}=\left{i: x_{i} \geq 0\right}$, $I_{-}=\left{i | x_{i}&lt;0\right}$, and $C=\left{\xi \in \mathbb{R}^{\left|I_{+}\right|} | e^{T} \xi \leq 1, \xi \geq 0\right}$. The projection $\bar{x}=\Pi_{\partial f(0)}(x)$  is given by<br>$$<br>\bar{x}<em>{I</em>{+}}=\Pi_{C}\left(x_{I_{+}}\right)\<br>\bar{x}<em>{I</em>{-}}=x_{I_{-}}<br>$$</p>
</li>
</ol>
</blockquote>
<h1 id="3-10-Semismoothness"><a href="#3-10-Semismoothness" class="headerlink" title="3.10 Semismoothness"></a>3.10 Semismoothness</h1><p>Definition (semismoothness)</p>
<p>semismooth: </p>
<p>strongly semismooth</p>
<p>Theorem.</p>
<p>Any convex function is semismooth</p>
<p>Piecewise smooth functions are semismooth functions.</p>
<h1 id="3-11-Moreau-Yosida-regularization-and-proximal"><a href="#3-11-Moreau-Yosida-regularization-and-proximal" class="headerlink" title="3.11 Moreau-Yosida regularization and proximal"></a>3.11 Moreau-Yosida regularization and proximal</h1><p>mapping</p>
<p><strong>Definition:</strong></p>
<p>$f: \mathcal E \rightarrow (-\infty, \infty]$ is a closed proper convex function.<br>$$<br>\begin{array}{ll}{\text { MY regularization of } f \text { at } x:} &amp; {M_{f}(x)=\min <em>{y \in \mathcal{E}}\left{\phi(y ; x):=f(y)+\frac{1}{2}|y-x|^{2}\right}} \ {\text { Proximal mapping } f \text { at } x:} &amp; {P</em>{f}(x)=\operatorname{argmin}<em>{y \in \mathcal{E}}{\phi(y ; x):=f(y)+\frac{1}{2}|y-x|^{2}}}\end{array}<br>$$<br>We also have<br>$$<br>\begin{array}{ll} {M</em>{\lambda f}(x)=\min <em>{y \in \mathcal{E}}\left{\phi(y ; x):=f(y)+\frac{1}{2\lambda}|y-x|^{2}\right}} \ {P</em>{\lambda f}(x)=\operatorname{argmin}_{y \in \mathcal{E}}{\phi(y ; x):=f(y)+\frac{1}{2\lambda}|y-x|^{2}}}\end{array}<br>$$<br><strong>Proposition</strong> </p>
<ol>
<li>$P_f(x)$ exists and it unique.</li>
<li>$M_f(x)\le f(x)$ for all $x\in \mathcal E$.</li>
</ol>
<p><strong>Description:</strong></p>
<ul>
<li><p><strong>MY regularization</strong> is also referred to as the <strong>Moreau envelope</strong> of $f$ with parameter $λ$.</p>
</li>
<li><p>The Moreau envelope $M_f$ is essentially a <strong>smoothed</strong> or <strong>regularized</strong> form of $f$, it has <strong>domain</strong> $\mathbb R^n$ even when $f$ does not.</p>
</li>
<li><p>It is <strong>continuously differentiable</strong>, even when $f$ is not. </p>
</li>
<li><p>The sets of minimizer of $f$ and $M_f$ are the same. The problems of minimizing $f$ and $M_f$ are thus equivalent, and the latter is always a smooth optimization problem ($M_f$ may be difficult to evaluate).<br>$$<br>\mathrm {argmin} {f(x)|x\in \mathcal E} = \mathrm{argmin} {M_f(x)|x\in\mathcal E}<br>$$</p>
</li>
<li><p>To see why $M_f$ is a smoothed form of $f$, we have<br>$$<br>M_f = (f^* + \frac{1}{2} |\cdot|_2^2)^*<br>$$<br>Because we have $M_f^{**} = M_f$, it can be interpreted as obtaining a smooth approximation to a function by taking its conjugate, adding regulation and then taking the conjugate again. With no regularization, it would simply give the original function. With the quadratic regularization, it gives a smooth approximation.</p>
</li>
</ul>
<p><strong>Basic Operations:</strong></p>
<p><strong>Theorem (Moreau decomposition)</strong></p>
<p>Let $f$ be closed proper convex function and $f^*$ be its conjugate. Then<br>$$<br>\begin{aligned}<br>x&amp;=P_f(x)+P_{f^*}(x), \forall x\in \mathcal E \<br>\frac{1}{2} |x|^2 &amp;= M_f(x) + M_{f^*}(x)<br>\end{aligned}<br>$$<br>For any $t&gt;0$,<br>$$<br>\begin{aligned}<br>x&amp;=P_{tf}(x)+tP_{tf^*}(\frac{x}{t}), \forall x\in \mathcal E \<br>\frac{1}{2} |x|^2 &amp;= M_{tf}(x) + tM_{tf^*}(\frac{x}{t})<br>\end{aligned}<br>$$</p>
<blockquote>
<p><strong>Examples:</strong></p>
<ol>
<li><p>$f(x)=\lambda |x|_1$, $f^*(z)=\delta_C(z)$ where $C=\part f(0)={z\in \mathbb R^n | |z|_\infty \le \lambda }$</p>
</li>
<li><p>$C$ is closed and convex. $f=\delta_C$,<br>$$<br>P_{\delta_{C}}(x)=\operatorname{argmin}<em>{y \in \mathcal{E}}\left{\delta</em>{C}(y)+\frac{1}{2}|y-x|\right}=\operatorname{argmin}<em>{y \in C} \frac{1}{2}|y-x|^{2}=\Pi</em>{C}(x)<br>$$<br>Suppose $C=\mathbb S_+^n$, the cone of $n\times n$ symmetric PSD matrices. Then<br>$$<br>\Pi_C(x)=Q\mathrm{diag}d_+ Q^T \quad \text{using spectral decomposition } x=Q \mathrm{diag}dQ^T<br>$$</p>
</li>
</ol>
</blockquote>
<p><strong>Theorem:</strong></p>
<ol>
<li><p>Proximal mapping is nonexpansive, like projection.</p>
<p>$P_f$ and $Q_f:=I-P_f$ are firmly nonexpansive, i.e.,<br>$$<br>|P_f(x)-P_f(y)|^2 \le \langle P_f(x)-P_f(y), x-y \rangle, \quad \forall x,y\in \mathcal E \<br>|Q_f(x)-Q_f(y)|^2 \le \langle Q_f(x)-Q_f(y), x-y \rangle, \quad \forall x,y\in \mathcal E<br>$$<br>Hence $|P_f(x)-P_f(y)| \le |x-y|$. That is, $P_f$ and $Q_f$ are lipstchitz continuous with modulus 1.</p>
</li>
<li><p>$M_f$ is continuously differentiable and $\nabla M_f(x) = x-P_f(x), \forall x\in \mathcal E$.</p>
</li>
<li><p>$\nabla M_{tf}(x)=tP_{f^*/t}(x/t)$ and $\nabla M_{f^*/t}(x)=t^{-1}P_{tf}(tx)$</p>
</li>
</ol>
<p><strong>Proposition:</strong></p>
<p>$\part P_f(x)$ has the following properties:</p>
<ol>
<li>any $V\in \part P_f(x)$ is self-adjoint.</li>
<li>$\langle Vd,d \rangle \ge |Vd|^2$ for any $V\in \part P_f(x)$ and $d\in \mathcal E$.</li>
</ol>
<h1 id="3-12-Directional-Derivatives-of-matrix-valued-function"><a href="#3-12-Directional-Derivatives-of-matrix-valued-function" class="headerlink" title="3.12 Directional Derivatives of matrix-valued function"></a>3.12 Directional Derivatives of matrix-valued function</h1><p>​    </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/7_Nonlinear%20Conic%20Programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/7_Nonlinear%20Conic%20Programming/" class="post-title-link" itemprop="url">7. Nonlinear Conic Programming</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:02:29" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>Description</strong>:</p>
<p><strong>Conic optimization</strong> is a subfield of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Convex_optimization">convex optimization</a> that studies problems consisting of minimizing a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Convex_function">convex function</a> over the intersection of an <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Affine_subspace">affine subspace</a> and a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Convex_cone">convex cone</a>.</p>
<p>The class of conic optimization problems includes some of the most well known classes of convex optimization problems, namely <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Linear_programming">linear</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Semidefinite_programming">semidefinite programming</a>.</p>
<p><em><strong>Certain special cases of conic optimization problems have notable closed-form expressions of their dual problems.</strong></em></p>
<h1 id="7-1-Nonlinear-conic-programming"><a href="#7-1-Nonlinear-conic-programming" class="headerlink" title="7.1 Nonlinear conic programming"></a>7.1 Nonlinear conic programming</h1><p><strong>Optimization problem:</strong><br>$$<br>\begin{array}{cl} \text{(OP)} &amp;<br>\begin{array}{cl}<br>{\min} &amp; {f(x)} \ {\text {s.t.}} &amp; {g(x)=0} \ {} &amp; {h(X) \in \mathcal C}<br>\end{array}<br>\end{array}<br>$$<br>where $f: \mathcal X \rightarrow \mathbb R$, $g: \mathcal X \rightarrow \mathcal U$ and $h: \mathcal X \rightarrow \mathcal U$ and $h:= \mathcal X \rightarrow \mathcal V$ are continuous functions. $\mathcal X, \mathcal U, \mathcal V$ are finite dimensional Euclidean spaces each equipped with a scalar product $\langle \cdot, \cdot \rangle$ and its induced norm $| \cdot |$, and <u><strong>$C\in V$ is a closed convex set.</strong></u></p>
<p><strong>Example :</strong></p>
<blockquote>
<ul>
<li><p>NLP is one kind of OP</p>
<p>$\mathcal X = \mathbb R^N, \mathcal U = \mathbb R^m, \mathcal V = \mathbb R^p, \mathcal C = \mathbb R_+^p$<br>$$<br>\begin{array}{cl} \text{(NLP)} &amp;<br>\begin{array}{cl}<br>{\min} &amp; {f(x)} \ {\text {s.t.}} &amp; {g(x)=0} \ {} &amp; {h(x)\geq 0}<br>\end{array}<br>\end{array}<br>$$</p>
</li>
<li><p>==Linear SemiDefinite Programming==<br>$$<br>\begin{array}{cl} \text{(SDP)} &amp;<br>\begin{array}{cl}<br>{\min} &amp; {\langle C, X\rangle} \ {\text {s.t.}} &amp; {\mathcal{A}(X)-b=0} \ {} &amp; {X \succeq 0 \quad\left(\equiv X \in \mathbb{S}<em>{+}^{n}\right)}<br>\end{array}<br>\end{array}<br>$$<br>$\mathcal X = \mathbb S^n$, the real Euclidean space of $n\times n$ real symmetric matrices with the standard trace inner product $\langle X,Y \rangle = \operatorname{tr} (X^T Y)$ and its induced norm $| X | = \sqrt{\langle X, X \rangle}$. $\mathcal C = \mathbb S</em>+^n$ (the cone of positive semidefinite matrices). The linear map $\mathcal A(X) = \begin{bmatrix} \langle A_1, X \rangle \ \vdots \  \langle A_m, X \rangle  \end{bmatrix}$.</p>
<p>Let $\mathcal A^* : \mathbb R^m \rightarrow \mathbb S^n$ be adjoint of $\mathcal A$ as $\mathcal A^* y = \sum\limits_{k=1}^m y_kA_k, y\in \mathbb R^m$.<br>$$<br>\langle \mathcal A^* y, X \rangle = \langle y, \mathcal A X \rangle<br>$$<br>if $\mathcal A(X) = \operatorname{diag} (X)$, then $A_k = E_{kk}$, and $\mathcal A^* y =  \operatorname{diag} (y)$.</p>
</li>
</ul>
<blockquote>
<p>Let $G \in \mathbb R^{p\times n}$ and $H\in \mathbb R^{q\times n}$. For the linear map $\mathcal A: \mathbb S^m \rightarrow \mathbb R^{p \times q}$ defined by $\mathcal A(X) = GXH^T$ . Show that the adjoint map $\mathcal A^*: \mathbb R^{p \times q} \rightarrow \mathbb S^n$ is given by<br>$$<br>\mathcal A^*(Y) = \frac{1}{2} \left( G^TYH+H^TY^TG \right)<br>$$<br><strong>Solution:</strong></p>
<p>for $Y\in \mathbb R^{n\times n}$, we can have<br>$$<br>\begin {aligned}<br>\langle Y, \mathcal A(X) \rangle &amp; = \operatorname{tr} (Y^T \mathcal A(X)) \<br>&amp; = \operatorname{tr} (Y^TGXH^T) \<br>&amp; = \operatorname{tr} (H^TY^TGX) \<br>&amp; = \frac{1}{2} \operatorname{tr} ( (H^TY^TG + G^TYH)X ) \<br>&amp; = \langle X, \underbrace{\frac{1}{2} (H^TY^TG + G^TYH)}_{\text{symmetric}} \rangle<br>\end{aligned}<br>$$<br>because<br>$$<br>\begin{aligned}<br>\operatorname{tr} (H^TY^TGX) &amp; = \operatorname{tr} (XH^TY^TG) \<br>&amp; =  \operatorname{tr} (G^TYHX^T)\<br>&amp; =  \operatorname{tr} (G^TYHX) \quad X\text{ is symmetric}\<br>\end{aligned}<br>$$</p>
</blockquote>
<ul>
<li><p>Semidefinite least squares problem</p>
<p>if $f(X) = \frac{1}{2} | X-B |^2$ where $B\in \mathbb S^n$ is a given matrix,<br>$$<br>\begin {array}<br>\text{(SDPLS)} &amp; \min\left{ \frac{1}{2} | X-B |^2 \ \vert \ \mathcal A(X)=b, X \succeq 0 \right}<br>\end{array}<br>$$</p>
</li>
</ul>
</blockquote>
<p><em>Let  $\mathcal Y := \mathcal U \times \mathcal V$, $\mathcal K := {0^\mathcal U} \times C \subset \mathcal Y$. Define $G: \mathcal X \rightarrow \mathcal Y$ by $G(x):= (g(x), h(x)), x\in \mathcal X$.</em></p>
<h2 id="7-1-1-COP-problem"><a href="#7-1-1-COP-problem" class="headerlink" title="7.1.1 COP problem"></a>7.1.1 COP problem</h2><p>==Then rewrite the OP into more compact form COP==<br>$$<br>\begin{array}{cl} \text{(COP)} &amp;<br>\begin{array}{cl}<br>{\min} &amp; {f(x)} \ {\text {s.t.}} &amp; {G(x)\in \mathcal K} \<br>\end{array}<br>\end{array}<br>$$</p>
<ul>
<li>Lagrangian function $L : \mathcal X \times \mathcal Y \rightarrow \mathbb R$ for COP by<br>$$<br>L(x,\mu) = f(x) - \langle \mu, G(x) \rangle, x\in \mathcal X, \mu\in \mathcal Y \<br>\theta(\mu) := \inf { L(x,\mu) | x\in \mathcal X }<br>$$</li>
</ul>
<p>==Dual Problem of COP==:<br>$$<br>\begin{array}{cl}<br>{\text{(COD)}} &amp; \max\left{ \inf \theta(\mu) | \mu \in \mathcal K^* \right}<br>\end{array}<br>$$<br>where $\mathcal K^* = \mathcal U \times  \mathcal C^*$ is the dual cone of $\mathcal K$. Because $({ 0^\mathcal U })^* = \mathcal U$ and $\mathbb S_+^n$ is self-dual.</p>
<p>We can see in the conic programming, we only consider $μ∈\mathcal K^*$ in the dual problem. However, if needed, we can also put this constraint to the Lagrangian dual function and Lagrangian function like what we usually do in the basic nonlinear programming.</p>
<p>We also define the Lagrangian function in the conic programming as $L(x,μ)=f(x)$ <strong>minus</strong> $⟨μ,G(x)⟩$ <strong>instead of plus</strong>. This is for convenience because the cone is usually defined as “positive”, which means $h(x)⪰0$ in the conic programming <strong>instead of</strong> $h(x)≤0$ in the basic nonlinear programming is the common case.</p>
<blockquote>
<ul>
<li>NLP</li>
</ul>
<p>$\mathcal K = { 0^m } \times \mathbb R_+^p$, and $\mathcal K^* =\mathbb R^m \times \mathbb R_+^p$. The dual problem is<br>$$<br>\max { \theta(\lambda,\rho) \ \vert \ (\lambda, \rho)\in \mathcal K^* = \mathbb R^m \times \mathbb R_+^n  } \<br>\theta (\lambda,\rho) = \inf { f(x)- \langle (\lambda,\rho),(g(x),h(x)) \rangle \ \vert \ x\in \mathbb R^n  }<br>$$</p>
<ul>
<li>linear SDP</li>
</ul>
<p>$\mathcal K = {0^m} \times \mathbb S_+^n$. For $X\in \mathbb S^n$, $(y,Z) \in \mathcal y := \mathbb R^m \times \mathbb S^n$.<br>$$<br>\begin{aligned}<br>L(X,y,Z) &amp; = \langle C,X \rangle - \langle (y,Z), (\mathcal A(X)-b, X) \rangle \<br>&amp; = \langle C,X \rangle - \langle y, \mathcal A(X)-b\rangle - \langle Z,X \rangle\<br>&amp; = \langle C,X \rangle - \langle \mathcal A^* y, X \rangle + \langle b, y \rangle - \langle Z,X \rangle \<br>&amp; = \langle b,y \rangle + \langle C - \mathcal A^* y -Z, X \rangle<br>\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned}<br>\theta (y,Z) &amp;= \inf { L(X,y, Z) \ \vert \ X\in \mathbb S^n } \<br>&amp;= \inf { \langle b,y \rangle +  \langle C - \mathcal A^* y -Z, X \rangle \ \vert \ X\in \mathbb S^n } \<br>&amp; = \begin{cases}<br>\langle b,y \rangle \quad \text{if } C-\mathcal A^* y - Z = 0 \<br>-\infty \quad \text{otherwise}<br>\end{cases}<br>\end{aligned}<br>$$</p>
<p>Dual problem of SDP is<br>$$<br>\max {\theta (y,Z) \ \vert \ (y,Z)\in \mathcal K^* } = \max \left{ \langle b,y \rangle \ \vert \ C-\mathcal A^* y - Z = 0, y\in \mathbb R^m, Z\succeq0 \right}<br>$$</p>
<ul>
<li>SDPLS</li>
</ul>
<p>$\mathcal K = {0^m}\times \mathbb S_+^n$. For $X\in \mathbb S^n$, $(y,Z)\in \mathcal Y:=\mathbb R^m \times \mathbb S^n$<br>$$<br>\begin{aligned}<br>L(X,y,Z) = \frac{1}{2} | X-B |^2 + \langle y,b-\mathcal A(X) \rangle + \langle Z,-X \rangle \<br>= \langle b,y \rangle + \frac{1}{2} | B|^2 + \frac{1}{2} |X |^2 - \langle B + \mathcal A^* y +Z, X \rangle<br>\end{aligned}<br>$$<br>thus, $\min { L(X,y,Z) \vert X\in \mathbb S^n }$ can be found by<br>$$<br>\nabla_XL(X,y,Z) = X-(B + \mathcal A^* y +Z) = 0<br>$$<br>substituting, get<br>$$<br>\begin{aligned}<br>\theta(y,Z) := \min{ L(X,y,Z) | X\in \mathbb S^n } \<br>= \langle b,y \rangle + \frac{1}{2} | B|^2 - \frac{1}{2} |X |^2 \<br>= \langle b,y \rangle + \frac{1}{2} | B|^2 - \frac{1}{2} | B + \mathcal A^* y +Z |^2<br>\end{aligned}<br>$$<br>Dual problem is<br>$$<br>\begin{aligned}<br>&amp; \quad \ \max \left{ \theta(y,Z) | (y,Z)\in \mathcal K^* \right} \<br>&amp;= \max \left{ \langle b,y \rangle  - \frac{1}{2} | B + \mathcal A^* y +Z |^2 | y\in \mathbb R^m, Z\in \mathbb S_+^n \right} + \frac{1}{2} | B|^2 \<br>&amp;= \min \left{ \langle -b,y \rangle  + \frac{1}{2} | B + \mathcal A^* y +Z |^2 | y\in \mathbb R^m, Z\in \mathbb S_+^n \right}\<br>&amp;= \min\limits_{y\in \mathbb R^m} \langle -b,y \rangle  + \min \left{ \frac{1}{2} | B + \mathcal A^* y +Z |^2 | Z\in \mathbb S_+^n \right}\<br>&amp;= \min\limits_{y\in \mathbb R^m} \phi(y):= \langle -b,y \rangle + \frac{1}{2} | \Pi_{\mathbb S_+^n} (B+A^* y) |^2\<br>\end{aligned}<br>$$<br>because<br>$$<br>\begin{aligned}<br>\text{set } G = B + \mathcal A^* y +Z, &amp; \<br>&amp; G  = \Pi_{\mathbb S_+^n}(G) - \Pi_{\mathbb S_+^n} (-G) \<br>\Rightarrow \  &amp;G + \Pi_{\mathbb S_+^n} (-G) = \Pi_{\mathbb S_+^n} (G) \<br>\text{set } Z =\Pi_{\mathbb S_+^n} (-G), \quad \quad &amp;\<br>\therefore &amp; \ B + \mathcal A^* y +Z = \Pi_{\mathbb S_+^n} (B+A^* y)<br>\end{aligned}<br>$$<br>This dual problem is unconstrained problem, we can solve it by finding the root of the gradient<br>$$<br>0=\nabla\phi(y) = -b+\mathcal A \Pi_{\mathcal S_+^n} (B+\mathcal A^*y)<br>$$</p>
<ul>
<li>Sparse Regression problem:<br>$$<br>\min \left{\frac{1}{2} | Ax-b |^2 + \lambda | x |_1 | x\in \mathbb R^n \right}<br>$$<br>Let $u=b-Ax$, rewrite as<br>$$<br>\begin{array}{cl} \text{(P)} &amp;<br>\begin{array}{cl}<br>{\min} &amp; {f(x)+g(x)} \ {\text {s.t.}} &amp; {u+Ax=b} \<br>\end{array}<br>\end{array}<br>$$<br>where $f(x) = \frac{1}{2} |u|^2$ and $g(x) = \lambda |x |_1$<br>$$<br>\begin{aligned}<br>L(u,x;\xi) &amp;= f(u) + g(x) + \langle \xi, b-u-Ax \rangle \<br>&amp;= f(u) - \langle \xi,u \rangle + g(x) - \langle A^T\xi, x \rangle + \langle \xi,b \rangle<br>\end{aligned}<br>$$</li>
</ul>
<p>The dual function is<br>$$<br>\begin{aligned}<br>\min\limits_{u,x} L(u,x;\xi) &amp; = \langle \xi,b \rangle + \min\limits_u \left{ f(u) - \langle \xi,u \rangle \right} + \min\limits_x \left{ g(x) - \langle A^T\xi, x \rangle \right} \<br>&amp; = \langle \xi,b \rangle - \frac{1}{2} |u|^2 + \min\limits_x \left{ g(x) - \langle A^T\xi, x \rangle \right} \<br>&amp; = \langle \xi,b \rangle - \frac{1}{2} |u|^2 + \delta_{B_\lambda}(A^T\xi)<br>\end{aligned}<br>$$<br>where $B_\lambda = { v\in \mathbb R^n \ \vert \  | v|_\infty\leq\lambda }$.  (reason refer to Chapter 3.12 Fenchel conjugate)</p>
<p>The dual problem is<br>$$<br>\begin{aligned}<br>\max\limits_{\xi \in \mathbb R^m} \left{ \min\limits_{u,x} L(u,x;\xi) \right} &amp;= \max\limits_{\xi \in \mathbb R^m} \langle \xi,b \rangle - \frac{1}{2} |u|^2 + \delta_{B_\lambda}(A^T\xi) \<br>&amp;= - \min\limits_{\xi \in \mathbb R^m} \langle \xi,-b \rangle + \frac{1}{2} |u|^2 - \delta_{B_\lambda}(A^T\xi)<br>\end{aligned}<br>$$</p>
</blockquote>
<h2 id="7-1-2-Langrage-Multiplier-amp-KKT"><a href="#7-1-2-Langrage-Multiplier-amp-KKT" class="headerlink" title="7.1.2 Langrage Multiplier &amp; KKT:"></a>7.1.2 Langrage Multiplier &amp; KKT:</h2><p>$\bar \mu \in \mathcal Y$ is a langrage multiplier at feasible point $\bar x$ ($G(\bar x) \in \mathcal K$), if it satisfiy the KKT condition<br>$$<br>0=\nabla_x L(\bar x, \bar \mu) = \nabla f(\bar x) - \nabla G(\bar x) \bar\mu, \quad  0\in \bar\mu+\mathcal N_\mathcal K (G(\bar x))<br>$$<br>or<br>$$<br>0=\nabla_x L(\bar x, \bar \mu) = \nabla f(\bar x) - \nabla G(\bar x) \bar\mu, \quad  \mathcal K \ni G(\bar x) \perp \bar \mu \in \mathcal K^*<br>$$</p>
<blockquote>
<p>because<br>$$<br>\begin{aligned}<br>0\in \bar\mu+\mathcal N_\mathcal K (G(\bar x)) &amp;\iff -\bar \mu \in \mathcal N_\mathcal K (G(\bar x))\<br>&amp;\iff G(\bar x)\in \mathcal K, \quad \langle -\bar\mu, d-G(\bar x) \rangle \leq 0, \quad  \forall d\in \mathcal K \<br>&amp;\iff G(\bar x)\in \mathcal K, \quad \langle \bar\mu, G(\bar x) \rangle = 0, \quad \langle \bar\mu, d \rangle \geq 0, \quad  \forall d\in \mathcal K \ (\text{set }d=2G(\bar x) \text{ and } d=0)\<br>&amp;\iff G(\bar x)\in \mathcal K, \quad \langle \bar\mu, G(\bar x) \rangle = 0, \bar\mu\in\mathcal K^* \ (\because \text{definition of dual cone})\<br>&amp;\iff \mathcal K \ni G(\bar x) \perp \bar \mu \in \mathcal K^*<br>\end{aligned}<br>$$</p>
</blockquote>
<ul>
<li>$\mathcal M(\bar x)$ maybe empty or unbounded.</li>
</ul>
<blockquote>
<p>Example:</p>
<ul>
<li><p>NLP:</p>
<p>Let $\bar\mu = (\lambda,\rho)$, $G(x) = (g(x),h(x))$, $\mathcal C=\mathbb R_+^p$, $\mathcal K={0_m}\times\mathcal C$. KKT conditions:<br>$$<br>\nabla f(\bar x) - \nabla g(\bar x)\lambda - \nabla h(\bar x)\rho = 0, G(\bar x) = (g(\bar x), h(\bar x)), \langle h(\bar x),\rho \rangle = 0, \rho\in\mathcal C^*=\mathbb R_+^p<br>$$</p>
</li>
<li><p>DNN projection</p>
<p>problem:<br>$$<br>\min \left{ \frac{1}{2}|X-B|^2 \ \vert \ X\in \mathbb S_+^n \cap \mathcal N^n \right}<br>$$<br>Rewritten it as<br>$$<br>\min \left{ f(X) := \frac{1}{2}|X-B|^2 \ \vert \ G(X)=(X,X)\in \mathcal K := \mathbb S_+^n \times \mathcal N^n \right}<br>$$<br>where $\mathcal N^n$ denote the cone of $n\in \mathcal N^n$ symmetric and element wise nonnegative matrices.</p>
<p>Lagrange function:<br>$$<br>\begin{aligned}<br>L(X;S,Z) &amp; = \frac{1}{2}|X-B|^2 + \langle (S,Z), (X,X) \rangle \ \vert \ (S,Z)\in \mathcal K^* := \mathbb S_+^n \times \mathcal N^n \<br>\theta(S,Z) &amp; = \inf \left{ \frac{1}{2}|X-B|^2 + \langle (S,Z), (X,X) \rangle \ \vert \ (S,Z)\in \mathcal K^* \right}<br>\end{aligned}<br>$$<br>dual problem:<br>$$<br>\max \left{ \theta(S,Z) \ \vert \ (S,Z)\in \mathcal K^* \right}<br>$$<br>KKT condition:<br>$$<br>\nabla f(X) - \nabla G(X)(S,Z) = 0, \ (S,Z)\in \mathcal K^* \<br>\iff X-B-S-Z = 0, S \in \mathbb S_+^n, Z\in \mathcal N^n<br>$$<br>Rewrite Dual Problem:<br>$$<br>\begin{aligned}<br>\max \left{ \theta(S,Z) \ \vert \ (S,Z)\in \mathcal K^* \right} &amp;= \max \left{ \theta(S,Z) \ \vert \ X-B-S-Z = 0,(S,Z)\in \mathcal K^* \right} \<br>&amp;= \max \left{ \frac{1}{2}|S+Z|^2 + \langle (S,Z), (B+S+Z,B+S+Z) \rangle  \vert \ (S,Z)\in \mathcal K^* \right}\<br>&amp;= \max \left{ \frac{1}{2}|S+Z|^2 + \langle S, (B+S+Z) \rangle + \langle Z, (B+S+Z) \rangle  \vert \ (S,Z)\in \mathcal K^* \right}\<br>&amp;= \max \left{ \frac{1}{2}|S+Z|^2 + |S|^2 + |Z|^2+ 2\langle S, B \rangle + \langle (S+Z),B \rangle  \vert \ (S,Z)\in \mathcal K^* \right}\<br>\end{aligned}<br>$$</p>
</li>
</ul>
</blockquote>
<h2 id="7-1-3-Robinson’s-constraint-qualification"><a href="#7-1-3-Robinson’s-constraint-qualification" class="headerlink" title="7.1.3 Robinson’s constraint qualification:"></a>7.1.3 Robinson’s constraint qualification:</h2><p>conditions $\longrightarrow$ KKT condition to be necessary for a local minimizer $\bar x$.</p>
<p>conditions is to ensure the existence of Lagrange multipliers and the boundedness of $\mathcal M(\bar x)$.</p>
<p>Robinson’s condition generalize the LICQ.</p>
<ul>
<li><p>Definition</p>
<p>$\bar x$ is feasible solution, Robinson’s CQ means<br>$$<br>0 \in \operatorname{int} \left( G(\bar x) + G’(\bar x) \mathcal X -\mathcal K \right)<br>$$</p>
</li>
</ul>
<blockquote>
<p>example:</p>
<ol>
<li><p>when $\mathcal X=\mathbb R^n, G(x)=g(x), \mathcal K={\mathbf 0_m}$, the Robinson’s CQ reduces to $0\in \mathrm{int}(G’(\bar x) \mathbb R^n)$, which is equivalent to $\mathrm{Range}(G’(\bar x))=\mathbb R^m \iff G’(\bar x)^T$ is 1-1.</p>
</li>
<li><p>For NLP, the Robinson’s CQ can be written as<br>$$<br>0\in\mathrm {int} \left( \begin{pmatrix} g(\bar x) \ h(\bar x)\end{pmatrix} + \begin{pmatrix} g’(\bar x) \ h’(\bar x)\end{pmatrix}\mathbb R^n - \begin{pmatrix} {\mathbf 0_m} \ \mathbb R_+^p \end{pmatrix} \right)<br>$$<br> ==MFCQ==:<br>$$<br>\begin{array}{l}{\left{\nabla g_{i}(\bar{x}) | i=1, \ldots, m\right} \quad \text { are linearly independent }} \ {\exists d \in \mathbb{R}^{n} \text { s.t. } \quad\left\langle\nabla g_{i}(\bar{x}), d\right\rangle= 0 \quad \forall i=1, \ldots, m, \quad\left\langle\nabla h_{j}(\bar{x}), d\right\rangle&gt; 0 \quad \forall j \in J(\bar{x})} \ {B_{g}(\bar{x})=\left[\nabla g_{1}(\bar{x}), \ldots, \nabla g_{m}(\bar{x})\right], \quad B_{h}(\bar{x})=\left[\nabla h_{j}(\bar{x}): j \in J(\bar{x})\right]} \ {B(\bar{x})=\left[B_{g}(\bar{x}), B_{h}(\bar{x})\right] \in \mathbb{R}^{n \times(m+|J|)}}\end{array}<br>$$<br>This is equivalent to saying that $Bg(\bar x)$ has full column rank and the systems<br>$$<br>B_g(\bar x)^Td = \mathbf 0_m, B_h(\bar x)^Td &gt;0<br>$$<br>has a solution.</p>
<blockquote>
<p>In the case when $\bar x$ satisfies the LICQ, then $B(\bar x)$ has full column rank and hence $B(\bar x)^T$ full row rank. In this case, $\mathrm{Range}(B(\bar x)^T ) = \mathbb R^n$ and it is clear that the linear system $B(\bar x)T d = \mathrm{rhs}$ has a solution for any given vector $\mathrm{rhs}$. In particular, for $\mathrm{rhs} = [\mathbf 0_m; I_{|J|}]$.</p>
</blockquote>
</li>
</ol>
</blockquote>
<p><strong>Proposition</strong>:</p>
<p>Assume $G(\bar x)\in \mathcal K$. The following conditions are equivalent to each other and to Robinson’s CQ:<br>$$<br>\begin{array}{l}{\text { (a) } \quad G^{\prime}(\bar{x}) \mathcal{X}+T_{\mathcal{K}}(G(\bar{x}))=\mathcal{Y}} \ {\text { (b) } \quad\left[G^{\prime}(\bar{x}) \mathcal{X}\right]^{\perp} \cap \quad N_{\mathcal{K}}(G(\bar{x}))={0}}\end{array}<br>$$<br>For a set $S$ in $\mathcal Y$ and a point $y \in S$, the tangent cone to $S$ at $y$ is defined by<br>$$<br>\begin{aligned} T_{S}(y) &amp;=\left{d \in \mathcal{Y} | \exists y^{k} \in S \rightarrow y, t_{k} \downarrow 0, \text { s.t. }\left(y^{k}-y\right) / t^{k} \rightarrow d\right} \ &amp;=\left{d \in \mathcal{Y} | \exists t_{k} \downarrow 0, \operatorname{dist}\left(y+t_{k} d, S\right)=o\left(t_{k}\right)\right} \end{aligned}<br>$$<br>Note that if $S$ is nonempty convex subset of $\mathcal Y$, then<br>$$<br>T_s(y) = \mathrm{closure}({ t(x-y) | x\in S, t\ge 0 })<br>$$<br><strong>Remark:</strong></p>
<ul>
<li><p>for $\mathcal C = \mathbb R_+^p, y\in \mathcal C$. Let $J(y)={j|y_j=0}$. We have that<br>$$<br>T_C(y) = { d\in \mathbb R^p | d_j\ge 0, j\in J(y) }<br>$$</p>
</li>
<li><p>For NLP, above proposition that Robinson’s CQ is equivalent to the following form:<br>$$<br>\left(\begin{array}{c}{g^{\prime}(\bar{x})} \ {h^{\prime}(\bar{x})}\end{array}\right) \mathbb{R}^{n}+\left(\begin{array}{c}{\left{0_{m}\right}} \ {T_{\mathbb{R}_{+}^{p}}(h(\bar{x}))}\end{array}\right)=\left(\begin{array}{c}{\mathbb{R}^{m}} \ {\mathbb{R}^{p}}\end{array}\right)<br>$$</p>
</li>
</ul>
<p><strong>Proposition:</strong> </p>
<p>Assume that $G(\bar x) \in \mathcal  K$. If $\mathcal K$ has a nonempty interior, then <u>Robinson’s CQ</u> is equivalent to<br>$$<br>\exist d \in \mathcal X \quad \text{s.t.} \quad G(\bar x) + G’(\bar x)d \in \mathrm{int}(\mathcal K)<br>$$<br><strong>Proposition</strong>:</p>
<p>$G(\bar x)=(g(x),h(x)), \mathcal K={\mathbf 0_m}\times \mathcal C, G(\bar x)\in \mathcal K$. If $C$ has a nonempty interior, then Robinson’s CQ is equivalent to<br>$$<br>\left{\begin{array}{l}{g^{\prime}(\bar{x}) \text { is onto; }} \ {\exists d \in \mathcal{X} \quad \text { such that } \quad g^{\prime}(\bar{x}) d=0, h(\bar{x})+h^{\prime}(\bar{x}) d \in \operatorname{int}(\mathcal{C})}\end{array}\right.<br>$$</p>
<h1 id="7-2-First-order-necessary-conditions"><a href="#7-2-First-order-necessary-conditions" class="headerlink" title="7.2 First order necessary conditions"></a>7.2 First order necessary conditions</h1><p>Proposition:</p>
<p>$\bar x$ is locally optimal solution of COP. Then, the point $d=0$ is an optimal solution of the following problem:<br>$$<br>\min_{d\in \mathcal X} f’(\bar x)d \<br>\text{s.t. } d\in T_\mathcal F(\bar x)<br>$$<br>where $\mathcal F = {x\in \mathcal X | G(x)\in \mathcal K}$ is the feasible region of COP.</p>
<p>Lemma:</p>
<p>If Robinson’s CQ holds at a feasible point of $\bar x$ of COP, then there exists a constant $c&gt;0$ such that<br>$$<br>\mathrm {dist}(x,G^{-1}(\mathcal K + y)) \le c \ \mathrm{dist}(G(x)-y, \mathcal K)<br>$$<br>for all $(x,y)$ in an open neighborhood of $(\bar x,0)$.</p>
<ul>
<li><p>In particular, when $y=0$, we have for all $x$ in a neighborhood of $\bar x$,<br>$$<br>\mathrm {dist}(x,\mathcal F) = O\ \mathrm{dist}(G(x), \mathcal K)<br>$$</p>
<blockquote>
<p>This lemma is very important because it can be used to measure the distance from the current point to the feasible point. For example, if we have the constraint $Ax=b$, it is difficult to know the distance $\mathrm{dist}(x^c,\mathcal F)$, but it is easy to calculate $||b−Ax^c||$. Since $\mathrm{dist}(x^c,\mathcal F)$ is bounded by $||b−Ax^c||$, we can use $||b−Ax^c||$ to estimate the distance. </p>
</blockquote>
</li>
<li><p>One has<br>$$<br>0\in \mathrm{int}(G(\mathcal X)-\mathcal K)<br>$$</p>
</li>
</ul>
<p><strong>Proposition</strong>:</p>
<p>$\bar x$ is a locally optimal solution of COP. If Robinsons’ CQ holds at $\bar x$, then it holds that<br>$$<br>T_\mathcal F(\bar x) = { d\in \mathcal X | G’(\bar x)d\in T_\mathcal K(G(\bar x)) }<br>$$<br>Hence $d=0$ is an optimal solution of the following problem<br>$$<br>\text{(LCOP)} \quad \min_{d\in \mathcal X} { f’(\bar x)d \  | \ G’(\bar x)d \in T_{\mathcal K} (\bar x) }<br>$$<br><strong>Theorem: (Important)</strong></p>
<p>Suppose that $\bar x$ is a locally optimal solution of (COP). Then, $\mathcal M(\bar x)$ is a nonempty, convex compact subset of $\mathcal Y$ if and only if Robinson’s CQ holds at $\bar x$.</p>
<p>Proposition (Uniqueness of Lagrange Multipliers) </p>
<p>Suppose that $\bar x$ is a locally optimal solution to (COP) and $\mathcal M(\bar x) \neq 0$. Let $\mu_0 \in \mathcal M(\bar x)$.</p>
<ul>
<li><p>$\mathcal M(\bar x)$ is a singleton if and only if<br>$$<br>\left[G^{\prime}(\bar{x}) \mathcal{X}\right]^{\perp} \cap R_{D}\left(-\mu_{0}\right)={0}<br>$$<br>where $D = N_K(G(\bar x))$.</p>
</li>
<li><p>Then, the following condition is sufficient for uniqueness of $\mu_0$:<br>$$<br>\left[G^{\prime}(\bar{x}) \mathcal{X}\right]+\left(T_{\mathcal{K}}(G(\bar{x})) \cap\left[-\mu_{0}\right]^{\perp}\right)=\mathcal{Y}<br>$$</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2021/08/16/Nonlinear%20Optimization/8_ADMM_boyd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/16/Nonlinear%20Optimization/8_ADMM_boyd/" class="post-title-link" itemprop="url">8. Alternating Direction Method of Multipliers (ADMM)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-08-16 12:00:00 / Modified: 15:03:13" itemprop="dateCreated datePublished" datetime="2021-08-16T12:00:00+08:00">2021-08-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Take form of decomposition-coordination procedure (solution of subproblem is coordinated to solution of global problem)</p>
<p>ADMM : benefits of dual decomposition + augmented Lagrangian methods for constrained optimization</p>
<h1 id="0-precursor"><a href="#0-precursor" class="headerlink" title="0 precursor"></a>0 precursor</h1><h2 id="0-1-Gradient-Ascent"><a href="#0-1-Gradient-Ascent" class="headerlink" title="0.1 Gradient Ascent"></a>0.1 Gradient Ascent</h2><p>Primal problem<br>$$<br>\begin{array}{rc}<br>\min &amp; f(x) \<br>\text{s.t.} &amp; Ax=b<br>\end{array}<br>$$<br>Lagrangian function is<br>$$<br>L(x,y) = f(x) + y^T(Ax-b)<br>$$<br>and the dual function is<br>$$<br>g(y) = \inf_x L(x,y) = -f^*(-A^Ty) - b^Ty<br>$$<br>dual problem is<br>$$<br>\max g(y)<br>$$<br>recover a primal optimal point $x^*$ from a dual optimal point $y^*$<br>$$<br>x^* = \mathop{\mathrm{argmin}}<em>x L(x,y^*)<br>$$<br>Solve this dual problem using gradient ascent:<br>$$<br>\begin{array}{rl}<br>x^{k+1} &amp;=&amp; \mathop{\mathrm{argmin}}</em>{x} L(x,y^k) &amp; x\text{-minimization step} \<br>y^{k+1} &amp;=&amp; y^k +\alpha^k(Ax^{k+1} - b) &amp; \text{dual variable update}<br>\end{array}<br>$$<br>dual ascent method can lead to a decentralized algorithm in some case.</p>
<h2 id="0-2-Dual-Decomposition"><a href="#0-2-Dual-Decomposition" class="headerlink" title="0.2 Dual Decomposition"></a>0.2 Dual Decomposition</h2><p>If objective $f$ is separable,<br>$$<br>f(x) = \sum_{i=1}^N f_i(x_i), \text {where } x=(x_1, \cdots, x_N), \ A = [A_1, \cdots, A_N]<br>$$<br>the Lagrangian function is<br>$$<br>L(x,y) = \sum_{i=1}^N L_i(x_i,y) = \sum_{i=1}^N (f_i(x_i)+y^TA_ix_i - \frac{1}{N}(y^Tb)<br>$$<br>which is separable in $x$. <u>This means that $x$-minimization step splits into $N$ separate problems that can be solved in parallel.</u></p>
<p>Solve:<br>$$<br>\begin{array}{rl}<br>x_i^{k+1} &amp;=&amp; \mathop{\mathrm{argmin}}_{x_i} L_i(x_i,y^k) &amp; x_i\text{-minimization step} \<br>y^{k+1} &amp;=&amp; y^k +\alpha^k(Ax^{k+1} - b) &amp; \text{dual variable update}<br>\end{array}<br>$$<br>So, the $x$-minimization step is carried out independently, in parallel.</p>
<p>**Each iteration of the dual decomposition requires a <u>broadcast</u> and <u>gather</u> operation **</p>
<p>[in dual update, collect $A_i x_i^{k+1}$ to compute the residual $Ax^{k+1}-b$. Once global dual variable $y^{k+1}$ is computed, it will be broadcast to $N$ individual $x_i$ minimization steps. ]</p>
<blockquote>
<ul>
<li>[Book] Cooperative distributed multi-agent optimization (this book discusses dual decomposition methods and consensus problems).</li>
<li>Distributed Dual Averaging in Networks (distributed methods for graph-structured optimization problems)</li>
</ul>
</blockquote>
<h2 id="0-3-Augmented-Lagrangians-and-Method-of-Multipliers"><a href="#0-3-Augmented-Lagrangians-and-Method-of-Multipliers" class="headerlink" title="0.3 Augmented Lagrangians and Method of Multipliers"></a>0.3 Augmented Lagrangians and Method of Multipliers</h2><p>augmented lagrangian is to bring robustness to dual ascent method, and to yield convergence without assumptions like strict convexity or finiteness of $f$.</p>
<p>Augmented Lagrangian is<br>$$<br>L_\sigma(x,y) = f(x) + y^T(Ax-b) + \frac{\sigma}{2} |Ax-b|^2<br>$$<br>Dual function is<br>$$<br>g_\sigma(y) = \inf_x L_\sigma(x,y)<br>$$<br>Adding the penalty term is to be differentiable under rather mild conditions on the original problem.</p>
<blockquote>
<p>find gradient of the augmented dual function by minimizing over $x$, then evaluating the resulting equality constraint residual.</p>
</blockquote>
<p>Algorithm:<br>$$<br>\begin{array}{rl}<br>x^{k+1} &amp;=&amp; \mathop{\mathrm{argmin}}_{x} L_\sigma(x,y^k) &amp; x\text{-minimization step} \<br>y^{k+1} &amp;=&amp; y^k +\sigma(Ax^{k+1} - b) &amp; \text{dual variable update}<br>\end{array}<br>$$<br>Here, the parameter $\sigma$ is used as the step size $\alpha^k$. </p>
<p>[The method of multipliers converges under far more general conditions than dual ascent, including cases when $f$ takes on the value $+\infty$ or is not strictly convex.]</p>
<ul>
<li><p>How to choose $\sigma$:</p>
<p>The optimality condition are primal and dual feasibility, i.e.,<br>$$<br>Ax^* - b = 0, \quad \nabla f(x^*) + A^Ty^* = 0<br>$$<br>Then, we have $x^{k+1}$ can minimize $L_\sigma(x,y^k)$, so<br>$$<br>\begin{aligned}<br> 0 &amp;= \nabla_x L_\sigma(x^{k+1}, y^k) \<br>   &amp;= \nabla_x f(x^{k+1}) + A^T(y^k + \sigma(Ax^{k+1}-b)) \<br>   &amp;= \nabla_x f(x^{k+1}) + A^T y^{k+1}<br> \end{aligned}<br>$$<br>So, using $\sigma$ as step size in dual update, the iterate $(x^{k+1}, y^{k+1})$ is dual feasible. $\rightarrow$ primal residual $Ax^{k+1}-b$ converges to 0. $\rightarrow$ optimality.</p>
</li>
</ul>
<p>Shortcoming: When $f$ is separable, the augmented Lagrangian $L_\sigma$ is not separable, so the $x$-minimization step cannot be carried out separately in parallel for each $x_i$. This means that the basic method of multipliers cannot be used for decomposition</p>
<h1 id="1-ADMM"><a href="#1-ADMM" class="headerlink" title="1. ADMM"></a>1. ADMM</h1><p>blend the decomposability of dual ascent with the superior convergence properties of the method of multipliers.</p>
<p>Problem:<br>$$<br>\begin{array}{rcl}<br>\min &amp; f(x)+g(z) \<br>\text{s.t. } &amp; Ax+Bz = c<br>\end{array}<br>$$<br>difference: $x$ splits into two parts, $x$ and $z$, with the objective function separable across the splitting. </p>
<p>optimal value:<br>$$<br>p^* = \inf { f(x)+g(z) | Ax+Bz = c }<br>$$<br>Augmented Lagrangian:<br>$$<br>L_\sigma (x,z;y) = f(x)+g(z)+y^T(Ax+Bz-c)+\frac{\sigma}{2} |Ax+Bz-c|^2<br>$$</p>
<h2 id="1-1-Algorithm"><a href="#1-1-Algorithm" class="headerlink" title="1.1 Algorithm"></a>1.1 Algorithm</h2><h3 id="1-1-1-Unscaled-form"><a href="#1-1-1-Unscaled-form" class="headerlink" title="1.1.1 Unscaled form"></a>1.1.1 Unscaled form</h3><p>$$<br>\begin{aligned}<br>x^{k+1} &amp;= \mathop{\mathrm{argmin}}_x L_\sigma(x,z^k,y^k) &amp; x\text{-minimization}\<br>z^{k+1} &amp;= \mathop{\mathrm{armmin}}_z L_\sigma(x^{k+1},z,y^k) &amp; z\text{-minimization}\<br>y^{k+1} &amp;= y^k + \sigma(Ax^{k+1}+Bz^{k+1}-c)  &amp; \text{dual variable update}<br>\end{aligned}<br>$$</p>
<blockquote>
<p><u><em>why ADMM is alternating direction:</em></u></p>
<p>If use method of multipliers to solve this problem, we will have<br>$$<br>\begin{aligned}<br>(x^{k+1}, z^{k+1}) &amp;= \mathop{\mathrm{argmin}}_{x,z} L_\sigma(x,z,y^k) \<br>y^{k+1} &amp;= y^k + \sigma(Ax^{k+1}+Bz^{k+1}-c)<br>\end{aligned}<br>$$<br>So, the augmented lagrangian is minimized jointly with two variables $x,z$.</p>
<p>But, ADMM update $x$ and $z$ in an alternating or sequential fashion, –&gt; alternating direction.</p>
<p><em>ADMM can be viewed as a version of method of multipliers where a single Gauss-Seidel pass over $x$ and $z$ is used instead of joint minimization.</em></p>
</blockquote>
<h3 id="1-1-2-Scaled-form"><a href="#1-1-2-Scaled-form" class="headerlink" title="1.1.2 Scaled form"></a>1.1.2 Scaled form</h3><p>combine the linear and quadratic terms in the augmented lagrangian and scale the residual variable.</p>
<p>Set $u = \frac{1}{\sigma} y$, the ADMM becomes<br>$$<br>\begin{array}{l}<br>{x^{k+1}:=\underset{x}{\operatorname{argmin}} \left(f(x) + \frac{\sigma}{2} \left|A x+B z^{k}-c+u^{k}\right|^{2}\right)} \<br>{z^{k+1}:=\underset{z}{\operatorname{argmin}} \left(g(z) + \frac{\sigma}{2} \left|A x^{k+1}+B z-c+u^{k}\right|^{2}\right)} \<br>{u^{k+1}:=u^{k}+A x^{k+1}+B z^{k+1}-c}\end{array}<br>$$<br>Define residual at $k$ iteration as $r^k=Ax^k+Bz^k-c$, we have<br>$$<br>u^k = u^0 + \sum_{j=1}^k r^j<br>$$</p>
<h2 id="1-2-Convergence"><a href="#1-2-Convergence" class="headerlink" title="1.2 Convergence"></a>1.2 Convergence</h2><p><strong>Assumption 1:</strong></p>
<p>function $f: \mathbb R^n \rightarrow \mathbb R \cup {+\infty}$ and $g: \mathbb R^m \rightarrow \mathbb R \cup {+\infty}$ are closed, proper and convex</p>
<blockquote>
<p>this assumption means the subproblem in $x$-update and $z$-update are solvable. </p>
<p>allows $f$ and $g$ are nondifferentiable and to assume value $+\infty$. For example, $f$ is indicator function</p>
</blockquote>
<p><strong>Assumption 2:</strong></p>
<p>Unaugmented Lagrangian $L_0$ has a saddle point. This means there exists $(x^*, z^*, y^*)$ such that<br>$$<br>L_0 (x^*, z^*, y) \leq L_0(x^*, z^*, y^*) \leq L_0(x^*, z^*, y^*)<br>$$</p>
<blockquote>
<p>Based on assumption1, $L_0(x^*, z^*, y^*)$ is finite for saddle point $(x^*, z^*, y^*)$. So, $(x^*, z^*)$ is solution to $\mathrm{argmin}_{x,z} L(x,z,y)$. So, $Ax^* + Bz^* = c$ and $f(x^*)&lt;\infty$, $g(z^*)&lt;\infty$. =</p>
<p>It also implies that $y^*$ is dual optimal, and the optimal value of the primal and dual problem are equal, i.e., the strong duality holds. </p>
</blockquote>
<p>Under Assumption 1 and Assumption 2, ADMM iteration satisfy:</p>
<ul>
<li>residual convergence: $r^k \rightarrow 0$ as $k\rightarrow \infty$.</li>
<li>objective convergence: $f(x^k)+g(z^k) \rightarrow p^*$ as $k\rightarrow \infty$.</li>
<li>dual variable convergence: $y^k \rightarrow y^*$ as $k\rightarrow \infty$.</li>
</ul>
<h2 id="1-3-Optimality-condition-and-Stopping-creterion"><a href="#1-3-Optimality-condition-and-Stopping-creterion" class="headerlink" title="1.3 Optimality condition and Stopping creterion"></a>1.3 Optimality condition and Stopping creterion</h2><h3 id="1-3-1-Optimality-condition-for-ADMM"><a href="#1-3-1-Optimality-condition-for-ADMM" class="headerlink" title="1.3.1 Optimality condition for ADMM:"></a>1.3.1 Optimality condition for ADMM:</h3><ol>
<li>Primal feasibility<br>$$<br>Ax^* + Bz^* -c = 0<br>$$</li>
<li>Dual feasibility<br>$$<br>\begin{aligned}<br>0 &amp;\in \part f(x^*) + A^Ty^* \<br>0 &amp;\in \part g(z^*) + B^Ty^*<br>\end{aligned}<br>$$</li>
</ol>
<p>Since $z^{k+1}$ minimizes $L_\sigma(x^{k+1},z,y^k)$, we have<br>$$<br>0\in \part g(z^{k+1}) + B^T y^k + \sigma B^T(Ax^{k+1}+Bz^{k+1}-c)\<br>= \part g(z^{k+1}) + B^T y^k + \sigma B^T r^{k+1} \<br>= \part g(z^{k+1}) + B^T y^k<br>$$<br>This means that $z^{k+1}$ and $y^{k+1}$ satisfy $0\in \part g(z^*)+ B^T u^*$. Similarly, can obtain other conditions. </p>
<blockquote>
<p>the last condition holds for $(x^{k+1}, z^{k+1}, y^{k+1})$, the residual for the other two are primal and dual residuals $r^{k+1}$ and $s^{k+1}$. </p>
</blockquote>
<h3 id="1-3-2-Stopping-criteria"><a href="#1-3-2-Stopping-criteria" class="headerlink" title="1.3.2 Stopping criteria"></a>1.3.2 Stopping criteria</h3><p>suboptimality<br>$$<br>f(x^k)+g(z^k)-p^* \leq -(y^k)^Tr^k + (x^k-x^*)^T s^k<br>$$<br>shows that when the residuals $r^k$ and $s^k$ are small, the objective suboptimality will be small.</p>
<p>It’s hard to use this inequality as stopping criterion. But if we guess $|x^k-x^* | \leq d$, we have<br>$$<br>f(x^k)+g(z^k)-p^* \leq -(y^k)^Tr^k + d |s^k| \leq |y^k| |r^k| + d|s^k|<br>$$<br>so, the stopping criterion is that the primal and dual residual are small, i.e.,<br>$$<br>|r^k| \leq \epsilon^{\mathrm {pri}} \quad \text{and } \quad |s^k| \leq \epsilon^{\text{dual}}<br>$$<br>where $\epsilon^{\text{pri}}, \epsilon^{\text{dual}}$ are feasibility tolerance for primal and dual feasibility conditions. These tolerance can be chosen using an absolute and relative criterion, such as<br>$$<br>\begin{aligned}<br>\epsilon^{\text{pri}} &amp;= \sqrt{p} \epsilon^{\text{abs}} + \epsilon^{\text{rel}} \max{ |Ax^k|, B|z^k|, |c| }, \<br>\epsilon^{\text{dual}} &amp;= \sqrt{n} \epsilon^{\text{abs}} + \epsilon^{rel} |A^Ty^k|<br>\end{aligned}<br>$$</p>
<h2 id="1-4-Extension-and-Variations"><a href="#1-4-Extension-and-Variations" class="headerlink" title="1.4 Extension and Variations"></a>1.4 Extension and Variations</h2><ol>
<li><p><strong>Varying Penalty Parameter</strong></p>
<p>using different penalty parameters $\sigma^k$ for each iteration.  -&gt; improve convergence, reduce independence on initial $\sigma$.<br>$$<br>\sigma^{k+1}:=\left{\begin{array}{ll}<br>{\tau^{\text {incr }} \sigma^{k}} &amp; {\text{if } |r^{k}| &gt; \mu |s^{k}|} \<br>{\sigma^{k} / \tau^{\text {decr }}} &amp; {\text {if }|s^{k}| &gt; \mu |r^{k}|} \<br>{\sigma^{k}} &amp; {\text { otherwise }}<br>\end{array}\right.<br>$$<br>where $\mu&gt;1, \tau^{\text{incr}}&gt;1, \tau^{\text{decr}}&gt;1$ are parameters. </p>
<p>The idea behind this penalty parameter update is to try to keep the primal and dual residual norms within a factor of $\mu$ of one another as they both converge to zero. </p>
<ul>
<li><p><strong>large values of $\sigma$ place a large penalty on violations of primal feasibility and so tend to produce small primal residuals.</strong></p>
</li>
<li><p><strong>Conversely, small values of $\sigma$ tend to reduce the dual residual, but at the expense of reducing the penalty on primal feasibility, which may result in a larger primal residual.</strong>  </p>
</li>
</ul>
<p>So, when primal residual becomes large, inflates $\sigma$ by $\tau^{\text{incr}}$; when primal residual seems too small, deflates $\sigma$ by $\tau^{\text{decr}}$.  </p>
<p>When a varying penalty parameter is used in the scaled form of ADMM, the scaled dual variable $u^k=\frac{1}{\sigma} y^k$ should be rescaled after updating $\sigma$.</p>
</li>
<li><p><strong>More general Augmenting terms</strong></p>
<ul>
<li>idea 1: use different penalty parameters for each constraint, </li>
<li>idea 2: replace the quadratic term $\frac{\sigma}{2} |r|^2$ with $\frac{1}{2} r^TPr$, where $P$ is a symmetric positive definite matrix.</li>
</ul>
</li>
<li><p>Over-relaxation in the $z$- and $y$- updates, $Ax^{k+1}$ can be replaced with<br>$$<br>\alpha^k Ax^{k+1} - (1-\alpha^k) (Bz^k-c)<br>$$</p>
<p>where $\alpha^k \in (0,2)$ is a relaxation parameter. When $\alpha^k &gt;1$, over-relaxation; when $\alpha^k&lt;1$, under-relaxation.</p>
</li>
<li><p><strong>Inexact minimization</strong></p>
<p>ADMM will converge even when the $x$- and $z$-minimization steps are not carried out exactly, provided certain suboptimality measures in the minimizations satisfy an appropriate condition, such as being summable.</p>
<blockquote>
<p>in situations where the x- or z-updates are carried out using an iterative method; it allows us to solve the minimizations only approximately at first, and then more accurately as the iterations progress</p>
</blockquote>
</li>
<li><p><strong>Update ordering</strong>    </p>
<p>$x$-, $y$-, $z$- updates in varying orders or multiple times.   </p>
<p>for example:  </p>
<p>divide variables into $k$ blocks and update each of them in turn (multiple times). –&gt; multiple Gauss-Seidel passes.</p>
</li>
<li><p><strong>Related algorithms</strong> </p>
<p>Dual ADMM [80]: applied ADMM to a dual problem.</p>
<p>Distributed ADMM [183]: </p>
<p>combination ADMM and proximal method of multipliers.</p>
</li>
</ol>
<h1 id="2-General-Patterns"><a href="#2-General-Patterns" class="headerlink" title="2. General Patterns"></a>2. General Patterns</h1><p>Three cases: </p>
<ul>
<li>quadratic objective terms; </li>
<li>separable objective and constraints; </li>
<li>smooth objective terms.</li>
</ul>
<p>Here, we just discuss $x$-update, and can be easily applied to $z$-update.<br>$$<br>x^+ = \mathop{\mathrm{argmin}}_x (f(x)+\frac{\sigma}{2} |Ax-v|^2)<br>$$<br>where $v=-Bz+c-u$ is a known constant vector for the purpose of the $x$-update.</p>
<h2 id="2-1-Proximity-Operator"><a href="#2-1-Proximity-Operator" class="headerlink" title="2.1 Proximity Operator"></a>2.1 Proximity Operator</h2><p>If $A=I$, $x$-update is<br>$$<br>x^+ = \mathop{\mathrm{argmin}}<em>x (f(x)+\frac{\sigma}{2} |x-v|^2) = \mathrm{Prox}</em>{f,\sigma}(v)<br>$$<br>Moreau envelope or MY regularization of $f$:<br>$$<br>\tilde f(v) = \inf_x (f(x)+\frac{\sigma}{2} |x-v|^2 )<br>$$<br>So, the $x$-minimization in the proximity operator is called <em>proximal minimization</em>.</p>
<blockquote>
<p>for example: if $f$ is the indicator function of set $\mathcal C$, the $x$-update is<br>$$<br>x^+ = \mathop{\mathrm{argmin}}<em>x (f(x)+\frac{\sigma}{2} |x-v|^2) = \Pi</em>{\mathcal C}(v)<br>$$</p>
</blockquote>
<h2 id="2-2-Quadratic-Objective-Terms"><a href="#2-2-Quadratic-Objective-Terms" class="headerlink" title="2.2 Quadratic Objective Terms"></a>2.2 Quadratic Objective Terms</h2><p>$$<br>f(x) = \frac{1}{2} x^TPx+q^Tx+r<br>$$</p>
<p>where $P\in \mathbb S^n_+$, the set of symmetric positive semidefinite $n\times n$ matrices.</p>
<p>Assume $P+\sigma A^TA$ is invertible, $x^+$ is an affine function of $v$ given by<br>$$<br>x^+ = (P+\sigma A^TA)^{-1} (\sigma A^Tv-q)<br>$$</p>
<ol>
<li><h3 id="direct-methods"><a href="#direct-methods" class="headerlink" title="direct methods:"></a>direct methods:</h3></li>
</ol>
<blockquote>
<p>If we want to solve $Fx=g$, steps:</p>
<ol>
<li><p>factoring $F = F_1F_2\cdots F_k$</p>
</li>
<li><p>back-solve: solve $F_iz_i=z_{i-1}$ where $z_1=F_1^{-1}g$ and $x=z_k$ </p>
<p>(given $z_1$, can compute $z_2$ based on $F_2z_2 =z_1$, then iterates, so can compute $z_k$, i.e., can compute $x$).</p>
</li>
</ol>
<blockquote>
<p>The cost of solving $Fx = g$ is the sum of the cost of factoring $F$ and the cost of the back-solve.</p>
</blockquote>
</blockquote>
<p>​    In our case, $F=P+\sigma A^TA, \ g=\sigma A^Tv-q$, where $P\in \mathbb S_+^n, A\in \mathbb R^{p\times n}$. </p>
<blockquote>
<p> We can form $F = P + \sigma A^TA$ at a cost of $O(pn^2)$ flops (floating point operations). We then carry out a Cholesky factorization of $F$ at a cost of $O(n^3)$ flops; the back-solve cost is $O(n^2)$. (The cost of forming $g$ is negligible compared to the costs listed above.) When $p$ is on the order of, or more than $n$, the overall cost is $O(pn^2)$. (When $p$ is less than $n$ in order, the matrix inversion lemma described below can be used to carry out the update in $O(p^2n)$ flops.)</p>
</blockquote>
<ol start="2">
<li><p><strong>Exploiting Sparsity</strong></p>
<p>When $A$ and $P$ can make $F$ sparse.  can be more efficient</p>
</li>
<li><p><strong>Caching Factorizations</strong></p>
<p>$Fx^{(i)}=g^{(i)}$,  If $t$ is the factorization cost and $s$ is the back-solve cost, then the total cost becomes $t + Ns$ instead of $N(t + s)$, which would be the cost if we were to factor $F$ each iteration. As long as $\sigma$ does not change, we can factor $P + \sigma A^TA$ once, and then use this cached factorization in subsequent solve steps</p>
</li>
<li><p><strong>Matrix Inversion  Lemma</strong><br>$$<br>\left(P+\sigma A^{T} A\right)^{-1}=P^{-1}-\sigma P^{-1} A^{T}\left(I+\sigma A P^{-1} A^{T}\right)^{-1} A P^{-1}<br>$$<br>For efficient computation.</p>
</li>
<li><p><strong>Quadratic Function restricted to an Affine Set</strong><br>$$<br>f(x)=(1 / 2) x^{T} P x+q^{T} x+r, \quad \text { dom } f={x | F x=g}<br>$$<br>Here, $x^+$ is still an affine function of $v$. the update involves solving the KKT condition:<br>$$<br>\left[\begin{array}{cc}{P+\sigma I} &amp; {F^{T}} \ {F} &amp; {0}\end{array}\right]\left[\begin{array}{c}{x^{k+1}} \ {\nu}\end{array}\right]+\left[\begin{array}{c}{q-\sigma\left(z^{k}-u^{k}\right)} \ {-g}\end{array}\right]=0<br>$$<br>==So, here, A=I​?==</p>
</li>
</ol>
<h2 id="2-3-Smooth-Objective-Terms"><a href="#2-3-Smooth-Objective-Terms" class="headerlink" title="2.3 Smooth Objective Terms"></a>2.3 Smooth Objective Terms</h2><p>If $f$ is smooth, can use iterative methods. (gradient method, nonlinear conjugate gradient, L-BFGS)</p>
<blockquote>
<p>The convergence of these methods depends on the conditioning of the function to be minimized. The presence of the quadratic penalty term $\frac{\sigma}{2} |Ax-v|^2$ tends to improve the conditioning of the problem and thus improve the performance of an iterative method for updating $x$. </p>
<p>Can adjust $\sigma$ in each iteration to converges quichly.</p>
</blockquote>
<ol>
<li><p><strong>Early Termination</strong></p>
<p>Early termination in the $x$- or $z$-updates can result in more ADMM iterations, but much lower cost per iteration, giving an overall improvement in efficiency.</p>
</li>
<li><p><strong>Warm Start</strong></p>
<p>initialize the iterative method used in the $x$-update at the solution $x^k$ obtained in the previous iteration</p>
</li>
<li><p><strong>Quadratic Objective Terms</strong></p>
<p>worth using an iterative method than a direct method.  –&gt; conjugate gradient method.</p>
<p>Can use when direct method do not work, or $A$ is dense.</p>
</li>
</ol>
<h2 id="2-4-Decomposition"><a href="#2-4-Decomposition" class="headerlink" title="2.4 Decomposition"></a>2.4 Decomposition</h2><ol>
<li><p><strong>Block Separability</strong> </p>
<p>$x=(x_1, \cdots, x_N)$, then $f(x)=f_1(x_1) + \cdots +f_N(x_N)$. If the quadratic term $|Ax|^2$ is also separable, i.e., $A^TA$ is block diagonal, then the augmented lagrangian $L_\sigma$ is separable.  –&gt;  compute in parallel.</p>
</li>
<li><p><strong>Component separability</strong></p>
<p>$f(x)=f_1(x_1) + \cdots +f_n(x_n)$ and $A^TA$ is diagonal. $x$-minimization step can be carried out via $n$ scalar minimization. </p>
</li>
<li><p><strong>Soft Thresholding</strong></p>
<p>If $f(x)=\lambda ||x|<em>1$ and $A=I$,  –&gt;  component separability. the scalar $x_i$-update is<br>$$<br>x_i^+ =  \mathop{\operatorname{argmin}}</em>{x_i}(\lambda |x_i| + \frac{\sigma}{2}(x_i-v_i)^2)<br>$$<br>Even the first term is not differentiable, But can use subdifferential.<br>$$<br>\begin{array}{ll}<br>&amp;x_i^+ = S_{\frac{\lambda}{\sigma}} (v_i)&amp; \<br>\text{where }&amp; &amp; \<br>&amp;S_\kappa(a)  = \begin{cases} a-\kappa,\quad&amp; a&gt;\kappa \ 0, \quad&amp;  |a|\leq \kappa \<br>a+\kappa, \quad&amp; a&lt; -\kappa\end{cases} \text{ or } S_\kappa(a) = (a-\kappa)<em>- - (-a-\kappa)</em>+<br>\end{array}<br>$$<br><strong>Actually, this is proximity operator of the $\ell_1$ norm.</strong></p>
</li>
</ol>
<h1 id="3-Constrained-Convex-Optimization"><a href="#3-Constrained-Convex-Optimization" class="headerlink" title="3. Constrained  Convex Optimization"></a>3. Constrained  Convex Optimization</h1><h3 id="3-1-1-Alternating-Projection"><a href="#3-1-1-Alternating-Projection" class="headerlink" title="3.1.1 Alternating Projection"></a>3.1.1 Alternating Projection</h3><p>find intersection of two sets.</p>
<p>alternating projection:<br>$$<br>\begin{array}{l}{x^{k+1}:=\Pi_{\mathcal{C}}\left(z^{k}\right)} \ {z^{k+1}:=\Pi_{\mathcal{D}}\left(x^{k+1}\right)}\end{array}<br>$$<br>ADMM:<br>$$<br>\begin{array}{ll}<br>\min &amp; f(x)+g(z) \<br>\text{s.t. } &amp; x-z = 0<br>\end{array}<br>$$<br>where $f$ and $g$ are indicator function of $\mathcal C$ and $\mathcal D$.<br>$$<br>\begin{array}{l}{x^{k+1}:=\Pi_{\mathcal{C}}\left(z^{k}-u^k\right)} \ {z^{k+1}:=\Pi_{\mathcal{D}}\left(x^{k+1}+u^k\right)} \<br>u^{k+1} := u^k+x^{k+1}-z^{k+1}  \end{array}<br>$$<br>This ADMM method is more efficient than previous method.</p>
<h3 id="3-1-2-Parallel-Projection"><a href="#3-1-2-Parallel-Projection" class="headerlink" title="3.1.2  Parallel Projection"></a>3.1.2  Parallel Projection</h3><p>find a  point in the intersection of $N$ sets $\mathcal A_1, \cdots, \mathcal A_N$.</p>
<p>So, $\mathcal C=\mathcal A_1\times \cdots \times \mathcal A_N$, $\mathcal D={(x_1, \cdots, x_N) \in \mathbb R^{nN} | x_1 = \cdots = x_N }$. </p>
<p>So, $\Pi_\mathcal C(x) = (\Pi_{\mathcal A_1}(x_1), \cdots, \Pi_{\mathcal A_N}(x_N))$, $\Pi_\mathcal D(x)=(\bar x, \cdots, \bar x)$, where $\bar x = \frac{1}{N}\sum_{i=1}^N x_i$.</p>
<p>each step of ADMM can be carried out by projecting points onto each of the sets $\mathcal A_i$ in parallel and then averaging the results:<br>$$<br>\begin{aligned} x_{i}^{k+1} &amp;:=\prod_{\mathcal{A}<em>{i}}\left(z^{k}-u</em>{i}^{k}\right) \ z^{k+1} &amp;:=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}^{k+1}+u_{i}^{k}\right) \ u_{i}^{k+1} &amp;:=u_{i}^{k}+x_{i}^{k+1}-z^{k+1} \end{aligned}<br>$$<br>The first and third steps can be carried out in parallel.</p>
<blockquote>
<p>indicator function of $\mathcal A_1 \cap \cdots \cap \mathcal A_N$ splits into the sum of the indicator functions of each $\mathcal A_i$.</p>
</blockquote>
<p>Another compact representation:<br>$$<br>\begin{array}{ll}<br>&amp;x_i^{k+1} = \Pi_{\mathcal A_i}(\bar x^k - u_i^k) \<br>&amp;u_i^{k+1} = u_i^k + (x_i^{k+1}-\bar x^{k+1}) \<br>\text{where} &amp; \bar u^{k+1}=\bar u^k+\bar x^{k+1}- z^k, z^{k+1} = \bar x^{k+1}+\bar u^k<br>\end{array}<br>$$<br>This shows that the $u_i^k$ is the running sum of the  discrepancies $x_i^k-\bar x^k$. The first step is a parallel projection onto the sets $\mathcal C_i$; the second involves averaging the projected points.</p>
<h2 id="3-2-Linear-and-Quadratic-Programming"><a href="#3-2-Linear-and-Quadratic-Programming" class="headerlink" title="3.2 Linear and Quadratic Programming"></a>3.2 Linear and Quadratic Programming</h2><p> Quadratic program (QP)<br>$$<br>\begin{array}{rc}<br>\min &amp; \frac{1}{2}x^TPx + q^Tx \<br>\text{s.t. } &amp; Ax=b, x\ge 0<br>\end{array}<br>$$<br>where $x\in \mathbb R^n$, $P\in \mathbb S_+^n$.</p>
<p>Represent it as ADMM form as<br>$$<br>\begin{array}{rcc}<br>&amp;\min &amp; f(x)+g(z) \<br>&amp;\text{s.t. } &amp; x-z=0  \<br>\text{where, } &amp; &amp;f(x)=\frac{1}{2}x^TPx + q^Tx, &amp; \mathrm{dom} f={x|Ax=b}, &amp; g(z)=\delta_{\mathbb R_+^n}(z)<br>\end{array}<br>$$<br>$f$ is the original objective with restricted domain and $g$ is the indicator function of $\mathbb R_+^n$.</p>
<p>Scaled form of ADMM:<br>$$<br>\begin{aligned}<br>x^{k+1} &amp;:=\operatorname{argmin}<em>{x}\left(f(x)+\frac{\sigma}{2} \left|x-z^{k}+u^{k}\right|^{2}\right) \<br>z^{k+1} &amp;:=\left(x^{k+1}+u^{k}\right)</em>{+} \<br>u^{k+1} &amp;:=u^{k}+x^{k+1}-z^{k+1}<br>\end{aligned}<br>$$<br>Here, the $x$-update is an equality-constrained least square problem with optimality KKT conditions, $\nu$ is multiplier.<br>$$<br>\begin{bmatrix}{P+\sigma I} &amp; {A^{T}} \ {A} &amp; {0}\end{bmatrix} \begin{bmatrix}{x^{k+1}} \ {\nu}\end{bmatrix} + \begin{bmatrix}{q-\sigma\left(z^{k}-u^{k}\right)} \ {-b}\end{bmatrix} = 0<br>$$</p>
<h3 id="Linear-and-Quadratic-Cone-Programming"><a href="#Linear-and-Quadratic-Cone-Programming" class="headerlink" title="Linear and Quadratic Cone Programming"></a>Linear and Quadratic Cone Programming</h3><p>conic constraint $x\in \mathcal K$ to replace $x\ge 0$ above</p>
<p>The only change to ADMM is the $z$-update (Projection onto $\mathcal K$)</p>
<blockquote>
<p>For example, we can solve a semidefinite program with $x \in \mathbb S^n_+$ by projecting $x^{k+1} + u^k$ onto $\mathbb S^n_+$ using an ==eigenvalue decomposition==.</p>
</blockquote>
<h1 id="4-ell-1-Norm-Problems"><a href="#4-ell-1-Norm-Problems" class="headerlink" title="4. $\ell_1$ Norm Problems"></a>4. $\ell_1$ Norm Problems</h1><p>ADMM explicitly targets problems that split into two distinct parts, $f$ and $g$, that can then be handled separately.</p>
<p>ADMM can decouple the nonsmooth $\ell_1$ term from the smooth loss term.</p>
<h2 id="4-1-Least-Absolute-Deviations"><a href="#4-1-Least-Absolute-Deviations" class="headerlink" title="4.1 Least Absolute Deviations"></a>4.1 Least Absolute Deviations</h2><p>minimize $|Ax-b|_1$ instead of $|Ax-b|_2^2$. provide a more robust fit than least squares</p>
<p>ADMM form:<br>$$<br>\begin{array}{ll}<br>\min &amp; |z|_1 \<br>\text{s.t. } &amp; Ax-z=b<br>\end{array}<br>$$<br>so, $f=0$, $g=| \cdot|_1$.</p>
<p>ADMM algorithm:<br>$$<br>\begin{aligned}<br>x^{k+1} &amp;:= (A^TA)^{-1} A^T (b+z^k-u^k) \<br>z^{k+1} &amp;:= S_{\frac{1}{\sigma}}(Ax^{k+1}-b+u^k) \<br>u^{k+1} &amp;:= u^{k} + Ax^{k+1} - z^{k+1} - b<br>\end{aligned}<br>$$</p>
<ol>
<li><p>Huber fitting</p>
<p>between least square and lease absolute.<br>$$<br>\begin{array}<br>{} &amp;\min &amp; g^{\text{hub}}(Ax-b) \<br>\text{where} &amp; &amp;g^{\text{hub}}(a)= \begin{cases} \frac{a^2}{2},\quad &amp; |a|\le 1 \ |a|-\frac{1}{2},\quad &amp;  |a|&gt;1 \end{cases} &amp;<br>\end{array}<br>$$</p>
<p>ADMM algorithm: </p>
<p>same. Only difference is the $z$-update<br>$$<br>z^{k+1}:=\frac{\sigma}{1+\sigma}\left(A x^{k+1}-b+u^{k}\right)+\frac{1}{1+\sigma} S_{1+1 / \sigma}\left(A x^{k+1}-b+u^{k}\right)<br>$$</p>
</li>
</ol>
<h2 id="4-2-Basis-Pursuit"><a href="#4-2-Basis-Pursuit" class="headerlink" title="4.2 Basis Pursuit"></a>4.2 Basis Pursuit</h2><p>$$<br>\begin{array}{ll}<br>\min &amp; |x|_1 \<br>\text{s.t. } &amp; Ax=b<br>\end{array}<br>$$</p>
<p>with $x\in \mathbb R^n$, $A\in \mathbb R^{m\times n}$, $b\in \mathbb R^m$ ($m&lt;n$).</p>
<p>Aim: to find a sparse solution to an underdetermined system with equality equations.</p>
<p>ADMM form:<br>$$<br>\begin{array}{ll}<br>\min &amp; f(x)+|z|_1 \<br>\text{s.t. } &amp; x-z=0<br>\end{array}<br>$$<br>where $f$ is indicator function of set $\mathcal C={x\in \mathbb R^n | Ax=b}$. </p>
<p>ADMM algorithm:<br>$$<br>\begin{aligned}<br>x^{k+1} &amp;:= \Pi_\mathcal C (z^k-u^k) \<br>z^{k+1} &amp;:= S_{\frac{1}{\sigma}}(x^{k+1}+u^k) \<br>u^{k+1} &amp;:= u^{k} + x^{k+1} - z^{k+1}<br>\end{aligned}<br>$$<br>The subproblem of $x$-update is solving a linearly-constrained minimum Euclidean norm problem</p>
<hr>
<p>$$<br>x^{k+1} = (I-A^T (AA^T)^{-1} A)(z^k-u^k) + A^T(AA^T)^{-1}b<br>$$</p>
<h2 id="4-3-General-ell-1-regularized-loss-minimization"><a href="#4-3-General-ell-1-regularized-loss-minimization" class="headerlink" title="4.3 General $\ell_1$ regularized loss minimization"></a>4.3 General $\ell_1$ regularized loss minimization</h2><p>Problem:  $\min \quad l(x)+\lambda |x|_1$</p>
<p>ADMM form:<br>$$<br>\begin{array}{rll}<br>&amp;\min &amp; l(x)+g(z) \<br>&amp;\text{s.t. } &amp; x-z=0 \<br>\text{where,} &amp; &amp; g(z)=\lambda |z|<em>1<br>\end{array}<br>$$<br>ADMM algorithm:<br>$$<br>\begin{aligned}<br>x^{k+1} &amp;:=\operatorname{argmin}</em>{x}\left(l(x)+\frac{\sigma}{2} \left|x-z^{k}+u^{k}\right|^{2}\right) \<br>z^{k+1} &amp;:= S_{\frac{\lambda}{\sigma}}(x^{k+1}+u^k) \<br>u^{k+1} &amp;:=u^{k}+x^{k+1}-z^{k+1}<br>\end{aligned}<br>$$<br>For subproblem $x$-update:</p>
<ul>
<li>If $l$ is smooth, this can be done by any standard method, such as Newton’s method, a quasi-Newton method such as L-BFGS, or the conjugate gradient method.</li>
<li>If $l$ is quadratic, the $x$-minimization can be carried out by solving linear equations</li>
</ul>
<h2 id="4-2-Lasso"><a href="#4-2-Lasso" class="headerlink" title="4.2 Lasso"></a>4.2 Lasso</h2><p>$$<br>\min \frac{1}{2} |Ax-b|^2 + \lambda |x|_1<br>$$</p>
<p>ADMM form:<br>$$<br>\begin{array}{rll}<br>&amp;\min &amp; f(x)+g(z) \<br>&amp;\text{s.t. } &amp; x-z=0 \<br>\text{where,} &amp; &amp; f(x)=\frac{1}{2}|Ax-b|^2,\quad g(z)=\lambda |z|<em>1<br>\end{array}<br>$$<br>ADMM algorithm:<br>$$<br>\begin{aligned}<br>x^{k+1} &amp;:= (A^TA+\sigma I)^{-1} (A^Tb+\sigma(z^k-u^k)) \<br>z^{k+1} &amp;:= S</em>{\frac{\lambda}{\sigma}}(Ax^{k+1}-b+u^k) \<br>u^{k+1} &amp;:= u^{k} + Ax^{k+1} - z^{k+1}<br>\end{aligned}<br>$$</p>
<h3 id="4-4-1-Generalized-Lass"><a href="#4-4-1-Generalized-Lass" class="headerlink" title="4.4.1 Generalized Lass"></a>4.4.1 Generalized Lass</h3><p>$$<br>\min \frac{1}{2} |Ax-b|^2 + \lambda |Fx|_1<br>$$</p>
<p>where $F$ is an arbitrary linear transformation.</p>
<blockquote>
<p>==Special cases:==<br>$$<br>F_{ij}=\begin{cases} 1, \quad &amp; j=i+1 \ -1, \quad &amp; j=i \ 0, \quad &amp; \text{otherwise} \end{cases}<br>$$<br>and $A=I$, the generation reduces to<br>$$<br>\min \frac{1}{2} |x-b|^2 + \lambda \sum_{i=1}^{n-1} |x_{i+1}-x_i|<br>$$</p>
</blockquote>
<p>ADMM form:<br>$$<br>\begin{array}{rll}<br>&amp;\min &amp; \frac{1}{2} |Ax-b|^2 + \lambda |z|<em>1\<br>&amp;\text{s.t. } &amp; Fx-z=0 \<br>\end{array}<br>$$<br>ADMM algorithm:<br>$$<br>\begin{aligned}<br>x^{k+1} &amp;:= (A^TA+\sigma F^TF)^{-1} (A^Tb+\sigma F^T(z^k-u^k)) \<br>z^{k+1} &amp;:= S</em>{\frac{\lambda}{\sigma}}(Fx^{k+1}+u^k) \<br>u^{k+1} &amp;:= u^{k} + Fx^{k+1} - z^{k+1}<br>\end{aligned}<br>$$</p>
<blockquote>
<p>For the previous special case, the $A^TA+\sigma F^TF$ is tridiagonal, so can be carried out in $O(n)$ flops.</p>
</blockquote>
<h3 id="4-4-2-Group-Lasso"><a href="#4-4-2-Group-Lasso" class="headerlink" title="4.4.2 Group Lasso"></a>4.4.2 Group Lasso</h3><p>Replace $|x|<em>1$ with $\sum</em>{i=1}^N |x_i|_2$ where $x=(x_1, \cdots, x_N)$ with $x_i \in \mathbb R^{n_i}$.</p>
<p>All the same, except the $z$-update:<br>$$<br>z_i^{k+1} := S_{\frac{\lambda}{\sigma}}(x_i^{k+1}+u^k), \quad o=1, \cdots, N \<br>$$<br>where the thresholding operator $S_\kappa$ is<br>$$<br>S_\kappa(a)=(1-\frac{\kappa}{|a|<em>2} )</em>+ a<br>$$</p>
<blockquote>
<p>Extend to handle the <u>overlapping groups.</u></p>
<p>$N$ potentially overlapping groups $G_i \subseteq {1,\cdots, n}$, the objective is<br>$$<br>\frac{1}{2} |Ax-b|^2 + \lambda \sum_{i=1}^N |x_{G_i} |<br>$$<br>because the groups can overlap, this kind of objective is difficult to optimize with many standard methods, but it is straightforward with ADMM. </p>
<p>ADMM: Introduce $x_i\in \mathbb R^{|G_i|}$ and consider problem:<br>$$<br>\begin{array}{ll}<br>\min &amp; \frac{1}{2} |Az-b|^2 + \lambda \sum_{i=1}^N |x_i| \<br>\text{s.t. } &amp; x_i-\tilde z_i=0, i=1, \cdots, N<br>\end{array}<br>$$</p>
</blockquote>
<h2 id="Sparse-Inverse-Covariance-Selection"><a href="#Sparse-Inverse-Covariance-Selection" class="headerlink" title="Sparse Inverse Covariance Selection"></a>Sparse Inverse Covariance Selection</h2><h1 id="5-Consensus-and-Sharing"><a href="#5-Consensus-and-Sharing" class="headerlink" title="5. Consensus and Sharing"></a>5. Consensus and Sharing</h1><p>$$<br>\min f(x) = \sum_{i=1}^N f_i(x)<br>$$</p>
<h2 id="5-1-Global-Variable-Consensus"><a href="#5-1-Global-Variable-Consensus" class="headerlink" title="5.1 Global Variable Consensus"></a>5.1 Global Variable Consensus</h2><p>Global consensus problem: </p>
<p>add <strong>global consensus</strong> variable $z$ to split the minimization problem<br>$$<br>\begin{aligned}<br>\min \quad&amp;  \sum_{i=1}^N f_i(x_i) \<br>\text{s.t.} \quad&amp;  x_i-z = 0, \ i=1,\cdots,N<br>\end{aligned}<br>$$</p>
<blockquote>
<p>constraint means all the local variables $x_i$ should be equal. </p>
</blockquote>
<p>Augmented Lagrangian:<br>$$<br>L_\sigma(x_1, \cdots, x_N, z, y) = \sum_{i=1}^N \left( f_i(x_i) + y_i^T(x_i-z) + \frac{\sigma}{2} |x_i-z|^2 \right)<br>$$<br>constraint set is<br>$$<br>\mathcal C = {(x_1, \cdots, x_N) | x_1=x_2=\cdots=x_N }<br>$$<br>ADMM algorithm:<br>$$<br>\begin{aligned}<br>x_i^{k+1} &amp;:= \mathop{\mathrm{argmin}}<em>{x_i} \left( f_i(x_i)+y_i^{kT}(x_i-z^k)+\frac{\sigma}{2}|x_i-z^k|^2 \right) \<br>z^{k+1} &amp;:= \frac{1}{N} \sum</em>{i=1}^N \left( x_i^{k+1} + \frac{1}{\sigma}y_i^k \right) \<br>y_i^{k+1} &amp;:= y^k_i + \sigma(x_i^{k+1}-z^{k+1})<br>\end{aligned}<br>$$<br>Here, the first and last step can be carried out independently. </p>
<p>Simplify this algorithm:<br>$$<br>\begin{array}{ll}<br>&amp;\begin{aligned}<br>x_i^{k+1} &amp;:= \mathop{\mathrm{argmin}}_{x_i} \left( f_i(x_i)+y_i^{kT}(x_i-z^k)+\frac{\sigma}{2}|x_i-\bar x^k|^2 \right) \<br>y_i^{k+1} &amp;:= y^k_i + \sigma(x_i^{k+1}-\bar x^{k+1})<br>\end{aligned} \<br>\text{where, } &amp; \quad z^{k+1} = \bar x^{k+1} + \frac{1}{\sigma} \bar y^k \<br>\end{array}<br>$$<br>The dual variables are separately updated to drive the variables into consensus, and quadratic regularization helps pull the variables toward their average value while still attempting to minimize each local $f_i$.</p>
<blockquote>
<p>We can interpret consensus ADMM as a method for solving problems in which the objective and constraints are distributed across multiple processors. Each processor only has to handle its own objective and constraint term, plus a quadratic term which is updated each iteration. The quadratic terms (or more accurately, the linear parts of the quadratic terms) are updated in such a way that the variables converge to a common value, which is the solution of the full problem.</p>
</blockquote>
<p>Primal and dual residual are<br>$$<br>{r^{k}=\left(x_{1}^{k}-\bar{x}^{k}, \ldots, x_{N}^{k}-\bar{x}^{k}\right), \quad s^{k}=-\sigma\left(\bar{x}^{k}-\bar{x}^{k-1}, \ldots, \bar{x}^{k}-\bar{x}^{k-1}\right)} <br>$$<br>So, the square norms are<br>$$<br>{\qquad\left|r^{k}\right|<em>{2}^{2}=\sum</em>{i=1}^{N}\left|x_{i}^{k}-\bar{x}^{k}\right|<em>{2}^{2}, \quad\left|s^{k}\right|</em>{2}^{2}=N \sigma^{2}\left|\bar{x}^{k}-\bar{x}^{k-1}\right|_{2}^{2}}<br>$$</p>
<h3 id="5-1-2-Global-Variable-Consensus-with-Regularization"><a href="#5-1-2-Global-Variable-Consensus-with-Regularization" class="headerlink" title="5.1.2 Global Variable Consensus with Regularization"></a>5.1.2 Global Variable Consensus with Regularization</h3><p>$$<br>\begin{array}{ll}<br>\min &amp; \sum_{i=1}^N f_i(x_i) + g(z) \<br>\text{s.t. } &amp; x_i-z=0, i=1,\cdots, N<br>\end{array}<br>$$</p>
<p>ADMM algorithm:<br>$$<br>\begin{array}{l}{x_{i}^{k+1}:=\underset{x_{i}}{\operatorname{argmin}}\left(f_{i}\left(x_{i}\right)+y_{i}^{k T}\left(x_{i}-z^{k}\right)+\frac{\sigma}{2} \left|x_{i}-z^{k}\right|<em>{2}^{2}\right)} \ {z^{k+1}:=\underset{z}{\operatorname{argmin}}\left(g(z)+\sum</em>{i=1}^{N}\left(-y_{i}^{k T} z+\frac{\sigma}{2} \left|x_{i}^{k+1}-z\right|<em>{2}^{2}\right)\right)} \ {y</em>{i}^{k+1}:=y_{i}^{k}+\sigma\left(x_{i}^{k+1}-z^{k+1}\right)} \end{array}<br>$$<br>Express the $z$-update as an averaging step, so we can have<br>$$<br>{\quad z^{k+1}:=\underset{z}{\operatorname{argmin}}\left(g(z)+\frac{N\sigma}{2}\left|z-\bar{x}^{k+1}-\frac{1}{\sigma} \bar{y}^{k}\right|_{2}^{2}\right)}<br>$$</p>
<blockquote>
<p>example 1:</p>
<p>$g(z)=\lambda |z|<em>1$, $z$-update becomes $z^{k+1}:=S</em>{\frac{\lambda}{N\sigma}}(\bar x^{k+1}-\frac{1}{\sigma} \bar y^k)$</p>
<p>Example 2:</p>
<p>$g$ is indicator function of $\mathbb R^n_+$, i.e., $g=\delta_{\mathbb R_+^n}$. $z$-update is $z^{k+1}:=(\bar x^{k+1}-\frac{1}{\sigma} \bar y^k)_+$</p>
</blockquote>
<p>Algorithm:<br>$$<br>\begin{array}{l}{x_{i}^{k+1}:=\underset{x_{i}}{\operatorname{argmin}}\left(f_{i}\left(x_{i}\right)+\frac{\sigma}{2} \left|x_{i}-z^{k}+u_i^k\right|<em>{2}^{2}\right)} \<br>{z^{k+1}:=\underset{z}{\operatorname{argmin}}\left(g(z)+\frac{N\sigma}{2} \left|z-x</em>{i}^{k+1}-\bar u^k\right|<em>{2}^{2}\right)} \<br>{u</em>{i}^{k+1}:=u_{i}^{k}+x_{i}^{k+1}-z^{k+1}} \end{array}<br>$$</p>
<h2 id="5-2-General-Form-Consensus-Optimization"><a href="#5-2-General-Form-Consensus-Optimization" class="headerlink" title="5.2 General Form Consensus Optimization"></a>5.2 General Form Consensus Optimization</h2><p>Objective: $f_1(x_1) + \cdots + f_N(x_N)$. Each of these local variables consists of a selection of the components of the global variables $z\in \mathbb R^n$. This means each components of local variable corresponds to some global variable component $z_g$.</p>
<p>Mapping from local to global: $g=\mathcal G(i,j)$ </p>
<blockquote>
<p>In the context of model fitting, the following is one way that general form consensus naturally arises. The global variable $z$ is the full feature vector (i.e., vector of model parameters or independent variables in the data), and different subsets of the data are spread out among $N$ processors. Then xi can be viewed as the subvector of $z$ corresponding to (nonzero) features that appear in the $i$th block of data. In other words, each processor handles only its block of data and only the subset of model coefficients that are relevant for that block of data. If in each block of data all regressors appear with nonzero values, then this reduces to global consensus.</p>
</blockquote>
<img src="https://raw.githubusercontent.com/Lisnol1/PicGo--/master/20191121201431.png" style="zoom:40%">

<p>$\tilde z_i \in \mathbb R^{n_i}$ defined by $(\tilde z_i)<em>j = z</em>{\mathcal G(i,j)}$.</p>
<p>General form consensus problem is<br>$$<br>\begin{array}{ll}<br>\min &amp; \sum_{i=1}^N f_i(x_i) \<br>\text{s.t. }&amp; x_i-\tilde z_i = 0, i = 1, \cdots, N<br>\end{array}<br>$$<br>Augmented Lagrangian  is<br>$$<br>L_\sigma(x,z,y) = \sum_{i=1}^N \left( f_i(x_i) + y^T_i(x_i-\tilde z_i) + \frac{<br>\sigma}{2} |x_i - \tilde z_i |^2 \right)<br>$$<br>with the dual variables $y_i\in \mathbb R^{n_i}$.</p>
<p>ADMM algorithm:<br>$$<br>\begin{array}{l}<br>{x_{i}^{k+1}:=\underset{x_{i}}{\operatorname{argmin}}\left(f_{i}\left(x_{i}\right)+y_{i}^{k T}x_{i}+\frac{\sigma}{2} \left|x_{i}-\tilde z_i^{k}\right|<em>{2}^{2}\right)} \ {z^{k+1}:=\underset{z}{\operatorname{argmin}}\left(\sum</em>{i=1}^{m}\left(-y_{i}^{k T} \tilde z_i+\frac{\sigma}{2} \left|x_{i}^{k+1}-\tilde z_i\right|<em>{2}^{2}\right)\right)} \ {y</em>{i}^{k+1}:=y_{i}^{k}+\sigma\left(x_{i}^{k+1}-\tilde z_i^{k+1}\right)}<br>\end{array}<br>$$<br>where $x_i$- and $y_i$- updates can be carried out independently for each $i$.</p>
<p>The $z$- update step decouples across the components of $z$, since $L_\sigma$ is fully separable in its components:<br>$$<br>z_{g}^{k+1}:=\frac{\sum_{\mathcal{G}(i, j)=g}\left(\left(x_{i}^{k+1}\right)<em>{j}+(1 / \sigma)\left(y</em>{i}^{k}\right)<em>{j}\right)}{\sum</em>{\mathcal{G}(i, j)=g} 1}<br>$$</p>
<blockquote>
<p>the $z$-update is local averaging for each component $z_g$ rather than global averaging; </p>
<p>in the language of collaborative filtering, we could say that only the processing elements that have an opinion on a feature $z_g$ will vote on $z_g$.</p>
</blockquote>
<h3 id="5-2-1-General-Form-Consensus-with-Regularization"><a href="#5-2-1-General-Form-Consensus-with-Regularization" class="headerlink" title="5.2.1 General Form Consensus with Regularization"></a>5.2.1 General Form Consensus with Regularization</h3><p>$$<br>\begin{array}{ll}<br>\min &amp; \sum_{i=1}^N f_i(x_i) + g(z) \<br>\text{s.t. }&amp; x_i-\tilde z_i = 0, i = 1, \cdots, N<br>\end{array}<br>$$</p>
<p>where $g$ is a regularization function. </p>
<p>$z$-update:</p>
<ul>
<li>first, the local averaging step from the unregularized setting, same to compute $z_g^{k+1}$. </li>
<li>Then, proximity operator $\mathrm{Prox}_{g,k_g\sigma}$ for averaging </li>
</ul>
<h2 id="5-3-Sharing"><a href="#5-3-Sharing" class="headerlink" title="5.3 Sharing"></a>5.3 Sharing</h2><p>$$<br>\min \sum_{i=1}^N f_i(x_i) + g(\sum_{i=1}^N x_i)<br>$$</p>
<p>where $x_i\in \mathbb R^n, i=1,\cdots, N$. $g$ is the shared object. </p>
<p>Problem:<br>$$<br>\begin{array}{ll}<br>\min &amp; \sum_{i=1}^N f_i(x_i) + g(\sum_{i=1}^N z_i) \<br>\text{s.t. }&amp; x_i- z_i = 0, \quad x_i, z_i \in \mathbb R^n, \quad i = 1, \cdots, N, \quad<br>\end{array}<br>$$<br>The scaled ADMM algorithm;<br>$$<br>\begin{array}{l}<br>{x_{i}^{k+1}:=\underset{x_{i}}{\operatorname{argmin}}\left(f_{i}\left(x_{i}\right)+\frac{\sigma}{2} \left|x_{i}- z_i^{k}+u_i^k\right|<em>{2}^{2}\right)} \<br>{z^{k+1}:=\underset{z}{\operatorname{argmin}}\left(g\left(\sum</em>{i=1}^{N}z_i\right) + \frac{\sigma}{2} \sum_{i=1}^N \left|z_i-x_{i}^{k+1}-u_i^k\right|<em>{2}^{2}\right)} \ {u</em>{i}^{k+1}:=u_{i}^{k}+x_{i}^{k+1}- z_i^{k+1}}<br>\end{array}<br>$$<br>$x_i$- and $u_i$ update can be carried out independently. $z$-update solve problem in $Nn$ variables. </p>
<p>But we can simplify it to solve only $n$ variables.<br>$$<br>\begin{array}{ll}<br>&amp;\min &amp; g(N\bar z)+\frac{\sigma}{2} \sum_{i=1}^N |z+i-a_i|^2 \<br>&amp;\text{s.t. } &amp; \bar z=\frac{1}{N} \sum_{i=1}^N z_i \<br>\text{where,} &amp;&amp; a_i = u_i+x_i^{k+1}<br>\end{array}<br>$$<br>Because $\bar z$ fixed, we have $z_i = a_i +\bar z+\bar a$.</p>
<p>So, $z$-update can be computed by solving unconstrained problem<br>$$<br>\min \quad g(N\bar z)+\frac{\sigma}{2} \sum_{i=1}^N |\bar z - \bar a |^2<br>$$<br>Then, we can have<br>$$<br>u_i^{k+1} = \bar u^k + \bar x^{k+1} - \bar z^{k+1}<br>$$<br>It’s obvious that all of the dual variables $u_i^k$ are equal, (i.e., consensus), and can be replaces with a single dual variable $u\in \mathbb R^m$. </p>
<p>So, the final algorithm:<br>$$<br>\begin{array}{l}<br>{x_{i}^{k+1}:=\underset{x_{i}}{\operatorname{argmin}}\left(f_{i}\left(x_{i}\right)+\frac{\sigma}{2} \left|x_{i}-x_i^k+\bar x^k - \bar z^{k}+u^k\right|<em>{2}^{2}\right)} \<br>{\bar z^{k+1}:=\underset{\bar z}{\operatorname{argmin}}\left(g\left(N \bar z\right) + \frac{N\sigma}{2} \left|\bar z-u^k-\bar x^{k+1}\right|</em>{2}^{2}\right)} \ {u^{k+1}:=u^{k}+\bar x^{k+1}- \bar z^{k+1}}<br>\end{array}<br>$$<br>The $x$-update can be carried out in parallel, for $i = 1,\cdots,N$. The $z$-update step requires gathering $x^{k+1}_i$ to form the averages, and then solving a problem with $n$ variables. After the $u$-update, the new value of $\bar x^{k+1} − \bar z^{k+1} + u^{k+1}$ is scattered to the subsystems.</p>
<h3 id="5-3-1-Duality"><a href="#5-3-1-Duality" class="headerlink" title="5.3.1 Duality"></a>5.3.1 Duality</h3><p>Attaching Lagrange multipliers $\nu_i$ to the constraints $x_i-z_i=0$, the dual function $\Gamma$ of the ADMM sharing problem is<br>$$<br>\Gamma(\nu_1, \cdots, \nu_N) = \begin{cases} -g^*(\nu_1)-\sum_if^*<em>i(-\nu_i), \quad &amp;\text{if }\nu_1=\nu_2=\cdots=\nu_N \ -\infty, \quad &amp;\text{otherwise} \end{cases}<br>$$<br>Letting $\psi=g^*$, $h_i(\nu)=f^*(-\nu)$, the dual problem can be rewritten as<br>$$<br>\begin{array}{ll}<br>\min &amp; \sum</em>{i=1}^N h_i(\nu_i) +\psi(\nu) \<br>\text{s.t. } &amp; \nu_i-\nu=0<br>\end{array}<br>$$<br>This is same as the regularized global variable consensus problem. </p>
<p>$d_i\in \mathbb R^n$ is the multiplers to constraints $\nu_i-\nu=0$. So, the Dual regularized global consensus problem is<br>$$<br>\min \quad \sum_{i=1}^N f_i(d_i) + g(\sum_{i=1}^N d_i)<br>$$<br>Thus, there is a <strong>close dual relationship</strong> between the consensus problem and the sharing problem. In fact, the global consensus problem can be solved by running ADMM on its dual sharing problem, and vice versa.</p>
<h3 id="5-3-2-Optimal-Exchange"><a href="#5-3-2-Optimal-Exchange" class="headerlink" title="5.3.2 Optimal Exchange"></a>5.3.2 Optimal Exchange</h3><p>exchange problem:<br>$$<br>\begin{array}{ll}<br>\min &amp; \sum_{i=1}^N f_i(x_i) \<br>\text{s.t.} &amp; \sum_{i=1}^N x_i=0<br>\end{array}<br>$$<br>The sharing objective $g$ is the indicator function of the set ${0}$. The components of the vectors $x_i$ represent quantities of commodities that are exchanged among $N$ agents or subsystems. $(x_i)_j$ is the exchanging amount. If $(x_i)_j&gt;0$, $i$ send to $j$, and vice versa. Constraints $\sum x_i=0$ means the each commodity clears. </p>
<p>Scaled ADMM algorithm:<br>$$<br>\begin{aligned}<br>x_i^{k+1} &amp;= \mathop{\operatorname{argmin}}<em>{x_i} \left( f_i(x_i) + \frac{\sigma}{2} |x_i-x_i^k+\bar x^k+u^k |^2 \right) \<br>u^{k+1} &amp;= u^k + \bar x^{k+1}<br>\end{aligned}<br>$$<br>Unscaled ADMM algorithm:<br>$$<br>\begin{aligned}<br>x_i^{k+1} &amp;= \mathop{\operatorname{argmin}}</em>{x_i} \left( f_i(x_i) + y^{kT}x_i + \frac{\sigma}{2} |x_i-(x_i^k-\bar x^k) |^2 \right) \<br>y^{k+1} &amp;= y^k + \sigma \bar x^{k+1}<br>\end{aligned}<br>$$<br>The proximal term in the $x$-update is a penalty for $x^{k+1}$ deviating from $x^k$, projected onto the feasible set. </p>
<blockquote>
<p>$x$-update can be carried out in parallel. $u$-update needs gathering $x_i^{k+1}$  and averages it, then broadcast $u^k + \bar x^{k+1}$ back for $x$-update</p>
</blockquote>
<blockquote>
<p>Dual decomposition is the simplest algorithmic expression of this kind of problem. In this setting, each agent adjusts his consumption $x_i$ to minimize his individual cost $f_i(x_i)$ adjusted by the cost $y^T x_i$, where $y$ is the price vector. ADMM differs only in the inclusion of the proximal regularization term in the updates for each agent. As $y^k$ converges to an optimal price vector $y^*$, the effect of the proximal regularization term vanishes. The proximal regularization term can be interpreted as each agent’s commitment to help clear the market.</p>
</blockquote>
<h1 id="6-Distributed-Model-Fitting"><a href="#6-Distributed-Model-Fitting" class="headerlink" title="6. Distributed Model Fitting"></a>6. Distributed Model Fitting</h1><p>$$<br>\min l(Ax-b) + r(x)<br>$$</p>
<p>where $l(Ax-b)=\sum_{i=1}^m l_i(a_i^Tx-b_i)$.</p>
<p>assume the regularization function $r$ is separable, like $r(x)=\lambda |x|_2^2$ or $r(x)=\lambda |x|_1$</p>
<h1 id="7-Nonconvex"><a href="#7-Nonconvex" class="headerlink" title="7. Nonconvex"></a>7. Nonconvex</h1><h2 id="7-1-Nonconvex-constraints"><a href="#7-1-Nonconvex-constraints" class="headerlink" title="7.1 Nonconvex constraints"></a>7.1 Nonconvex constraints</h2><p>$$<br>\begin{array}{ll}<br>\min &amp; f(x) \<br>\text{s.t.} &amp; x\in \mathbb S<br>\end{array}<br>$$</p>
<p>with $f$ convex but $\mathbb S$.<br>$$<br>\begin{array}{l}{x^{k+1}:=\underset{x}{\operatorname{argmin}}\left(f(x)+(\rho / 2)\left|x-z^{k}+u^{k}\right|<em>{2}^{2}\right)} \ {z^{k+1}:=\prod</em>{\mathcal{S}}\left(x^{k+1}+u^{k}\right)} \ {u^{k+1}:=u^{k}+x^{k+1}-z^{k+1}}\end{array}<br>$$<br>Here, $x$-minimization step is convex since $f$ is convex, but $z$-update is projection onto nonconvex set. </p>
<ul>
<li>Cardinality. If $\mathbb S = {x | \mathrm {card}(x) ≤ c}$, where $\mathrm{card}$ gives the number of nonzero elements, then $\Pi_{\mathbb S}(v)$ keeps the $c$ largest<br>magnitude elements and zeroes out the rest.</li>
<li>Rank. If $\mathbb S$ is the set of matrices with rank $c$, then $\Pi_{\mathbb S}(v)$ is determined by carrying out a singular value decomposition, $v =\sum_i \sigma_i u_i v_i^T$, and keeping the top $c$ dyads, i.e., form $\Pi_{\mathbb S}(v) = \sum_{i=1}^c \sigma_i u_i v_i^T$.</li>
<li>Boolean constraints. If $\mathbb S={ x|x_i\in {0,1} }$, then $\Pi_{\mathbb S}(v)$ simply rounds each entry to 0 or 1, whichever is closer. Integer constraints can be handled in the same way.</li>
</ul>
<h1 id="8-QCQP"><a href="#8-QCQP" class="headerlink" title="8. QCQP"></a>8. QCQP</h1><p>$$<br>\begin{array}{rCl}<br>\text{(P1)} &amp;<br>\begin{array}{rCl}<br>&amp;\min\limits_{x \in \mathbb{R}^{n}} &amp; x^{T} C x\<br>&amp;\text { s.t. } &amp;x^{T} A_{i} x \underline\triangleright b_{i}, \quad i=1, \dots, m<br>\end{array}<br>\end{array}<br>$$</p>
<p>where $\underline\triangleright$ can represents either $\geq, \leq, =$; $C, A_1, A_2, \cdots, A_m \in \mathbb S^n$ where $\mathbb S^n$ denotes the set of all real symmetric $n\times n$ matrices. </p>
<p>if $A_i$ are positive semidefinite, it’s convex program. many nonlinear programming codes can be used. (primal-dual interior-point method)</p>
<p>Nonconvex QOP is NP-hard. there are two distinct techniques –&gt; branch-and-bound method</p>
<ol>
<li><p>generate a feasible solution or approximate feasible solution with a smaller objective value.</p>
</li>
<li><p>derive a tighter lower bound of the minimal objective value </p>
</li>
</ol>
<p>“Lift-and-project convex relaxation methods”  can be characterized as three steps:</p>
<ol>
<li><p>Lift the QOP to an equivalent problem in the space $\mathbb S^{1+n}$ of $(1+n)\times(1+n)$ symmetric matrices. The resulting problem is an Linear Programming with additional rank-1 and positive semidefinite constraints imposed on a matrix variable<br>$$<br>Y=\begin{pmatrix} 1 &amp; x^T \ x &amp; X \end{pmatrix} \in \mathbb S^{1+n}<br>$$</p>
</li>
<li><p>Relax the rank-1 and positive semidefinite constraints so that the feasible region of the resulting problem is convex. </p>
</li>
<li><p>Project the relaxed lifted problem in $\mathbb S^{1+n}$ back to the original Euclidean space $\mathbb R^n$.</p>
</li>
</ol>
<p>If only the rank-1 constraint is removed in step 2, it becomes an SDP –&gt; SDP relaxation</p>
<p>If remove both rank-1 and positive semidefinite constraints in step 2, it becomes LP. –&gt; cutting plane algorithm, LP relaxation</p>
<p>The SDP relaxation is at least as effective as LP relaxation. More efficient. 0.878 approximation of maximum cut based on the SDP relaxation is widely known.</p>
<h2 id="8-1-SDR"><a href="#8-1-SDR" class="headerlink" title="8.1 SDR"></a>8.1 SDR</h2><h3 id="8-1-1-Homogeneous-QCQP"><a href="#8-1-1-Homogeneous-QCQP" class="headerlink" title="8.1.1 Homogeneous QCQP"></a>8.1.1 Homogeneous QCQP</h3><p>$$<br>\begin{array}{rCCCl}<br>x^{T} C x &amp;=&amp; \operatorname{Tr}\left(x^{T} C x\right) &amp; = &amp; \operatorname{Tr}\left(C x x^{T}\right) \<br>x^{T} A_{i} x &amp;=&amp; \operatorname{Tr}\left(x^{T} A_{i} x\right) &amp; = &amp; \operatorname{Tr}\left(A_{i} x x^{T}\right)<br>\end{array}<br>$$</p>
<p>Set $xx^T = X$. </p>
<p>Equivalent formulation is<br>$$<br>\begin{array}{rCl}<br>\text{(P2)} &amp;<br>\begin{array}{rCl}<br>&amp;\min\limits_{X \in \mathbb{S}^{n}} &amp; \operatorname{Tr}(C X)\<br>&amp;\text { s.t. } &amp; \operatorname{Tr}(A_{i} X) \underline\triangleright b_{i}, \quad i=1, \dots, m \<br>&amp;&amp; X\succeq 0, : \operatorname{rank}(X) = 1<br>\end{array}<br>\end{array}<br>$$<br>In problem (P2), the rank constraint $\operatorname{rank}(X)=1$ is difficult and nonconvex. Thus, it can be dropped to obtain the relaxed version (P3):<br>$$<br>\begin{array}{rCl}<br>\text{(P3)} &amp;<br>\begin{array}{rCl}<br>&amp;\min\limits_{X \in \mathbb{S}^{n}} &amp; \operatorname{Tr}(C X)\<br>&amp;\text { s.t. } &amp; \operatorname{Tr}(A_{i} X) \underline\triangleright b_{i}, \quad i=1, \dots, m \<br>&amp;&amp; X\succeq 0<br>\end{array}<br>\end{array}<br>$$</p>
<blockquote>
<p>Most of the convex optimization toolboxes handles SDPs using interior-point algorithm with the worst complexity  $\mathcal O(\max{m,n}^4n^{1/2}\log(1/\epsilon))$ given a solution accuracy $\epsilon&gt;0$. [Primal-dual path-following method]</p>
</blockquote>
<p>SDR is a computationally efficient approximation approach to QCQP, in the sense that its complexity is polynomial in the problem size $n$ and the number of constraints $m$. </p>
<p>But, how to convert a globally optimal solution $X^*$ to a feasible solution $\tilde x$ is hard if $\operatorname{rank}(X) \neq 1$. Even if we obtain $\tilde x$, it may be not optimal solution $x^*$.</p>
<p>one method:</p>
<ul>
<li>If $\operatorname{rank}(X) \neq 1$, let $r=\operatorname{rank}(X^*)$, let $X^* = \sum_{i=1}^r \lambda_i q_iq_i^T$ denote the eigen-decomposition of $X^*$ where $\lambda_1\ge \lambda_2 \ge \cdots \ge \lambda_r \ge 0$ are eigen values ad $q_1, q_2, \cdots, q_r\in \mathbb R^n$ are eigen vectors. use the best rank-one approximation $X_1^*$ to $X^*$, where $X_1^* = \lambda_1q_1q_1^T$, we have $\tilde x = \sqrt{\lambda_1}q_1$ is one of the solution to (P1)</li>
<li>Randomization. rounding to generate feasible points from the random samples $\xi_l$. Moreover, we repeat the random sampling $L$ times and choose the one that yields the best objective.</li>
</ul>
<h3 id="8-1-2-Inhomogeneous-QCQP"><a href="#8-1-2-Inhomogeneous-QCQP" class="headerlink" title="8.1.2 Inhomogeneous QCQP"></a>8.1.2 Inhomogeneous QCQP</h3><p>$$<br>\begin{array}{rCl}<br>\min\limits_{x \in \mathbb{R}^{n}} &amp; x^{T} C x+2 c^{T} x \<br>\text { s.t. } &amp; x^{T} A_{i} x+2 a_{i}^{T} x \underline\triangleright b_{i}, \quad i=1, \ldots, m<br>\end{array}<br>$$</p>
<p>rewrite it as homogeneous form<br>$$<br>\begin{array}{rCl}<br>\min\limits_{x \in \mathbb{R}^{n}, t \in \mathbb{R}} &amp; \begin{bmatrix} x^{T} \quad t\end{bmatrix} \begin{bmatrix} C &amp; c \ c^{T} &amp; 0 \end{bmatrix} \begin{bmatrix} x \ t \end{bmatrix} \<br>\text { s.t. } &amp; t^{2}=1 \<br>&amp; \begin{bmatrix} x^{T} \quad t\end{bmatrix} \begin{bmatrix} A_i &amp; a_i \ a_i^T &amp; 0 \end{bmatrix} \begin{bmatrix} x \ t \end{bmatrix} \underline\triangleright b_i, \quad i=1, \ldots, m<br>\end{array}<br>$$<br>where both the problem size and the number of constraints increase by one.</p>
<h3 id="8-1-3-Complex-valued-problems"><a href="#8-1-3-Complex-valued-problems" class="headerlink" title="8.1.3 Complex-valued problems"></a>8.1.3 Complex-valued problems</h3><p><strong>One type:</strong><br>$$<br>\begin{array}{rCl}<br>\min\limits_{x \in \mathbb{C}^{n}} &amp; x^{H} C x \<br>\text { s.t. } &amp; x^{H} A_{i} x : \underline\triangleright : b_{i}, \quad i=1, \ldots, m<br>\end{array}<br>$$<br>where $C, A_1, A_2, \cdots, A_m \in \mathbb H^n$ with $\mathbb H^n$ being the set of all complex $n\times n$ Hermitian matrices.  </p>
<p>Convert it as<br>$$<br>\begin{array}{rCl}<br>\min\limits_{X \in \mathbb{H}^{n}} &amp; \operatorname{Tr}(C X)\<br>\text { s.t. } &amp; \operatorname{Tr}\left(A_{i} X\right)  : \underline\triangleright :  b_{i}, \quad i=1, \ldots, m\<br>&amp; X \succeq 0<br>\end{array}<br>$$<br><strong>Another type:</strong><br>$$<br>\begin{array}{rCl}<br>\min\limits_{x \in \mathbb{C}^{n}} &amp;x^{H} C x \<br>\text { s.t. } &amp;x_{i} \in\left{1, e^{j 2 \pi / k}, \cdots, e^{j 2 \pi(k-1) / k}\right}, i=1, \ldots, n<br>\end{array}<br>$$<br>transform it as<br>$$<br>\begin{array}{rCl}<br>\underset{X \in \mathbb H^{n}}{\min } &amp; \operatorname{Tr}(C X)\<br>\text { s.t. }&amp;  X \succeq 0, :  X_{i i}=1, i=1, \ldots, n<br>\end{array}<br>$$</p>
<h3 id="8-1-4-Separable-QCQPs"><a href="#8-1-4-Separable-QCQPs" class="headerlink" title="8.1.4 Separable QCQPs"></a>8.1.4 Separable QCQPs</h3><p>$$<br>\begin{array}{rCl}<br>\min\limits_{x_{1}, \cdots, x_{k} \in \mathbb{C}^{n}} &amp; \sum\limits_{i=1}^{k} x_{i}^{H} C_{i} x_{i}\<br>\text { S.t. } &amp; \sum\limits_{l=1}^{k} x_{l}^{H} A_{i, l} x_{l}  : \underline\triangleright :  b_{i}, \quad i=1, \cdots, m<br>\end{array}<br>$$</p>
<p>Let $X_i = x_i x_i^H$ for $i=1,2,\cdots, k$. By relaxing the rank constraint on each $X_i$, we have<br>$$<br>\begin{array}{rCl}<br>\min\limits_{X_{1}, \ldots, X_{k} \in \mathbb{H}^{n}} &amp; \sum\limits_{i=1}^{k} \operatorname{Tr}\left(C_{i} X_{i}\right)\<br>\text{s.t.} &amp; \sum\limits_{l=1}^{k} \operatorname{Tr}\left(A_{i, l} X_{l}\right)  : \underline\triangleright : b_{i}, \quad i=1, \ldots, m\<br>&amp; X_{1} \succeq 0, \cdots, X_{k} \succeq 0<br>\end{array}<br>$$</p>
<h3 id="8-1-5-Application-sensor-network"><a href="#8-1-5-Application-sensor-network" class="headerlink" title="8.1.5 Application: sensor network"></a>8.1.5 Application: sensor network</h3><p>goal: determine the coordinates of sensors in $\mathbb R^2$ such that the distances match the measured distance</p>
<p>Set of sensors $V_s = {1,2, \cdots, n}$, set of anchors $V_a = {n+1, n+2, \cdots, n+m}$. </p>
<p>Let $E_{ss}$ be the sensor-sensor edges, $E_{sa}$ be the set of sensor-anchor edges.</p>
<p>Assume the measured distance ${d_{ik}:(i,k)\in E_{ss}}$ and ${\overline d_{ik}:(i,k) \in E_{sa}}$ are noise-free. </p>
<p>Problem becomes<br>$$<br>\begin{array}{rCl}<br>\text{find} &amp; x_1,x_2,\cdots,x_n \in \mathbb R^2 \<br>\text{s.t. } &amp; |x_i-x_k|^2 = d_{ik}^2, : (i,k)\in E_{ss} \<br>&amp; |a_i-x_k|^2 = d_{ik}^2, : (i,k)\in E_{ss} \<br>\end{array}<br>$$<br>First,<br>$$<br>|x_i - x_k|^2 = x_i^Tx_i - 2x_i^Tx_k + x_k^Tx_k<br>$$<br>rewrite it as<br>$$<br>|x_i - x_k|^2 = (e_i-e_k)^TX^TX(e_i-e_k) = \operatorname{Tr}(E_{ik}X^TX)<br>$$<br>where $e_i\in \mathbb R^n$ is the $i$th vector, $E_{ik}=(e_i-e_k)^T(e_i-e_k) \in \mathbb S^n$, $X\in \mathbb R^{2\times n}$ whose $i$th column is $x_i$.</p>
<p>Now, we have<br>$$<br>|a_i-x_k|^2 = a_i^Ta_i - 2a_i^Tx_k + x_k^Tx_k<br>$$<br>Although $a_i^Tx_k$ is linear only in $x_k$, we homogenize it and write as<br>$$<br>|a_i-x_k|^2 = \begin{bmatrix} a_i^T &amp; e_k^T \end{bmatrix}\begin{bmatrix} I_2 &amp; X \ x^T &amp; X^TX  \end{bmatrix} \begin{bmatrix} a_i \ e_k \end{bmatrix} = \operatorname{Tr}(\bar M_{ik}Z)<br>$$<br>where $\bar M_{ik} = \begin{bmatrix} a_i \ e_k \end{bmatrix} \begin{bmatrix} a_i^T &amp; e_k^T \end{bmatrix}\in \mathbb S^{n+2} $ .</p>
<p>Using Schur complement, we have $Z = \begin{bmatrix} I_2 &amp; X \ X^T &amp; X^TX  \end{bmatrix} = \begin{bmatrix} I_2 \ X^T \end{bmatrix}\begin{bmatrix} I_2 &amp; X \end{bmatrix} \in \mathbb S^{n+2}$.</p>
<p>Set $M_{ik}= \begin{bmatrix} 0 &amp; 0 \ 0 &amp; E_{ik} \end{bmatrix}$.</p>
<p>We have the following equivalent SDP<br>$$<br>\begin{array}{rCl}<br>\text{find} &amp; Z \<br>s.t. &amp; \operatorname{Tr}\left(M_{i k} Z\right)=d_{i k}^{2}, \quad(i, k) \in E_{s s} \<br>&amp; \operatorname{Tr}\left(\bar{M}<em>{i k} Z\right) =\bar{d}</em>{i k}^{2}, \quad(i, k) \in E_{s a} \<br>&amp; Z_{1: 2,1: 2} =I_{2} \<br>&amp; Z \succeq 0, : \operatorname{rank}(Z)=2<br>\end{array}<br>$$</p>
<h2 id="8-2-Second-order-cone-relaxation-SOCR"><a href="#8-2-Second-order-cone-relaxation-SOCR" class="headerlink" title="8.2 Second order cone relaxation (SOCR)"></a>8.2 Second order cone relaxation (SOCR)</h2><blockquote>
<p>Optimal inverter var control in distribution systems with high pv penetration</p>
<p>Branch flow model: relaxations and convexification (parts I, II)</p>
<p>Optimal power flow in distribution networks</p>
<p>Convex Relaxation of Optimal Power FlowPart I: Formulations and Equivalence</p>
<p>Convex Relaxation of Optimal Power FlowPart II: Exactness,</p>
</blockquote>
<h2 id="8-3-Other"><a href="#8-3-Other" class="headerlink" title="8.3 Other"></a>8.3 Other</h2><p>SDR/SOCR are not exact. </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhang-xiaoxue.github.io/2020/07/30/Model%20Predictive%20Control/LQR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo_blue.jpg">
      <meta itemprop="name" content="Xiaoxue Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaoxue Zhang - NUS">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/07/30/Model%20Predictive%20Control/LQR/" class="post-title-link" itemprop="url">Linear Quadratic Regulator (LQR)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-07-30 12:00:00" itemprop="dateCreated datePublished" datetime="2020-07-30T12:00:00+08:00">2020-07-30</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-16 15:18:32" itemprop="dateModified" datetime="2021-08-16T15:18:32+08:00">2021-08-16</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Model-Predictive-Control/" itemprop="url" rel="index"><span itemprop="name">Model Predictive Control</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="LQR"><a href="#LQR" class="headerlink" title="LQR"></a>LQR</h1><p>State-feedback control via pole placement requires one to assign the closed-loop poles, <strong>LQR</strong> is a technique to place <u>automatically and optimally</u> the closed-loop poles</p>
<p>##Problem description </p>
<ul>
<li><p>consider system $x(k+1)=Ax(k)+Bu(k)$ with initial condition $x(0)$, we look for the optimal sequence of inputs $U={u(0),\ u(1),\cdots,\ u(N-1) }$ driving the state $x(k)$ towards the prigin and that minimizes the performance index </p>
<p>​    $J(x(0),U)=x’(N)Q_Nx(N)+\sum\limits_{k=0}^{N-1} x’(k)Qx(k) + u’(k)Ru(k)$        <u><em>quadratic cost</em></u></p>
<p>where $Q=Q’ \succeq 0, R=R’ \succeq 0, Q_N=Q’_N \succeq 0$, means positive definite matrix.</p>
</li>
<li><p>consider the controllability of the state to zero with minimum energy input</p>
<p>​    $\min_U \begin{Vmatrix} \begin{bmatrix} u(0)\ u(1)\ \vdots \ u(N-1) \end{bmatrix} \end{Vmatrix}$</p>
<p>s.t.  $x(N)=0$</p>
</li>
<li><p>The minimum-energy control problem can be seen as aparticular case of the LQ optimal control problem by setting  $R=I, \ Q=0, \ Q_N=\infin \cdot I $</p>
</li>
<li><p>By substituting $x(k)=A^k x(0) + \sum\limits_{i=0}^{k-1} A^iBu(k-1-i)$ in</p>
<p>​    $J(x(0),U)=\sum\limits_{k=0}^{N-1} x’(k)Qx(k) + u’(k)Ru(k) +x’(N)Qx(N)$</p>
<p>we obtain $J(x(0),U)=\frac{1}{2}U’HU+x(0)’FU+\frac{1}{2}x’(0)Yx(0)$ , where $H=H’ \succeq 0$ is a positive definite matrix</p>
</li>
</ul>
<p>##Solution1:</p>
<ul>
<li><p>The optimizer $U^*$ is obtained by <u>zeroing the gradient</u></p>
<p>​    $0=\nabla_U J(x(0),U)=HU+F’x(0)$</p>
<p>​    $\rightarrow    U^* = \begin{bmatrix} u^*(0)\ u^*(1)\ \vdots\ u^*(N-1) \end{bmatrix} =-H^{-1}F’x(0)$  </p>
<p>this $U^*$ is an <em><strong>open-loop</strong></em> one: $u(k)=f_k(x(0)), k=0,1,\cdots,N-1$</p>
<ul>
<li>Moreover the dimensions of the $H$ and $F$ matrices is proportional to the time horizon N </li>
</ul>
</li>
</ul>
<p>##Solution2:</p>
<ul>
<li><p>using <strong><u>dynamic programming</u></strong> <em><strong>[Bellman’s principle of optimality]</strong></em></p>
</li>
<li><p>at  a generic instant $k_1$ and state $x(k_1)=z$  consider the optimal cost-to-go</p>
<p>​    $V_{k_1}(z)=\min\limits_{u(k_1),\cdots,u(N-1)}\left{ \sum\limits_{k=k_1}^{N-1}x’(k)Qx(k)+u’(k)Ru(k)+x’(N)Q_Nx(N) \right}$</p>
<p>and </p>
<p>​    $\begin{align} V_{0}(x(0))&amp;=\min\limits_{U\triangleq u(0),\cdots,u(N-1)} J(x(0),U) \ &amp;=\min\limits_{u(0),\cdots,u(k_1-1)}\left{ \sum\limits_{k=0}^{k_1-1}x’(k)Qx(k)+u’(k)Ru(k)+V_{k_1}(x(k_1)))\right} \end{align}$    $\rightarrow$  <em><u>iteration</u></em></p>
</li>
<li><p>starting at $x(0)$, the minimum cost over $[0,N]$ equals the minumum cost spent until step $k_1$ plus the optimal cost-to-go from $k_1$ to $N$ starting at $x(k_1)$ </p>
<hr>
<p>Bellman’s optimality principle also applies to nonlinear system and/or non-quadratic cost functions:</p>
<ul>
<li>The optimal control law can be always written in <u>state-feedback form</u></li>
</ul>
<p>​    $u^*(k)=f_k(x^*(k)), \ \forall k=0,\cdots,N-1$</p>
<ul>
<li>Compared to the open-loop solution ${u^*(0),\cdots, u^*(N − 1)} = f(x(0))$ the <u>feedback form</u> has the big advantage of being more <strong>robust</strong> with respect to perturbations: at each time $k$ we apply the best move on theremaining period $[k, N]$</li>
</ul>
<hr>
</li>
</ul>
<h3 id="Riccati-iterations"><a href="#Riccati-iterations" class="headerlink" title="Riccati iterations"></a>Riccati iterations</h3><h4 id="Finite-horizon-optimal-control"><a href="#Finite-horizon-optimal-control" class="headerlink" title="Finite-horizon optimal control"></a>Finite-horizon optimal control</h4><p>compute the optimal inputs $u^*(k)$ recursively as a function of $x^*(k)$ (<strong>Riccati iterations</strong>):</p>
<ol>
<li>Initialisation: $P(N)=Q_N$</li>
<li>for $k=N,\cdots,1$ , compute recursively the following matrix</li>
</ol>
<p>​    $P(K-1)=Q-A’P(k)B(R+B’P(k)B)^{-1}B’P(k)A+A’P(k)A$</p>
<ol start="3">
<li>Define</li>
</ol>
<p>​    $K(k)=-(R+B’P(k+1)B)^{-1}B’P(k+1)A$</p>
<p>​       The <strong>optimal input</strong> is a <u>(linear time-varying) state feedback</u></p>
<p>​        $u^*(k)=K(k)x^*(k)$</p>
<h4 id="Infinite-horizon-optimal-control"><a href="#Infinite-horizon-optimal-control" class="headerlink" title="Infinite-horizon optimal control"></a>Infinite-horizon optimal control</h4><h5 id="regulate-x-k"><a href="#regulate-x-k" class="headerlink" title="regulate $x(k)$"></a>regulate $x(k)$</h5><p>​    $V^{\infin}(x(0)) =\min\limits_{u(0),u(1),\cdots} \sum\limits_{k=0}^{\infin}x’(k)Qx(k)+u’(k)Ru(k)$</p>
<ul>
<li><p><strong>Results:</strong></p>
<p>exists a unique solution $P_∞$ of the <strong>algebraic Riccati equation (ARE)</strong>:</p>
<p>​    $P_∞ = A′P_∞A + Q − A′P_∞B(B′P_∞B + R)^{−1}B′P_∞A$<br>such that the optimal cost is $V_∞(x(0)) = x′(0)P_∞x(0)$ and the optimal control law is the constant linear state feedback $u(k) = K_{LQR} x(k)$ with </p>
<p>​    $K_{LQR} = −(R + B′P_∞B)^{−1} B′P_∞A$</p>
</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">P_∞ = dare(A,B,Q,R) <span class="comment">% Solve discrete-time algebraic Riccati equations.</span></span><br><span class="line"> </span><br><span class="line">[-K_∞,P_∞]=dlqr(A,B,Q,R) <span class="comment">% Linear-quadratic regulator design for discrete-time systems.</span></span><br><span class="line"></span><br><span class="line">[-K_∞,P_∞,E] = lqr(sysd,Q,R) <span class="comment">% Linear-quadratic regulator design for state space systems. </span></span><br><span class="line">		where E= closed-loop poles = eigenvalues of (A + B K_&#123;LQR&#125;)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Closed-loop stability is ensured if (A,B) is stabilizable, $R≻0$, $Q⪰0$ , and $(A, Q^{\frac{1}{2}})$ is detectable, where $Q^{\frac{1}{2}}$ is the <strong>Cholesky factor</strong> of $Q$. </p>
<blockquote>
<p>In fact, Given a matrix $Q=Q′⪰0$ , its Cholesky factor is an upper-triangular matrix $C$ such that $C′C = Q$ <em>(<strong>MATLAB</strong>:chol)</em></p>
</blockquote>
</li>
<li><p>LQR is an automatic and optimal way of placing poles! </p>
</li>
</ul>
<h5 id="regulate-y-k"><a href="#regulate-y-k" class="headerlink" title="regulate $y(k)$"></a>regulate $y(k)$</h5><p>if we just want regulate $y(k)=Cx(k)$, rather than $x(k)$ , to zero, so</p>
<pre><code> $V^&#123;\infin&#125;(x(0))=\min\limits_&#123;u(0),u(1),\cdots&#125; \sum\limits_&#123;k=0&#125;^&#123;\infin&#125;y&#39;(k)Qy(k)+u&#39;(k)Ru(k)$
</code></pre>
<p> so, similarly, this problem is again an LQR problem with equivalent state weight $Q=C’Q_y C$</p>
<blockquote>
<p>MATLAB: <code>[-K_∞,P_∞,E] = dlqry(sysd,Qy,R) </code> </p>
</blockquote>
<p>Corollary: </p>
<p>Let $(A, B)$ stabilizable, $(A, C)$ detectable, $R &gt; 0$, $Q_y &gt; $0. The LQR control law $u(k) = K_{LQR} x(k)$ the <u><strong>asymptotically stabilize</strong></u>s the closed-loop system </p>
<p>​        $\lim\limits_{t \rightarrow \infin}x(t)=0$, $\lim\limits_{t \rightarrow \infin} u(t)=0$ </p>
<ul>
<li><p><strong>Idea</strong>:</p>
<p>Intuitively: the minimum cost $x′(0)P_∞x(0)$ is finite ⇒ $y(k) → 0$ and $u(k) → 0$.<br>$y(k) → 0$ implies that the observable part of the state $→ 0$. As $u(k) → 0$, the unobservable states<br>remain undriven and go to zero spontaneously (=detectability condition)</p>
</li>
</ul>
<h1 id="Kalman-filtering"><a href="#Kalman-filtering" class="headerlink" title="Kalman filtering"></a>Kalman filtering</h1><p>Assign observer poles in an optimal way, that is to minimize the state estimation error $\tilde{x}  = x − \hat{x}$ </p>
<p>##Model:</p>
<ul>
<li><p>linear time-varying system with noise</p>
<p>​    $\begin{align} x(k+1)&amp;=A(k)x(k)+B(k)u(k)+G(k)\xi(k) \ y(k)&amp;=C(k)x(k)+D(k)u(k)+\zeta(k)\ x(0)&amp;=x_0 \end{align}$</p>
<ul>
<li><p>$\xi(k)$  Process noise, assume $\mathbb{E}(\xi(k))=0$ <u><em><strong>[zero means]</strong></em></u>; $\mathbb{E}(\xi(k) \xi’(j))=0$ <u><em><strong>[white noise]</strong></em></u>; and $\mathbb{E}(\xi(k) \xi’(k))=Q(k)$ <u><em><strong>[covariance matrix]</strong></em></u></p>
</li>
<li><p>$\zeta(k)$ measurement noise; assume $\mathbb{E}(\zeta(k))=0$ <u><em><strong>[zero means]</strong></em></u>; $\mathbb{E}(\zeta(k) \zeta’(j))=0$ <u><em><strong>[white noise]</strong></em></u>; and $\mathbb{E}(\zeta(k) \zeta’(k))=R(k)$ <u><em><strong>[covariance matrix]</strong></em></u></p>
</li>
<li><p>$x_0$ random vector; $\mathbb{E}(x_0)=\bar{x}_0$ ; $\mathbb{E}[(x_0-\bar{x}_0)(x_0-\bar{x}_0)’]=Var[x_0]=P_0$</p>
</li>
<li><p>Vectors $\xi(k)$ , $\zeta(k)$ , $x_0$ are uncorrelated: $\mathbb{E}(\xi(k) \zeta’(j))=0$ ; and $\mathbb{E}(\xi(k) x’_0)=0$; $\mathbb{E}(\zeta(k) x’_0)=0$</p>
</li>
<li><p>Probability distributions: assume <u>normal (Gaussian) distributions</u></p>
<p>​    $\xi(k)\sim\mathcal{N}(0,Q(k))$ , $\xi(k) \sim \mathcal{N}(0,R(k))$ , $x_0 \sim \mathcal(\bar{x}_0,P_0)$</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gst6s3f0gpj315z0jtdji.jpg" alt="Annotations"></p>
</li>
</ul>
</li>
</ul>
<p>##Steps：</p>
<p>The Kalman filter provides the optimal estimate $\hat{x}(k|k)$ of $x(k)$ given the measurements up to time $k$. Optimality means that the trace of the variance $P(k+1|k)$ is minimized.</p>
<p>Step1: measurement update</p>
<ul>
<li><p>Measurement update based on the most recent $y(k)$</p>
<p>​    $M(k)=P(k|k-1) C(k)’ [ C(k) P(k|k-1) C(k)’+ R(k) ]^{-1}$</p>
<p>​    $\hat{x}(k|k)=\hat{x}(k|k-1)+M(k) [ y(k)- C(k) \hat{x}(k|k-1) -D(k)u(k) ]$</p>
<p>​    $P(k|k)=( I - M(k) C(k)) P(k|k-1)$</p>
<p>With initial conditions $\hat{x}(0|-1)=\hat{x}_0, P(0|-1)=P_0$</p>
</li>
</ul>
<p>Step 2: Time update</p>
<ul>
<li><p>time update based on the model of the system</p>
<p>​    $\hat{x}(k+1|k)=A\hat{x}(k|k)+Bu(k)$</p>
<p>​    $P(k+1|k)=A(k)P(k|k)A(k)’ + G(k)Q(k)G(k)’$</p>
</li>
</ul>
<h2 id="Stationary-Kalman-Filter"><a href="#Stationary-Kalman-Filter" class="headerlink" title="Stationary Kalman Filter"></a>Stationary Kalman Filter</h2><ul>
<li><p>assume $A,C,G,Q,R$ are constant</p>
</li>
<li><p>$P(k|k-1), M(k)$ converge to the constant matrices</p>
<p>​    $P_{\infin} = AP_{\infin}A’ + GQG’ -AP_{\infin}C’[CP_{\infin}C’+R]^{-1} CP_{\infin}A’$</p>
<p>​    $\ M\ =P_{\infin} C’(CP_{\infin}C’+R)^{-1}$</p>
</li>
<li><p>By setting $L=AM$, the dynamics of the prediction $\hat{x}(k|k-1)$ becomes the luenberger observer</p>
<p>​    $\hat{x}(k+1|k)=A\hat{x}(k|k-1)+B(k)u(k)+L(y(k)-C\hat{x}(k|k-1)-D(k)u(k))$</p>
<p>With all the eigenvalues of $(A-LC)$ inside the unit circle</p>
<blockquote>
<p>MATLAB:</p>
<p>[~,L,P_∞,M,Z]=kalman(sys,Q,R),</p>
</blockquote>
<p>​                where $Z=\mathbb{E}[\tilde{x}(k|k)\tilde{x}(k|k)’]$</p>
</li>
</ul>
<h3 id="Tuning-Kalman-Filters"><a href="#Tuning-Kalman-Filters" class="headerlink" title="Tuning Kalman Filters"></a>Tuning Kalman Filters</h3><ul>
<li>It is usually hard to quantify exactly the correct values of $Q$ and $R$ for a given process </li>
<li>The diagonal terms of $R$ are related to how noisy are output sensors </li>
<li>$Q$ is harder to relate to physical noise, it mainly relates to how rough is the $(A, B)$ model </li>
<li>After all, $Q$ and $R$ are the tuning knobs of the observer (similar to LQR) </li>
<li>The “larger” is $R$ with respect to $Q$, the “slower” is the observer to converge ($L$, $M$ will be small) </li>
<li>On the contrary, the “smaller” is $R$ than $Q$, the more precise are considered the measurments, and the “faster” observer will be to converge </li>
</ul>
<h2 id="Extended-Kalman-Filter"><a href="#Extended-Kalman-Filter" class="headerlink" title="Extended Kalman Filter"></a>Extended Kalman Filter</h2><p>extended to nonlinear system:</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model:"></a>Model:</h3><p>nonlinear model:</p>
<p>​    $\begin{align} x(k+1)&amp;=f(x(k),u(k),\xi(k)) \ y(k)&amp;=g(x(k),u(k))+\zeta(k) \end{align}$</p>
<h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps:"></a>Steps:</h3><ol>
<li><p>Measurement update:</p>
<p>​    $$\begin{align} C(k)&amp;=\frac{\partial{g}}{\partial{x}} (\hat{x}_{k|k-1},u(k)) \ M(k)&amp;=P(k|k-1)C(k)’[C(k)R(k|k-1)C(k)’+R(k)]^{-1}\ \hat{x}(k|k)&amp;=\hat{x}(k|k-1)+M(k)(y(k)-g(\hat{x}(k|k-1),u(k))) \ P(k|k)&amp;=(I-M(k)C(k))P(k|k-1) \end{align}$$</p>
</li>
<li><p>Time update:</p>
<p>​    $$\begin{align} \hat{x}(k+1|k)&amp;=f(\hat{x}(k|k),u(k)),&amp; \ \ \  \hat{x}(0|-1)=\hat{x}<em>0     \ A(k)&amp;=\frac{\partial f}{\partial x}(\hat{x}</em>{k|k},u{k},\mathbb{E}[\xi(k)]), &amp;\ \ G(k)=\frac{\partial f}{\partial \xi}(\hat{x}_{k|k},u(k),\mathbb{E}[\xi(k)])            \   P(k+1|k)&amp;=A(k)P(k|k)A(k)’ + G(k)Q(k)G(k)’, &amp;\ \ P(0|-1)=P_0  \end{align}$$</p>
</li>
</ol>
<ul>
<li>The EKF is in general <strong><u><em>not optimal and may even diverge, due to linearisation</u></em></strong>. But is the de-facto standard in nonlinear state estimation </li>
</ul>
<h1 id="LQG"><a href="#LQG" class="headerlink" title="LQG"></a>LQG</h1><p><strong>LQG</strong> (Linear Quadratic Gaussian) control:     <strong><u>LQR control</u></strong>     +    <u><strong>stationary Kalman predictor/filter</strong></u></p>
<h3 id="Model-1"><a href="#Model-1" class="headerlink" title="Model:"></a>Model:</h3><ul>
<li>stochastic dynamic system</li>
</ul>
<p>​    $\begin{align} x(k+1)&amp;=Ax(k)+Bu(k)+\xi(k),&amp; \ \ w\sim\mathcal{N}(0,Q_{KF})            \       y(k)&amp;=Cx(k)+\zeta(k),&amp; \ \ v\sim\mathcal{N}(0,R_KF)        \end{align}$</p>
<p>​    with initial condition $x(0)=x_0, x_0\sim\mathcal{N}(\bar{x}<em>0,P_0), P,Q</em>{KF}\succeq0, R_{KF}&gt;0$ , and $\xi$ and $\zeta$ are <u>independent and white noise</u> terms</p>
<ul>
<li>objective is to minimize the cost function:</li>
</ul>
<p>​    $J(x(0),U)=\lim\limits_{T\rightarrow \infin} \frac{1}{T} \mathbb{E} \left[ \sum\limits_{k=0}^{T} x’(k)Q_{LQ}x(k)+u’()k R_{LQ}U(K)\right]$</p>
<p>​    when the state $x$ is not measurable</p>
<h3 id="control"><a href="#control" class="headerlink" title="control"></a>control</h3><ul>
<li>the pair $(A,B)$ is reachable and the pair $(A,C_q)$ with $C_q$ such that $Q_{LQ} =C_qC_q′$ is observable (here $Q$ is the weight matrix of the LQ controller)</li>
<li>the pair $(A, B_q)$, with $B_q$  s.t.  $Q_{KF} = B_qB_q′$ , is stabilizable, and the pair $(A, C)$ is observable (here $Q$ is the covariance matrix of the Kalman predictor/filter)</li>
</ul>
<p>Then,</p>
<ol>
<li>Determine the optimal stationary Kalman predictor/filter, neglecting the fact that the control variable $u$ is generated through a closed-loop control scheme, and find the optimal gain $L_KF$ </li>
<li>Determine the optimal $L_{QR}$ strategy assuming the state accessible, and find the optimal gain $K_{LQR}$</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gst6rq6y4mj30wa0f4jst.jpg" alt="LQG controller"></p>
<blockquote>
<p>Analogously to the case of <u>output feedback control</u> using a <u>Luenberger observer</u>, it is possible to show that the extended <u>state $[x′ \ \   \tilde{x}’]’$ has eigenvalues</u> <em>equal to</em> the eigenvalues of $(A + BK_{LQR})$ plus those of $(A − L_{KF} C)$ (2n in total)</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>
<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoxue Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="199" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{&quot;enable&quot;:true,&quot;tags&quot;:&quot;none&quot;,&quot;single_dollars&quot;:true,&quot;cjk_width&quot;:0.9,&quot;normal_width&quot;:0.6,&quot;append_css&quot;:true,&quot;js&quot;:&quot;https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;npm&#x2F;mathjax@3.1.4&#x2F;es5&#x2F;tex-mml-chtml.js&quot;}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
